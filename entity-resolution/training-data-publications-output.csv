Cluster ID,confidence_score,Title,Authors,Abstract,Source,DatePublished
7,1.0,A Catalog EleutherAI's Large Language Models,Jennifer D’Souza,"Starting in early 2022, EleutherAI released open-sourced high-performing models built after GPT-2/GPT-3 with an intent to open-source strong LLMs. With its Pythia series, its initiative to open-source LLMs continues, which now extends into releasing the largest collection of 16 total open-source models at different parameter sizes to study the effects of scale on LLMs. This comparison showcases EleutherAI's models released so far in terms of their salient properties.",RESODATE,2023-01-01
0,0.90594435,A Comparison on Foundation Large Language Models,Sanju Tiwari,"This comparison analyzes how the current advances in foundational
LLM, like GPT-3,  REBEL, PaLM, BLOOM, OPT are compared with its different parameter.",RESODATE,2023-01-01
0,0.90594435,A Comparison on Foundation Large Language Models,Sanju Tiwari,"This comparison analyzes how the current advances in foundational
LLM, like GPT-3,  REBEL, PaLM, BLOOM, OPT are compared with its different parameter.",RESODATE,2023-01-01
8,1.0,A Catalog of EleutherAI's Large Language Models,Jennifer D’Souza,"Starting in early 2022, EleutherAI released open-sourced high-performing models built after GPT-2/GPT-3 with an intent to open-source strong LLMs. With its Pythia series, its initiative to open-source LLMs continues, which now extends into releasing the largest collection of 16 total open-source models at different parameter sizes to study the effects of scale on LLMs. This comparison showcases EleutherAI's models released so far in terms of their salient properties.",RESODATE,2023-01-01
9,1.0,A Catalog of MosaicML's MPT series Large Language Models,Jennifer D’Souza,"Thus far the MPT model series from MosaicML, i.e. MPT-7B and MPT-30B, have proven to be very competitive with MPT-7B surpassing GPT-3 in many academic tasks, and MPT-30B surpassing perfomances by Llama-1, Falcon-40B, and all prior models from EleutherAI. This comparison showcases MosaicML's models released so far in terms of their salient properties. Increasingly with stronger models released for research, the open-sourced development of LLMs offers a promising future trajectory.",RESODATE,2023-01-01
10,1.0,Exploring Large Language Models augmented Knowledge Graph Completion Methods,Dr. Sanju Tiwari,Knowledge Graph Completion (KGC) infers missing facts in a given knowledge graph. Integration of large language models enables Knowledge Graph Completion methods to generate facts or encode the text for improving the performance. This comparison compares different LLM-augmented Knowledge Graph Completion methods.,RESODATE,2023-01-01
11,1.0,Exploring Interpretability of Knowledge Graph-enhanced Large Language Models ,Dr. Sanju Tiwari,The large language model (LLM) interpretability indicates to the explanation and understanding of the internal workings and decision-making tasks of a large language model. Knowledge Graphs are utilizing to improve the interpretability of Large Language Models. This comparisons explores 10 models to check different parameters.,RESODATE,2023-01-01
12,1.0,Comparison of Knowledge Graph enhanced Large Language Models for inferencing,Dr. Sanju Tiwari,"This comparison explores 8 Knowledge Graph enhanced Large Language Models for inferencing tasks and highlights different properties to compare the datasets, methodology, experiments etc.",RESODATE,2023-01-01
13,1.0,On emergence,"Lu, Sheng; Bigoulaeva, Irina; Sachdeva, Rachneet; Tayyar Madabushi, Harish; Gurevych, Iryna","Output files for the paper Are Emergent Abilities in Large Language Models just In-Context
Learning.",RESODATE,2023-08-24T11:47:13Z
1,0.90594435,A Catalog of OpenAI's GPT series,Jennifer D’Souza,"The comparison showcases the GPT series of Large Language Models from GPT-1 to the latest, at the time of creation of this comparison, GPT-4.",RESODATE,2023-01-01
1,0.90594435,A Catalog of OpenAI's GPT series,Jennifer D’Souza,"The comparison showcases the GPT series of Large Language Models from GPT-1 to the latest, at the time of creation of this comparison, GPT-4.",RESODATE,2023-01-01
14,1.0,Comparison of KG-enhanced LLM methods for the pre-training of KG-enhanced LLM ,Dr. Sanju Tiwari,"This comparison has summarized 17 KG-enhanced large scale pre-trained language models for the pre-training of KG-enhanced LLM. This comparison have modeled with 10 different properties and explored the datasets, experiments, methodologies code source for 17 Large language Models.",RESODATE,2023-01-01
15,1.0,Artificial Intelligence and Librarianship,Martin Frické,"Courses on Artificial Intelligence (AI) and Librarianship in ALA-accredited Masters of Library and Information (MLIS) degrees are rare. We have all been surprised by ChatGPT and similar Large Language Models. Generative AI is an important new area for librarianship. It is also developing so rapidly that no one can really keep up. Those trying to produce AI courses for the MLIS degree need all the help they can get. This book is a gesture of support. It consists of about 95,000 words on the topic, with a 3-400 item bibliography.",OERSI,
16,1.0,Knowledge Graphs - Foundations and Applications,Harald Sack,"Despite the fact that it affects our lives on a daily basis, most of us are unfamiliar with the concept of a knowledge graph. When we ask Alexa about tomorrow's weather or use Google to look up the latest news on climate change, knowledge graphs serve as the foundation of today's cutting-edge information systems. In addition, knowledge graphs have the potential to elucidate, assess, and substantiate information produced by Deep Learning models, such as Chat-GPT and other large language models. Knowledge graphs have a wide range of applications, including improving search results, answering questions, providing recommendations, and developing explainable AI systems. In essence, the purpose of this course is to provide a comprehensive overview of knowledge graphs, their underlying technologies, and their significance in today's digital world.",OERSI,
2,0.55371606,Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models,Tiffany H Kung; Morgan Cheatham; Arielle Medenilla; Czarina Sillos; Lorie De Leon; Camille Elepaño; Maria Madriaga; Rimel Aggabao; Giezel Diaz-Candido; James Maningo; Victor Tseng,"We evaluated the performance of a large language model called ChatGPT on United States Medical Licensing Exam (USMLE), which consists three exams: Step 1, 2CK, and 3. performed at or near passing threshold for all exams without any specialized training reinforcement. Additionally, demonstrated high level concordance insight in its explanations. These results suggest that models may have potential to assist with medical education, potentially, clinical decision-making.",OpenAlex,2023-02-09
17,1.0,ChatGPT and Other Large Language Models Are Double-edged Swords,Yiqiu Shen; Laura Heacock; Jonathan Elias; Keith Hentel; Beatriu Reig; George Shih; Linda Moy,"HomeRadiologyVol. 307, No. 2 PreviousNext Reviews and CommentaryEditorialChatGPT Other Large Language Models Are Double-edged SwordsYiqiu Shen , Laura Heacock, Jonathan Elias, Keith D. Hentel, Beatriu Reig, George Shih, Linda MoyYiqiu MoyAuthor AffiliationsFrom the Center for Data Science, New York University, 60 5th Ave, York, NY 10011 (Y.S.); Department of Radiology, University School Medicine, (L.H., B.R., L.M.); Departments Primary Care (J.E.) Radiology (K.D.H., G.S.), Weill Cornell NY.Address correspondence to Y.S. (email: [email protected]).Yiqiu HeacockJonathan EliasKeith HentelBeatriu ReigGeorge ShihLinda MoyPublished Online:Jan 26 2023https://doi.org/10.1148/radiol.230163MoreSectionsFull textPDF ToolsImage ViewerAdd favoritesCiteTrack CitationsPermissionsReprints ShareShare onFacebookTwitterLinked In References1. Min B, Ross H, Sulem E, et al. Recent advances in natural language processing via large pre-trained models. arXiv 2111.01243 [preprint]. https://arxiv.org/abs/2111.01243. Posted November 1, 2021. Accessed January 19, 2023. Google Scholar2. Vaswani A, Shazeer N, Parmar Attention is all you need. Advances Neural Information Processing Systems 2017;30. https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. Scholar3. ChatGPT: Optimizing Dialogue. OpenAI. https://openai.com/blog/chatgpt/. Published 30, 2022. Scholar4. Brown T, Mann Ryder models are few-shot learners. 2020;33:1877–1901.https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Scholar5. OpenAI Used Kenyan Workers on Less Than $2 Per Hour Make ChatGPT Toxic. TIME. https://time.com/6247678/openai-chatgpt-kenya-workers/. 18,2023. 21, Scholar6. Christiano PF, Leike J, Martic M, Legg S, Amodei Deep reinforcement learning from human preferences. https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html. Scholar7. Rohrbach Hendricks LA, Burns K, Darrell Saenko K. Object Hallucination Image Captioning. In: Proceedings 2018 Conference Empirical Methods Natural 2018;4035–4045. Scholar8. Xiao Y, Wang WY. On Predictive Uncertainty Conditional Generation. 16th European Chapter Association Computational Linguistics: Main Volume Scholar9. Clinical Decision Support. American Medical Informatics Association. https://amia.org/community/working-groups/clinical-decision-support#:~:text=Clinical%20Decision%20Support%20(CDS)%20is,care%20services%20and%20patient%20outcomes. 16, Scholar10. Appropriate use criteria advanced diagnostic imaging services. Code Federal Regulations. https://www.ecfr.gov/current/title-42/chapter-IV/subchapter-B/part-414/subpart-B/section-414.94/. 18, Scholar11. @tiktokrheumdok. save time with Insurance Denials. TikTok. https://www.tiktok.com/@tiktokrheumdok/video/7176660771806383403. December 13, 4, Scholar12. @StuartBlitz. You: There’s no case healthcare. Docs: Watch this. Twitter. https://twitter.com/StuartBlitz/status/1602834224284897282. Scholar13. FAQ: Commonly asked questions about ChatGPT. https://help.openai.com/en/articles/6783457-chatgpt-faq#:~:text=It%20has%20limited%20knowledge%20of%20world%20and%20events%20after%202021/. 22, Scholar14. Reyes Meier R, Pereira Interpretability Artificial Intelligence Radiology: Challenges Opportunities. Radiol Artif Intell 2020;2(3):e190043. Link, Scholar15. Wu Phang An interpretable classifier high-resolution breast cancer screening images utilizing weakly supervised localization. Med Anal 2021;68:101908. Crossref, Medline, Scholar16. Shamout FE, Oliver JR, intelligence system reduces false-positive findings interpretation ultrasound exams. Nat Commun 2021;12(1):5645. Scholar17. artificial predicting deterioration COVID-19 patients emergency department. NPJ Digit 2021;4(1):80. Scholar18. Luft LM. The essential role physician as advocate: how why we pass it on. Can Educ J 2017;8(3):e109–e116. Scholar19. Earnest MA, Wong SL, Federico SG. Perspective: Physician advocacy: what do it? Acad 2010;85(1):63–67. Scholar20. Szakaly How Hijacks Democracy. Times. https://www.nytimes.com/2023/01/15/opinion/ai-chatgpt-lobbying-democracy.html. 15, Scholar21. Report Select Committee Intelligence, United States Senate, Russian Active Measures Campaigns Interference 2016 U.S. Election 2: Russia’s Use Social Media Additional Views. https://www.intelligence.senate.gov/sites/default/files/documents/Report_Volume2.pdf. Scholar22. Huang Alarmed by A.I. Chatbots Universities Start Revamping They Teach. https://www.nytimes.com/2023/01/16/technology/chatgpt-artificial-intelligence-universities.html. Scholar23. Tian E. GPTZero https://gptzero.substack.com/. 3, Scholar24. Gao CA, Howard FM, Markov NS, Comparing scientific abstracts generated original using an output detector, plagiarism blinded reviewers. bioRxiv https://doi.org/10.1101/2022.12.23.521610. 27, Scholar25. Bolton Hall D, Yasunaga Lee Manning C, Liang P. PubMedGPT 2.7B. Research Foundation Models, Stanford University. https://crfm.stanford.edu/2022/12/15/pubmedgpt.html. Scholar26. Clarification Model Policy LLM. ICML 2023: Fortieth International Machine Learning. https://icml.cc/Conferences/2023/llm-policy. Scholar27. Bik EM, Casadevall Fang FC. prevalence inappropriate image duplication biomedical research publications. MBio 2016;7(3):e00809–16. Scholar28. Yue W, AbdAlmageed Natarajan ManTra-Net: Manipulation Tracing Network Detection Localization Forgeries With Anomalous Features. IEEE/CVF Computer Vision Pattern Recognition, 2019; 9543–9552. https://ieeexplore.ieee.org/document/8953774. Scholar29. Aditya Dhariwal P, Nichol Chu Chen M. Hierarchical Text-Conditional Generation CLIP Latents. 2204.06125 https://arxiv.org/abs/2204.06125. April Scholar30. Rombach Blattmann Lorenz Esser Ommer B. High-resolution synthesis latent diffusion 2022 Recognition (CVPR). IEEE, June Scholar31. Chambon Bluethgen Langlotz CP, Chaudhari A. Adapting pretrained vision-language foundational medical domains. 2210.04133 https://arxiv.org/abs/2210.04133. October 9, Scholar32. Biswas S. Future Writing. 2023;307(2):e223312. Scholar33. Kitamura Is Shaping Writing But Still Requires Human Judgment. 2023;307(2):e230171. ScholarArticle HistoryReceived: Jan 23 2023Revision requested: received: 2023Accepted: 2023Published online: 2023 FiguresReferencesRelatedDetailsCited ByFeasibility Differential Diagnosis Based Imaging Patterns Using a ModelJonathan Kottlors, Grischa Bratke, Philip Rauen, Christoph Kabbasch, Thorsten Persigehl, Marc Schlamann, Simon Lennartz, 5 July | Vol. 308, 1A Context-based Chatbot Surpasses Trained Radiologists Generic Following ACR Appropriateness GuidelinesAlexander Rau, Stephan Daniela Zoeller, Anna Fink, Hien Tran, Caroline Wilpert, Johanna Nattenmueller, Jakob Neubauer, Fabian Bamberg, Marco Reisert, Maximilian F. Russe, 25 1Performance Board-style Examination: Insights into Current Strengths LimitationsRajesh Bhayana, Satheesh Krishna, Robert R. Bleakney, 16 May 5Rise It Be Time Reassess We Teach Test ResidentsAna Lourenco, Priscilla J. Slanetz, Grayson L. Baird, 5ChatGPT PublicationsErfan Darzidehkalani, 6 5GPT-4 Automated Determination Radiologic Study Protocol Request Forms: A Feasibility StudyRoman Johannes Gertz, Alexander Christian Bunck, Thomas Dratsch, Andra-Iza Iuga, David Maintz, 13 5How AI Responds Common Lung Cancer Questions: versus BardAmir Ali Rahsepar, Neda Tavakoli, Grace Hyun Kim, Cameron Hassani, Fereidoun Abtin, Arash Bedayat, 5The Role Limitations Such Settings JournalismFurkan Ufuk, 7 March 3Appropriateness Breast Prevention Screening Recommendations Provided ChatGPTHana Haver, Emily Ambinder, Manisha Bahl, Eniola T. Oluyemi, Jean Jeudy, Paul H. Yi, 4 4ChatGPT JudgmentFelipe C. Kitamura, February 2The potential impact clinical translational medicineVivian WeiwenXue, PingguiLei, William C.Cho2023 Translational 3Communications ScienceAravindKumaresan, LornaUden, ShazadAshraf2023 1825A comparison ChatGPT-generated articles human-written articlesSisithAriyaratne, Karthikeyan. P.Iyengar, NehaNischal, NaparlaChitti Babu, RajeshBotchu2023 Skeletal RadiologyLarge (LLM) will nuclear medicine be?Ian L.Alberts, LorenzoMercolli, ThomasPyka, GeorgePrenosil, KuangyuShi, AxelRominger, AliAfshar-Oromieh2023 Journal Nuclear Medicine Molecular Imaging, 50, 6Chat generative transformer (ChatGPT): implications rheumatology practiceArvindNune, CiroManzo, BhupenBarman, Rheumatology International, 43, 7Transformers, codes labels: modelling radiologyDenisRemedios, AlexRemedios2023 33, 6Does GPT4 dream counting electric nodules?ChristianBlüthgen2023 RadiologyThe open platform modern neurosurgical education: preliminary studyUmut TanSevgi, GökberkErol, YücelDoğruel, Osman FikretSönmez, Richard ShaneTubbs, AbuzerGüngor2023 Neurosurgical Review, 46, 1Transforming Maritime Health ChatGPT-Powered Healthcare Services MarinersManikSharma, SamritiSharma2023 Annals Biomedical Engineering, 51, 6ChatGPTs' Journey Revolution: Potential Panacea or Hidden Pathogen?JingYang2023 EngineeringChatGPT—a foe ally?Om PrakashYadava2023 Indian Thoracic Cardiovascular Surgery, 39, 3ChatGPT radiology: lights shadows bionetworkRiccardoLaudicella, Guido A.Davidzon, NikolaosDimos, GaetanoProvenzano, AndreiIagaru, SotiriosBisdas2023 ImagingLa strada della ricerca fra lo Scramjet e i chiodi quattro punte di Chat GPTFrancescoTrimarchi, LuigiBartalena2023 L'Endocrinologo, 24, 3ChatGPTDiveshSardana, Timothy R.Fagan, John TimothyWright2023 Dental Association, 154, promise peril geriatric nursing What know not knowXiangQi, ZhengZhu, BeiWu2023 Aging Research, 2Authors Age Language-generation AI: To be be, that Really Question?José DaríoMartínez-Ezquerro2023 Archives 54, 3Beyond chatting: opportunities challenges radiologyJuan LavistaFerres, B.Weeks, C.Chu, Steven P.Rowe, Elliot K.Fishman2023 Diagnostic Interventional 104, 6Attention need: complicated ethically healthcare medicineStefanHarrer2023 eBioMedicine, 90ChatGPT global public health: Applications, challenges, ethical considerations mitigation strategiesAteeb AhmadParray, Zuhrat MahfuzaInam, DiegoRamonfaur, Shams ShababHaider, Sabuj KantiMistry, Apurva KumarPandya2023 Global Transitions, 5ChatGPT: comprehensive review background, applications, key bias, ethics, limitations future scopePartha PratimRay2023 Internet Things Cyber-Physical Systems, 3Evaluating GPT Adjunct Making: GPT-4 Versus GPT-3.5 PilotAryaRao, JohnKim, MeghanaKamineni, MichaelPang, WinstonLie, J.Dreyer, D.Succi2023 College RadiologyArtificial Allergy Immunology Practice: Where Have Been Going?PolatGoktas, GulKarakaya, FuatKalyoncu, EbruDamadoglu2023 Immunology: PracticeChatGPT model chatbots: current state acceptability proposal guidelines utilization academic medicineJin K.Kim, MichaelChua, MandyRickard, ArmandoLorenzo2023 Pediatric UrologyChatting cheating? impacts other nurse educationEdmond Pui HangChoi, Jung JaeLee, Mu-HsingHo, Jojo Yan YanKwok, Kris Yuet WanLok2023 Nurse Education Today, 125Is valid author?Jaime A.Teixeira da Silva2023 Practice, 68Appropriateness Readability ChatGPT-4 Responses Surgical Treatment Retinal DiseasesBitaMomenaei, TakuWakabayashi, AbtinShahlaee, Asad F.Durrani, Saagar A.Pandit, KristineWang, Hana A.Mansour, M.Abishek, DavidXu, JayanthSridhar, YoshihiroYonekawa, Ajay E.Kuriyan2023 Ophthalmology RetinaChatGPT services: emerging stage innovative perspectiveMohdJavaid, AbidHaleem, Ravi PratapSingh2023 BenchCouncil Transactions Benchmarks, Standards Evaluations, 1Do Understand Chemistry? Conversation ChatGPTCayque MonteiroCastro Nascimento, André SilvaPimentel2023 Chemical Modeling, 63, 6Academic integrity intelligence: hype, hero heresy?Geoffrey M.Currie2023 Seminars MedicineReadership Awareness Series – Paper 4: - Ethical Considerations Scientific PublicationsMohammad JavedAli, AliDjalilian2023 Ophthalmology, 38, Ophthalmology: Exploring Its Discharge Summaries Operative NotesSwatiSingh, AliDjalilian, Mohammad JavedAli2023 5A SWOT analysis Implications educational practice researchMohammadrezaFarrokhnia, Seyyed KazemBanihashem, OmidNoroozi, ArjenWals2023 Innovations Teaching InternationalChatGPT: Systematic Agenda Multidisciplinary ResearchHarjitSingh, AvneetSingh2023 Chinese Economic Business Studies, 2Is Algorithm Good Bad World, Has Learned Bad? “Locked” “Continuously Learning” “Autonomous” “Assistive” Tools HealthcareAlaaYoussef, MichaelAbramoff, DantonChar2023 Bioethics, 23, takes Exam Core Cardiology: success story?IoannisSkalidis, AurelienCagnina, WongsakornLuangphiphat, ThaboMahendiran, OlivierMuller, EmmanuelAbbe, StephaneFournier2023 Heart Digital Health, Lacrimal Drainage Disorders: Performance Scope ImprovementMohammad Ophthalmic Plastic & Reconstructive applications speed up writingTzeng-JiChen2023 86, 4Who making decisions? retail managers can power ChatGPTAnujKumar, NimitGupta, GautamBapat2023 StrategyChatGPT tourism: benefits risksInêsCarvalho, StanislavIvanov2023 Tourism Review2023 Advancement Computation Technologies (InCACCT)SmitaWagholikar, ArtiChandani, RizwanaAtiq, MohitPathak, OmkarWagholikar20232023 Engineering Design Symposium (SIEDS)AramBahrini, MohammadsadraKhamoshifar, HosseinAbbasimehr, J.Riggs, MaryamEsmaeili, Rastin MastaliMajdabadkohne, MortezaPasehvar2023Implications such dental medicineFlorinEggmann, RolandWeiger, Nicola U.Zitzmann, Markus B.Blatz2023 Esthetic Restorative DentistryHow augment perioperative medicine: daring discourseRodney AGabriel, Edward RMariano, JulianMcAuley, Christopher LWu2023 Regional Anesthesia Pain MedicineThe wide range rheumatologyThomasHügle2023 RMD Open, 2ChatGPT publishing: ally adversary?SisithAriyaratne, RajeshBotchu, Karthikeyan PIyengar2023 Scottish JournalAccuracy References ChatGPT-3 Retrieval Radiological InformationMatthias W.Wagner, Birgit B.Ertl-Wagner2023 Canadian JournalChatGPT Deeper Look Into its Pathways ImprovementPartha PratimRay, PoulamiMajumder2023 JournalArtificial Augmenting telehealth modelsCentaine LSnoswell, Aaron JSnoswell, Jaimon TKelly, Liam JCaffery, Anthony CSmith2023 Telemedicine TelecareUnlocking Power Framework Applying Generative EducationJiahongSu (苏嘉红), WeipengYang (杨伟鹏)2023 ECNU Review EducationAI Management Scholars Practitioners: Guidelines ImplicationsSudhirRana2023 FIIB 12, 1Are “the answer” bringing us closer systematic automation?RiazQureshi, DanielShaughnessy, Kayden R.Gill, Karen A.Robinson, TianjingLi, EitanAgai2023 Reviews, 1Evaluation Accuracy Answering Questions Japanese Society Hypertension GuidelinesKenyaKusunose, ShuichiroKashima, MasatakaSata2023 Circulation Journal, 87, 7ChatGPT radiologists’ perspectiveHalit NahitŞendur, Aylin BillurŞendur, Mahi NurCerit2023 British RadiologyStatistical big data applicationsHaraldWitte, Tobias U.Blatter, PriyankaNagabhushana, DavidSchär, JamesAckermann, JanneCadamuro, B.Leichtle2023 Laboratory 0, 0Utility (Preprint)JialinLiu, ChangyuWang, SiruLiu2023 ResearchAn Opinion Care—Written Humans OnlyJensKleesiek, YonghuiWu, GregorStiglic, JanEgger, JiangBian2023 64, 5Can improve communication hospitals?DavidSantandreu-Calonge, PabloMedina-Aguerrebere, PatrikHultberg, Mariam-AmanShah2023 El Profesional de la informaciónYAPAY ZEKÂ SOHBET ROBOTU CHATGPT İLE İNANÇ- İNANÇSIZLIK, DOĞAL AFET VE ÖLÜM KONULARI ÜZERİNE NİTEL BİR ARAŞTIRMA: DİN MANEVİYATIN PSİKOLOJİK SAĞLIĞA ETKİLERİMuhammedKIZILGEÇİT, MuratÇİNİCİ, NesrullahOKAN2023 Ağrı İbrahim Çeçen Üniversitesi Sosyal Bilimler Enstitüsü Dergisi, 1Reflection whether should banned academia perspective education teachingHaoYu2023 Frontiers Psychology, 14Artificial Applications Imaging: Status DirectionsClayton R.Taylor, NatashaMonga, CandiseJohnson, Jeffrey R.Hawley, MitvaPatel2023 Diagnostics, 12ChatGPT Utility Education, Promising Perspectives Valid ConcernsMalikSallam2023 Healthcare, 11, 6Future Speech Interfaces Sensors IntelligenceBruceDenby, Tamás GáborCsapó, MichaelWand2023 Sensors, 4Evolutionary Game Analysis Pre-Trained Transformer EducationYanweiYou, YuquanChen, YujunYou, QiZhang, QiangCao2023 Sustainability, 12Large Education: Preparing Rapid Transformation Trainees Will Learn DoctorsAkshayRavi, AaronNeinstein, Sara G.Murray2023 ATS ScholarAI-based Impact PublicationLiMofan, Zhang, MMYongyue, Sun, MMYang, Cui, PhDLigang, Wang, PhDShumin2023 ADVANCED ULTRASOUND IN DIAGNOSIS AND THERAPY, 7, 2Fighting Obsolescence: Professional Assessment Era ChatGPTLincoln L.Berland, Seth M.Hardy2023 Applied RadiologyAdvances Educational Instructional DesignLindaMarron2023Editorial: (AI)-Assisted DiscourseArvieVitente, RolandoLazaro, Catherine JoyEscuadra, JocelRegino, EsmeritaRotor2023 Philippine Physical TherapyThe Application ChatGPT, Assisted Technology ServicesSedatYİĞİT, SonerBERŞE, EzgiDİRGAR2023 Eurasian AssessmentThe State-of-art NLP: Evidence ChatGPTYumingZhao2023 Highlights Technology, 49Artificial IntelligenceParampreetKaur, AaronAlexander Mack, NaitikPatel, AmitPal, RajwinderSingh, AllinciaMichaud, MollyMulflur2023 0ChatGPT Big Data: Enhancing Text-to-Speech ConversionHatim AbdelhakDida, DSKChakravarthy, FazleRabbi2023 Mesopotamian DataOverview Early ChatGPT’s Presence Literature: From Hybrid Literature ExpertsOmarTemsah, Samina AKhan, YazanChaiah, AbdulrahmanSenjab, KhalidAlhasan, AmrJamal, FadiAljamaan, Khalid HMalki, RabihHalwani, Jaffar AAl-Tawfiq, Mohamad-HaniTemsah, AymanAl-Eyadhy2023 CureusExploring Boundaries Reality: Investigating Phenomenon Through ReferencesSai AnirudhAthaluri, Sandeep VarmaManthena, V S R Krishna ManojKesapragada, VineelYarlagadda, TirthDave, Rama Tulasi SiriDuddumpudi2023 CureusChatGPT Dentistry: Comprehensive ReviewHind MAlhaidry, BaderFatani, Jenan OAlrayes, Aljowhara MAlmana, Nawaf KAlfhaed2023 CureusAn era significant futuristic support tool: study features, abilities, challengesAbidHaleem, MohdJavaid, PratapSingh2022 2, 4Artificial researchSMBalaji2022 4Accompanying This ArticleChatGPT- Special Radiology:AI Podcast CollaborationFeb 21 2023Default SeriesEpisode 17: ChatGPT- SeriesRecommended Articles Learning: Primer RadiologistsRadioGraphics2017Volume: 37Issue: 7pp. 2113-2131Deep Adversarial Networks: Musculoskeletal ImagingRadiology: Intelligence2021Volume: 3Issue: 3Current Learning RadiologyRadiology2018Volume: 288Issue: 2pp. 318-328Deep at Chest Radiography: Classification Pulmonary Tuberculosis Convolutional NetworksRadiology2017Volume: 284Issue: 574-582Artificial Mammography Tomosynthesis: Concepts PerspectivesRadiology2019Volume: 293Issue: 246-259See More RSNA Exhibits Structured Reporting And Processing: Successful MarriageDigital Posters2021Anatomy Project Prognosis Prediction: Collecting Building PipelineDigital Posters2019Artificial Past, Present, FutureDigital Posters2020 Case Collection Asymmetric lactational change Collection2021Dynamic MRI pneumoniaRSNA Collection2020Slow-growing cancerRSNA Collection2020 PodcastPodcastMetrics Altmetric Score PDF download",OpenAlex,2023-04-01
18,1.0,ChatGPT for good? On opportunities and challenges of large language models for education,Enkelejda Kasneci; Kathrin Sessler; Stefan Küchemann; Maria Bannert; Daryna Dementieva; Frank Fischer; Urs Gasser; Georg Groh; Stephan Günnemann; Eyke Hüllermeier; Stepha Krusche; Gitta Kutyniok; Tilman Michaeli; Claudia Nerdel; Jürgen Pfeffer; Oleksandra Poquet; Michael Sailer; Albrecht Schmidt; Tina Seidel; Matthias Stadler; J. Weller; Jochen Kuhn; Gjergji Kasneci,"Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities regions, large are here stay. This commentary presents potential benefits challenges educational applications models, from student teacher perspectives. We briefly discuss current state their applications. then highlight how these can be used create content, improve engagement interaction, personalize learning experiences. With regard challenges, we argue that education require teachers learners develop sets competencies literacies necessary both understand as well limitations unexpected brittleness such systems. In addition, clear strategy systems pedagogical approach with strong focus on thinking strategies for fact checking required integrate take full advantage settings teaching curricula. Other bias output, need continuous human oversight, misuse not unique application AI education. But believe that, if handled sensibly, offer insights opportunities scenarios acquaint students early societal biases, criticalities, risks conclude recommendations address ensure responsible ethical manner",OpenAlex,2023-04-01
19,1.0,How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment,Aidan Gilson; Conrad W Safranek; Thomas Huang; Vimig Socrates; Ling Chi; Andrew Taylor; David Chartash,"Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input.This study aimed evaluate the performance of ChatGPT on questions within scope United States Medical Licensing Examination Step 1 and 2 exams, as well analyze for interpretability.We used sets multiple-choice ChatGPT's performance, each with pertaining 2. The first set was derived from AMBOSS, commonly question bank medical students, which also provides statistics difficulty an exam relative base. second National Board Examiners (NBME) free 120 questions. compared other large models, GPT-3 InstructGPT. text output response evaluated across 3 qualitative metrics: logical justification answer selected, presence information internal question, external question.Of 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, NBME-Free-Step2, achieved accuracies 44% (44/100), 42% (42/100), 64.4% (56/87), 57.8% (59/102), respectively. outperformed InstructGPT by 8.15% average all performed similarly random chance. demonstrated significant decrease in increased (P=.01) AMBOSS-Step1 set. We found selection present 100% outputs NBME sets. Internal 96.8% (183/189) 44.5% 27% lower incorrect answers correct NBME-Free-Step1 (P<.001) NBME-Free-Step2 (P=.001) respectively.ChatGPT marks improvement models tasks answering. By performing at greater than 60% threshold NBME-Free-Step-1 set, we show achieves equivalent passing score third-year student. Additionally, highlight capacity provide logic informational context majority answers. These facts taken together make compelling case potential applications interactive education tool support learning.",OpenAlex,2023-02-08
20,1.0,Persistent Anti-Muslim Bias in Large Language Models,Abubakar Abid; Maheen Farooqi; James Zou,"It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias relatively unexplored. We demonstrate GPT-3, a state-of-the-art contextual model, captures persistent Muslim-violence bias. probe GPT-3 in various ways, including prompt completion, analogical reasoning, story generation, understand this anti-Muslim bias, demonstrating it appears consistently creatively different uses of the model is severe even compared biases about other groups. For instance, Muslim analogized terrorist 23% test cases, while Jewish mapped its most common stereotype, money, 5% cases. quantify positive distraction needed overcome with adversarial text prompts, find use 6 adjectives reduces violent completions for Muslims from 66% 20%, but which still higher than",OpenAlex,2021-07-21
21,1.0,Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm,Laria Reynolds; Kyle McDonell,"Prevailing methods for mapping large generative language models to supervised tasks may fail sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest the function of examples in these cases is better described locating an already learned task rather than meta-learning. This analysis motivates rethinking role controlling and evaluating powerful models. discuss prompt programming, emphasizing usefulness considering through lens natural language. explore techniques exploiting capacity narratives cultural anchors encode nuanced intentions encouraging deconstruction problem into components before producing verdict. Informed by this more encompassing theory also introduce idea metaprompt seeds model generate its own range tasks. Finally, how general interacting with be incorporated existing future benchmarks practical applications.",OpenAlex,2021-05-08
22,1.0,Structured Pruning of Large Language Models,Ziheng Wang; Jeremy Wohlwend; Tao Lei,"Large language models have recently achieved state of the art performance across a wide variety natural tasks. Meanwhile, size these and their latency significantly increased, which makes usage costly, raises an interesting question: do need to be large? We study this question through lens model compression. present generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, adaptively removing rank-1 components during training. On modeling tasks, our outperforms other unstructured block-structured baselines at various compression levels, while achieving significant speedups both training inference. also demonstrate that method can applied adaptive word embeddings in large models, BERT on several downstream fine-tuning classification benchmarks.",OpenAlex,2020-01-01
23,1.0,Strategies for training large scale neural network language models,Tomas Mikolov; Anoop Deoras; Daniel Povey; Lukas Burget; Jan Cernocký,"We describe how to effectively train neural network based language models on large data sets. Fast convergence during training and better overall performance is observed when the are sorted by their relevance. introduce hash-based implementation of a maximum entropy model, that can be trained as part model. This leads significant reduction computational complexity. achieved around 10% relative word error rate English Broadcast News speech recognition task, against 4-gram model 400M tokens.",OpenAlex,2011-12-01
3,0.6165474,Evaluating Large Language Models Trained on Code,"Mark Chen; Jerry Tworek; Heewoo Jun; Qiming Yuan; Henrique Ponde de Oliveira Pinto; Jared Kaplan; Harrison Edwards; Yuri Burda; Nicholas Joseph; Greg Brockman; Alex Ray; Raul Puri; Gretchen Krueger; M. N. Petrov; Heidy Khlaaf; Girish Sastry; Pamela Mishkin; Brooke Chan; Scott Gray; Nick Ryder; Mikhail Pavlov; Alethea Power; Lukasz Kaiser; Mohammad Bavarian; Clemens Winter; Philippe Tillet; Felipe Petroski Such; Dave Cummings; Matthias Plappert; Fotios Chantzis; Elizabeth Barnes; Ariel Herbert-Voss; William H. Guss; Alex Nichol; Alex Paino; Nikolas Tezak; Jie Tang; I. Babuschkin; Suchir Balaji; Shantanu Jain; William S. Saunders; Christopher Hesse; Andrew N. Carr; Jan Leike; Joshua Achiam; Misra, Vedant; Evan Morikawa; Alec Radford; Matthew M. Knight; Miles Brundage; Mira Murati; Katie Mayer; Peter Welinder; Bob McGrew; Dario Amodei; McCandlish, Sam; Ilya Sutskever; Wojciech Zaremba","We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, new evaluation set we release to measure functional correctness for synthesizing programs docstrings, our solves 28.8% the problems, while GPT-3 0% GPT-J 11.4%. Furthermore, find that repeated sampling is surprisingly effective strategy producing working solutions difficult prompts. Using this method, solve 70.2% problems with 100 samples per problem. Careful investigation reveals limitations, including difficulty docstrings describing long chains operations binding variables. Finally, discuss potential broader impacts deploying powerful generation technologies, covering safety, security, economics.",OpenAlex,2021-07-07
24,1.0,Paraphrasing with Large Language Models,Sam Witteveen; Martin Andrews,"Recently, large language models such as GPT-2 have shown themselves to be extremely adept at text generation and also been able achieve high-quality results in many downstream NLP tasks classification, sentiment analysis question answering with the aid of fine-tuning. We present a useful technique for using model perform task paraphrasing on variety texts subjects. Our approach is demonstrated capable generating paraphrases not only sentence level but longer spans paragraphs without needing break into smaller chunks.",OpenAlex,2019-01-01
4,0.82355136,A large language model for electronic health records,Xi Yang; Aokun Chen; Nima PourNejatian; Hoo Chang Shin; Kaleb E Smith; Christopher Parisien; Colin B. Compas; Cheryl Martin; Anthony Costa; Mona G. Flores; Ying Zhang; Tanja Magoc; Christopher A. Harle; Gloria Lipori; Duane A. Mitchell; William R. Hogan; Louis Shenkman; Jiang Bian; Yonghui Wu,"There is an increasing interest in developing artificial intelligence (AI) systems to process and interpret electronic health records (EHRs). Natural language processing (NLP) powered by pretrained models the key technology for medical AI utilizing clinical narratives. However, there are few models, largest of which trained domain comparatively small at 110 million parameters (compared with billions general domain). It not clear how large can help utilize unstructured EHRs. In this study, we develop from scratch a model-GatorTron-using >90 billion words text (including >82 de-identified text) systematically evaluate it on five NLP tasks including concept extraction, relation semantic textual similarity, natural inference (NLI), question answering (MQA). We examine (1) scaling up number (2) size training data could benefit these tasks. GatorTron scale model 8.9 improve (e.g., 9.6% 9.5% improvement accuracy NLI MQA), be applied healthcare delivery. The publicly available at: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og .",OpenAlex,2022-12-26
2,0.55371606,Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models,Tiffany H Kung; Morgan Cheatham; Arielle Medenilla; Czarina Sillos; Lorie De Leon; Camille Elepaño; Maria Madriaga; Rimel Aggabao; Giezel Diaz-Candido; James Maningo; Victor Tseng,"ABSTRACT We evaluated the performance of a large language model called ChatGPT on United States Medical Licensing Exam (USMLE), which consists three exams: Step 1, 2CK, and 3. performed at or near passing threshold for all exams without any specialized training reinforcement. Additionally, demonstrated high level concordance insight in its explanations. These results suggest that models may have potential to assist with medical education, potentially, clinical decision-making.",OpenAlex,2022-12-20
25,1.0,IRSTLM: an open source toolkit for handling large scale language models,Marcello Federico; Nicola Bertoldi; Mauro Cettolo,"Research in speech recognition and machine translation is boosting the use of large scale n-gram language models. We present an open source toolkit that permits to efficiently handle models with billions n-grams on conventional machines. The IRSTLM supports distribution ngram collection smoothing over a computer cluster, model compression through probability quantization, lazy-loading huge from disk. has been so far successfully deployed Moses for statistical FBK-irst system. Efficiency tool reported transcription task Italian political speeches using 1.1 billion four-grams.",OpenAlex,2008-09-22
26,1.0,Large language models generate functional protein sequences across diverse families,Ali Madani; Ben Krause; Eric R. Greene; Subu Subramanian; Benjamin P. Mohr; James M. Holton; J.L. Olmos; Caiming Xiong; Zachary Z. Sun; Richard Socher; James S. Fraser; Nikhil Naik,,OpenAlex,2023-01-26
27,1.0,Geant4—a simulation toolkit,S. Agostinelli; J. Allison; K. Amako; J. Apostolakis; H. M. Araújo; P. Arce; M. Asai; D. Axen; S. Banerjee; G. Barrand; F. Behner; L. Bellagamba; J. Boudreau; L. Broglia; A. Brunengo; H. Burkhardt; Stephane Chauvie; Joseph Chuma; R. Chytracek; Gene Cooperman; G. Cosmo; P. V. Degtyarenko; A. Dell’Acqua; G. Depaola; Dennis D. Dietrich; R. Enami; A. Feliciello; C. Ferguson; H. Fesefeldt; G. Folger; F. Foppiano; A. Forti; Sara Garelli; S. Giani; R. Giannitrapani; D. Gibin; J. J. Gomez Y Cadenas; I. D. Sandoval Gonzalez; G. Gracia Abril; G. Greeniaus; Walter Greiner; V. Grichine; A. Grossheim; Susanna Guatelli; P. Gumplinger; R. Hamatsu; K. Hashimoto; H. Hasui; Anna Maria Heikkinen; A. Howard; V. N. Ivanchenko; A. Johnson; F.W. Jones; Jan Kallenbach; N. Kanaya; Masahiro Kawabata; Y. Kawabata; M. Kawaguti; S. R. Kelner; Paul Kent; Akira Kimura; T. Kodama; R. P. Kokoulin; M. Kossov; H. Kurashige; E. Lamanna; T. Lampén; V. Lara; V. Lefebure; F. Lei; M. Liendl; W. S. Lockman; F. Longo; Simone Magni; M. Maire; E. Medernach; K. Minamimoto; P. Mora de Freitas; Y. Morita; K. Murakami; M. Nagamatu; R. Nartallo; P. Nieminen; T. Nishimura; K. Ohtsubo; M. Okamura; S. W. O'Neale; Y. Oohata; K. Paech; Joseph Perl; A. Pfeiffer; Maria Grazia Pia; F. Ranjard; A. M. Rybin; S. Sadilov; E. Di Salvo; G. Santin; Takashi Sasaki; N. Savvas; Yukimasa Sawada,"Geant4 is a toolkit for simulating the passage of particles through matter. It includes complete range functionality including tracking, geometry, physics models and hits. The processes offered cover comprehensive range, electromagnetic, hadronic optical processes, large set long-lived particles, materials elements, over wide energy starting, in some cases, from 250eV extending others to TeV range. has been designed constructed expose utilised, handle complex geometries, enable its easy adaptation optimal use different sets applications. result worldwide collaboration physicists software engineers. created exploiting engineering object-oriented technology implemented C++ programming language. used applications particle physics, nuclear accelerator design, space medical physics.",OpenAlex,2003-07-01
28,1.0,A systematic evaluation of large language models of code,Frank F. Xu; Uri Alon; Graham Neubig; Vincent J. Hellendoorn,"Large language models (LMs) of code have recently shown tremendous promise in completing and synthesizing from natural descriptions. However, the current state-of-the-art LMs (e.g., Codex) are not publicly available, leaving many questions about their model data design decisions. We aim to fill some these blanks through a systematic evaluation largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, CodeParrot, across various programming languages. Although Codex itself is open-source, we find that opensource do achieve close results languages, although targeted mainly for modeling. further identify an important missing piece form large open-source trained exclusively on multi-lingual corpus code. release new model, PolyCoder, with 2.7B parameters based GPT-2 architecture, was 249GB 12 languages single machine. In C language, PolyCoder outperforms all including Codex. Our available at https://github.com/VHellendoorn/Code-LMs, which enables future research application this area. online appendix https://arxiv.org/abs/2202.13169.",OpenAlex,2022-06-13
29,1.0,Large language models associate Muslims with violence,Abubakar Abid; Maheen Farooqi; James Zou,,OpenAlex,2021-06-17
30,1.0,Enriching Word Vectors with Subword Information,Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov,"Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is limitation, especially languages with vocabularies and rare words. In this paper, we propose new approach based skipgram model, where represented as bag character n-grams. A representation associated n-gram; words being sum these representations. Our method fast, allowing train quickly allows us compute did not appear in training data. We evaluate our nine different languages, both similarity analogy By comparing recently proposed morphological show vectors achieve state-of-the-art performance",OpenAlex,2017-12-01
31,1.0,FAMILIAR: A domain-specific language for large scale management of feature models,Mathieu Acher; Philippe Collet; Philippe Lahire; Robert France,"The feature model formalism has become the de facto standard for managing variability in software product lines (SPLs). In practice, developing an SPL can involve modeling a large number of features representing different viewpoints, sub-systems or concerns system. This activity is generally tedious and error-prone. this article, we present FAMILIAR Domain-Specific Language (DSL) that dedicated to scale management models complements existing tool support. language provides powerful support separating modeling, through provision composition decomposition operators, reasoning facilities scripting capabilities with modularization mechanisms. We illustrate how consisting medical imaging services be practically managed using reusable scripts implement also report on various usages applications its demonstrate their applicability domains use purposes.",OpenAlex,2013-06-01
32,1.0,Modeling and optimization with Optimica and JModelica.org—Languages and tools for solving large-scale dynamic optimization problems,Johan Åkesson; Karl-Erik Årzén; Magnus Gäfvert; Tove Bergdahl; Hubertus Tummescheit,"The Modelica language, targeted at modeling of complex physical systems, has gained increased attention during the last decade. is about to establish itself as a de facto standard in community with strong support both within academia and industry. While there are several tools, commercial free, supporting simulation models few efforts have been made area dynamic optimization models. In this paper, an extension entitled Optimica, reported. Optimica enables compact intuitive formulations problems, static dynamic, based on paper also reports novel Modelica-based open source project, JModelica.org, specifically optimization. JModelica.org supports offers platform established technologies, including Python, C, Java XML. Examples provided demonstrate capabilities JModelica.org.",OpenAlex,2010-11-01
33,1.0,Application of Lemmatization and Summarization Methods in Topic Identification Module for Large Scale Language Modeling Data Filtering,Lucie Zajícová,,Wikidata,2012-01-01
34,1.0,The Role and Limitations of Large Language Models Such as ChatGPT in                     Clinical Settings and Medical Journalism,Furkan Ufuk,,Wikidata,2023-05-01
35,1.0,Extracting Training Data from Large Language Models,Nicholas Carlini; Florian Tramèr; Colin Raffel; Ulfar Erlingsson; Tom B. Brown; Katherine Lee; Dawn Song; Adam Roberts; Alina Oprea; Eric Wallace; Ariel Herbert-Voss; Matthew Jagielski,,Wikidata,2020-12-14
36,1.0,Natural Language Processing in Large-Scale Neural Models for Medical Screenings,Catharina Marie Stille; Peter Blouw; Trevor Bekolay; Bernd J Kröger,,Wikidata,2019-08-02
37,1.0,Efficient Large Scale Language Modeling with Mixtures of Experts,Mikel Artetxe; Naman Goyal; Luke Zettlemoyer; Jeff Wang; Xian Li; Xing Zhou; Louis Martin; Mona Diab; Zornitsa Kozareva; Shuohui Chen; Jingfei Du; Myle Ott; Brian O'Horo; Giri Anantharaman; Halil Akin; Mandeep Baines; Punit Singh Koura; Ramakanth Pasunuru; Sam Shleifer; Shruti Bhosale; Srinivasan Iyer; Todor Mihaylov; Ves Stoyanov; Xi Victoria Lin,,Wikidata,2021-12-20
38,1.0,Associative processes between behavioral symbols and a large scale language model,Yoshihiko Nakamura; Wataru Takano,,Wikidata,2010-05-01
39,1.0,Gaussian mixture modeling of hemispheric lateralization for language in a large sample of healthy individuals balanced for handedness,Nathalie Tzourio-Mazoyer; Fabrice Crivello; Laure Zago; Gaël Jobard; Emmanuel Mellet; Laurent Petit; Marc Joliot; Guy Perchey; Bernard Mazoyer,,Wikidata,2014-06-30
40,1.0,Using Morphological Data in Language Modeling for Serbian Large Vocabulary Speech Recognition,Edvin Pakoci; Branislav Popović; Darko Pekar,,Wikidata,2019-03-03
41,1.0,SKILL: Structured Knowledge Infusion for Large Language Models,Martin Jaggi; Zhe Dong; Enrique Alfonseca; Fedor Moiseev,,Wikidata,2022-05-17
42,1.0,Can large language models reason about medical questions?,Ole Winther; Valentin Victor David Julien Lievin; Christoffer Egeberg Hother,,Wikidata,2022-07-17
43,1.0,Low-resource Bilingual Dialect Lexicon Induction with Large Language Models,Barbara Plank; Katya Artemova,,Wikidata,2023-05-01
44,1.0,Can artificial intelligence-strengthened ChatGPT or other large language models transform nucleic acid research?,Chiranjib Chakraborty; Sang-Soo Lee; Srijan Chatterjee; Manojit Bhattacharya,,Wikidata,2023-09-01
45,1.0,Large language models in anaesthesiology: use of ChatGPT for American Society of Anesthesiologists physical status classification,Daniel Yan Zheng Lim; Hairil Rizal Abdullah; Joshua Yi Min Tung; Gerald G.R. Sng; Jia X. Chai; Yu He Ke,,Wikidata,2023-07-01
46,1.0,Whole-proteome phylogeny of large dsDNA viruses and parvoviruses through a composition vector method related to dynamical language model,Ka Hou Chu; Li-Qian Zhou; Vo Anh; Zu-Guo Yu; Chi Pang Li; Roger Wei Wang,,Wikidata,2010-06-22
47,1.0,Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of ChatGPT and other large language models in scholarly peer review,Mohammad Hosseini; Serge Horbach,,Wikidata,2023-05-18
48,1.0,"Need an AI-Enabled, Next-Generation, Advanced ChatGPT or Large Language Models (LLMs) for Error-Free and Accurate Medical Information",Chiranjib Chakraborty; Manojit Bhattacharya; Sang-Soo Lee,,Wikidata,2023-06-27
49,1.0,Adversarial Training for Large Neural Language Models,Xiaodong Liu; Yu Wang; Hao Cheng; Pengcheng He; Jianfeng Gao; Weizhu Chen; Hoifung Poon,,Wikidata,2020-04-20
50,1.0,Automatic Topic Identification for Large Scale Language Modeling Data Filtering,Pavel Ircing; Aleš Pražák; Lucie Zajícová; Jan Lehečka,,Wikidata,2011-01-01
51,1.0,Strategies for Training Large Vocabulary Neural Language Models,Michael Auli; David Grangier; Welin Chen,,Wikidata,2015-01-01
52,1.0,Larger-Scale Transformers for Multilingual Masked Language Modeling,Alexis Conneau; Naman Goyal; Jingfei Du; Myle Ott; Giri Anantharaman,,Wikidata,2021-05-02
53,1.0,ChatGPT and large language model (LLM) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine,Mandy Rickard; Jin K Kim; Michael Chua; Armando Lorenzo,,Wikidata,2023-06-01
54,1.0,Implications of large language models such as <scp>ChatGPT</scp> for dental medicine,Markus Bernhard Blatz; Roland Weiger; Florin Eggmann; Nicola U. Zitzmann,,Wikidata,2023-04-05
55,1.0,A Domain-Specific Next-Generation Large Language Model (LLM) or ChatGPT is Required for Biomedical Engineering and Research,Chiranjib Chakraborty; Manojit Bhattacharya; Sang-Soo Lee; Soumen Pal,,Wikidata,2023-07-10
56,1.0,ChatGPT goes to the operating room: evaluating GPT-4 performance and its potential in surgical education and training in the era of large language models,Gyu-Seong Choi; Woo Yong Lee; Namkee Oh,,Wikidata,2023-01-01
57,1.0,"General framework for mining, processing and storing large amounts of electronic texts for language modeling purposes",Pavel Ircing; Aleš Pražák; Lucie Zajícová; Jan Švec; Jan Vavruška; Jan Hoidekr; Jan Lehečka; Petr Stanislav,,Wikidata,2013-07-24
5,0.8066712,Can Large Language Models Better Predict Software Vulnerability?,Charalampos Patrikakis; George Hurlburt; Evangelos Katsadouros,,Wikidata,2023-05-01
58,1.0,Risks and Benefits of Large Language Models for the Environment,Matthias C. Rillig; Marlene Ågerstrand; Uli Sauerland; Mohan Bi; Kenneth A. Gould,,Wikidata,2023-02-23
59,1.0,VetTag: improving automated veterinary diagnosis coding via large-scale language modeling,James Y Zou; Ashley Zehnder; Yuhui Zhang; Rodney L Page; Allen Nie,,Wikidata,2019-05-08
60,1.0,Chain of Thought Prompting Elicits Reasoning in Large Language Models,Dengyong Zhou; Quoc Viet Le; Dale Schuurmans; Xuezhi Wang; Jason Wei; Maarten Bosma; Ed Chi,,Wikidata,2022-01-28
61,1.0,"A prototype natural language interface to a large complex knowledge base, the Foundational Model of Anatomy",James F. Brinkley; Cornelius Rosse; Gregory Distelhorst; Vishrut Srivastava,,Wikidata,2003-01-01
4,0.82355136,A large language model for electronic health records,Jiang Bian; William Hogan; Elizabeth Shenkman; Yonghui Wu; Ying Zhang; Xi Yang; Gloria Lipori; Christopher A. Harle; Hoo Chang Shin; Cheryl Martin; Duane A. Mitchell; Anthony B. Costa; Tanja Magoc; Colin Compas; Mona G. Flores; Aokun Chen; Christopher Parisien; Kaleb E. Smith; Nima PourNejatian,,Wikidata,2022-12-26
3,0.6165474,Evaluating Large Language Models Trained on Code,Wojciech Zaremba; Alec Radford; Ilya Sutskever; Heewoo Jun; Girish Sastry; Nick Ryder; Dario Amodei; Alex Paino; Andrew N. Carr; Brooke Chan; Evan Morikawa; Fotios Chantzis; Harri Edwards; Heidy Khlaaf; Henrique Ponde de Oliveira Pinto; Jerry Tworek; Josh Achiam; Katie Mayer; Mira Murati; Mohammad Bavarian; Pamela Mishkin; Philippe Tillet; Raul Puri; Shantanu Jain; Suchir Balaji; William Hebgen Guss; Yuri Burda; Mikhail Pavlov; Jared Kaplan; Jie Tang; Mark Chen; Elizabeth Barnes; Alex Ray; Nicholas Joseph; Matthew Knight; Miles Brundage; Felipe Petroski Such; Peter Welinder; Igor Babuschkin; William Saunders; Gretchen Krueger; Michael Petrov; Vedant Misra; Matthias Plappert; Dave Cummings; Nikolas Tezak; Bob McGrew; Greg Brockman; Ariel Herbert-Voss; Christopher Hesse; Clemens Winter; Sam McCandlish; Scott Gray; Alethea Power; Qiming Yuan; Lukasz Kaiser; Jan Leike; Alex Nichol,,Wikidata,2021-07-14
62,1.0,Large language model (ChatGPT) as a support tool for breast tumor board,Yiftach Barash; Israel Cohen; Eli Konen; Eyal Klang; Miri Sklair-Levy; Vera Sorin; Nora Balint Lahat; Douglas B. Zippel,,Wikidata,2023-05-30
63,1.0,Are Large Language Models Our Limit Case?,"Klein, Lauren","Feature article published in Startwords, Issue 3: Parrots",Zenodo,2022-08-01
64,1.0,Annual Report Assessment Using Large Language Models,Sisodia Yogendra,"Annual Report Assessment Using Large Language Models
When researching a given business, annual reports are among the most dependable and thorough resources available. When things seem to be going smoothly, annual reports should still be used to assist develop ideas and investment decisions, as well as identify red flags and early warning signals of problems. 
The author of this study evaluated Bert and Roberta, two pre-trained language models for this job, and further fine-tuned them on Annual Reports. The author investigated zero-shot learning and few-shot learning using sentence-transformers for classification. The findings of the fine-tuned language models and further different classifiers are promising and will help financial analysts incorporate them into portfolio management and investment decision-making.
All work is based on Annual Report Assessment  Dataset from the Author
DOI : 10.5281/zenodo.7536332",Zenodo,2023-01-14
65,1.0,"ISSTA2023 Artifact for ""Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models""","Deng, Yinlin; Xia, Chunqiu Steven; Peng, Haoran; Yang, Chenyuan; Zhang, Lingming","This is the artifact for the ISSTA'2023 paper ""Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models"". For more information, please check our paper (https://arxiv.org/abs/2212.14834).
Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.
To address these limitations, we propose TitanFuzz – the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38%/50.84% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs. 
This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.",Zenodo,2023-05-28
66,1.0,XR and General Purpose AI: from values and principles to norms and standards,Laurynas Adomaitis; Alexei Grinbaum,"Ethical issues of AI systems are usually formulated through the lens of values and principles. However, European policy makers should go beyond merely listing such values and principles, because manufacturers may not immediately understand how to implement them in the design of AI systems. For the proposed EU regulation to be effective, we offer an operationalization of the values and principles in the form of suggested norms and standards. Here, we list new and emerging issues to supplement, enhance and update the Assessment List for Trustworthy Artificial Intelligence (ALTAI) developed by the High-Level Expert Group on AI. Based on our analysis, we formulate specific recommendations for AI regulation.",Zenodo,2023-02-22
67,1.0,The Challenges for Regulating Medical Use of ChatGPT and Other Large Language Models,"Minssen, Timo; Vayena, Effy; Cohen, Glenn","Plain language summary: This Viewpoint discusses how regulators across the world should approach the legal and ethical challenges, including privacy, device regulation, competition, intellectual property rights, cybersecurity, and liability, raised by the medical use of large language models.",Zenodo,2023-07-06
68,1.0,Citizen engagement in petroleum revenue management,"Ogbe, Michael; Ogbe, Michael",Dataset for citizen participation in petroleum revenue management.,Eudat,
69,1.0,SQL-PL4OCL: An Automatic Code Generator from OCL to SQL Procedural Language,Marina Egea; Carolina Dania,"Design models are widely spread as core artifacts in software engineering. Yet, a key problem is how to fulfill correctly these blueprint specifications when code components are developed. The best possible scenario occurs when a source modeling language can be perfectly linked to a target language of election. Namely, a well defined mapping bridges the gap between the source and the target language. Otherwise, manual encoding of the system design is cumbersome and error prone. In this setting, we introduce a SQL-PL code generator for OCL expressions that, in contrast to other proposals, is able to map OCL iterate and iterator expressions thanks to our use of stored procedures. More in detail, our source language is the Object Constraint Language (OCL), which nowadays is an ISO standard used to express constraints and queries in a textual notation on UML models. Our target language is the procedural language (PL) extension to the Structured Query Language (SQL). SQL is a special-purpose programming language designed for managing data in relational database management systems (RDBMS). The purpose of PL for SQL is to combine database language and procedural programming language. Although SQL is also an ISO standard, different RDBMS implement certain syntactic variations to the standard SQL notation. Thus, we had to adapt the implementation of our mapping to each of them. As implementation targets we selected MariaDB, PostgreSQL, and MS SQL Server. MariaDB and PostgreSQL were selected because they are open source and widely used by developers. MS SQL server was selected to be able to compare evaluation time from open source to commercial RDBMS. A variety of applications arises for a mapping from OCL to SQL expressions. Among others, there are three prominent types. These are i) evaluation of OCL expressions (analysis queries and metrics) on large model's instances, ii) identification of constraints during data modeling that have to be checked as integrity constraints on actual data; iii) automatic code generation from models. Indeed, our implementation was used as a key component of a toolkit that automatically generated ready-to-deploy web applications for secure data management from design models. Our component mapped and evaluated OCL constraints specified within authorization policies. Our code generator is defined recursively over the structure of OCL expressions and it is implemented in the SQL-PL4OCL tool that is publicly available at [1]. The seminal work of the mapping presented here can be found in [2], [3]. The key idea that enables the mapping from OCL iterator expressions to iterative stored procedures remains the same, but the work detailed in [4] introduces a novel mapping from OCL expressions to SQL-PL stored procedures. In the novel mapping we have taken design decisions which have facilitated the recursive definition of the code generator and simplified its definition. These decisions have also helped to significantly decrease the time required for the evaluation of the code generated. Regarding semantics, the new mapping is able to deal properly with the three-valued evaluation semantics of OCL. In addition, our original work and implementation was intended only for the procedural extension of MySQL, while our new definition eased the implementation of the mapping into other relational database management systems. In turn, we can now evaluate the resulting code using different RDBMS, which permits us to widen our discussion regarding efficiency in terms of evaluation-time of the code produced by SQL-PL4OCL tool.",IEEE,17-22 Sept. 2017
70,1.0,Towards Scalable Validation of Low-Code System Models: Mapping EVL to VIATRA Patterns,Qurat Ul Ain Ali; Benedek Horváth; Dimitris Kolovos; Konstantinos Barmpis; Ákos Horváth,"Adoption of low-code engineering in complex enterprise applications also increases the size of the underlying models. In such cases, the increasing complexity of the applications and the growing size of the underlying artefacts, various scalability challenges might arise for low-code platforms. Task-specific programming languages, such as OCL and EOL, are tailored to manage the underlying models. Existing model management languages have significant performance impact when it comes to complex queries operating over large-scale models reaching magnitudes of millions of elements in size. We propose an approach for automatically mapping expressions in Epsilon validation programs to VIATRA graph patterns to make the validation of large-scale low-code system models scalable by leveraging the incremental execution engine of VIATRA. Finally, we evaluate the performance of the proposed approach on large Java models of the Eclipse source code. Our results show performance speed-up up to 1481x compared to the sequential execution in Epsilon.",IEEE,10-15 Oct. 2021
71,1.0,Heterogeneous Model Query Optimisation,Qurat Ul Ain Ali,"With the growing size and complexity of software systems, the underlying models also grow in size proportionally. These large-scale models pose scalability issues for model-driven engineering technologies. These models can be persisted in various backend technologies (such as file systems, document and relational databases) and can be represented in different formats such as XMI and Flexmi. Several tailored high-level model management languages such as OCL and EOL enable developers to work on different backend technologies in a uniform way by shielding them from the complexities of different backends. On the contrary, performance with respect to execution time in tailored model management languages programs becomes one of the major scalability bottlenecks. In this work, we propose an architecture built on top of existing model query languages to facilitate query optimisation. The proposed approach will benefit from compile-time static analysis and automatic program rewriting to optimise queries operating over heterogeneous backend technologies. Optimisation strategies and performance will vary depending on the type of queries and the backend modelling technology. We expect to significantly improve performance (decrease in one order of magnitude of execution time) for model management programs, particularly over large-scale models.",IEEE,10-15 Oct. 2021
72,1.0,Executable Modelling for Highly Parallel Accelerators,Lorenzo Addazi; Federico Ciccozzi; Björn Lisper,"High-performance embedded computing is developing rapidly since applications in most domains require a large and increasing amount of computing power. On the hardware side, this requirement is met by the introduction of heterogeneous systems, with highly parallel accelerators that are designed to take care of the computation-heavy parts of an application. There is today a plethora of accelerator architectures, including GPUs, many-cores, FPGAs, and domain-specific architectures such as AI accelerators. They all have their own programming models, which are typically complex, low-level, and involve explicit parallelism. This yields error-prone software that puts the functional safety at risk, unacceptable for safety-critical embedded applications. In this position paper we argue that high-level executable modelling languages tailored for parallel computing can help in the software design for high performance embedded applications. In particular, we consider the data-parallel model to be a suitable candidate, since it allows very abstract parallel algorithm specifications free from race conditions. Moreover, we promote the Action Language for fUML (and thereby fUML) as suitable host language.",IEEE,15-20 Sept. 2019
73,1.0,TrueChange (TM) Under the Hood: How We Check the Consistency of Large Models (Almost) Instantly,Hugo Lourenço; Rui Eugénio,"The OutSystems Platform is a visual model-driven development and delivery platform that allows developers to create enterprise-grade web and mobile applications. The models created with the platform are translated by its compiler into a set of standard-technology artifacts (C#, JavaScript, SQL, etc). The model must be checked for consistency (i.e., that it is well-formed and well-typed) before compilation can proceed. Our Integrated Development Environment (IDE) does this in real-time: after each change made a developer, the IDE either automatically heals the other parts of the model that are impacted by the change, or provides immediate feedback on the errors that must be manually corrected. It is not uncommon for an OutSystems model to contain in excess of 200,000 individual elements. Handling large models efficiently is thus of paramount importance: consistency checks must run as fast as possible, otherwise the developer's experience is significantly impaired. In this paper we present the techniques we have developed to speed up consistency checks, and which resulted in the TrueChangeTM engine. We use an incremental approached paired with automatically managed back pointers. We believe these techniques are of general application and not limited to our particular case.",IEEE,15-20 Sept. 2019
74,1.0,Unifying multi-level modeling: A position paper,Manfred A. Jeusfeld; Ulrich Frank,"Multi-level modeling (MLM) as part of object-oriented modeling aims at fully utilizing the expressive power of multiple abstraction levels. While these levels where initially used to define domain-specific modeling languages, i.e. for linguistic purposes, the MLM community has long argued that there is much more to gain by tapping into ontological abstraction levels. While MLM is a rather specialized research field, there are now quite a number of different proposals. There is thus an opportunity to develop a uniform core of MLM that then possibly can become part of a standard and be taken up by the larger modeling community.",IEEE,10-15 Oct. 2021
75,1.0,Text conditioning and statistical language modeling for Romanian language,J. Domokos; G. Toderean; O. Buza,"In this paper we present a synthesis of the theoretical fundamentals and some practical aspects of statistical (n-gram) language modeling which is a main part of a large vocabulary statistical speech recognition system. There are presented the unigram, bigram and trigram language models as well as the Good-turing estimator based Katz back-off smoothing algorithm. There is also described the perplexity measure of a language model used for evaluation. The practical experiments were made on Romanian constitution corpus. There are also presented the text normalization steps before the language model generation. The results are ARPA-MIT format language models for Romanian language. The models were tested and compared using perplexity measure. Finally some comparisons were made between Romanian and English language modeling and conclusions are drawn.",IEEE,18-21 June 2009
76,1.0,Modeling Relationships Between Feature Model Views,Gökhan Kahraman; Loek Cleophas; Ramon Schiffelers,"The development of high-tech systems involves several models and artifacts, each focusing on one or more aspects or parts of the system. In product lines of such systems, managing the common and variable characteristics of these development artifacts by means of feature models gets complex due to the large number of features and constraints. Using multiple feature models, each with reduced number of features relevant only for specific artifacts, has been identified as a possible solution to deal with this complexity. However, the advantages of explicit representation of relationships between multiple feature models remain unexploited. These relationships need to be represented and managed explicitly in order to improve understanding of the complete variability model and support maintainability of the variability model under evolution. In this paper, we propose a modeling language that can be overlaid, non-intrusively, on existing multiple feature models to express the relationships between feature models. The language uses the concept of views to represent different aspects of the product line variability model, each view corresponding to a feature model represents variability information in one or more development artifacts of the product line. We give a metamodel for relationships between feature model views and provide the semantics of the relationships. We present how proposed relationship types can be used to analyze the impact of a change on the complete variability model. Furthermore, we provide formal definitions of relationship types, making them suitable for existing automated analysis operations in the literature.",IEEE,10-15 Oct. 2021
77,1.0,Leveraging Large Language Models With Vocabulary Sharing For Sign Language Translation,Huije Lee; Jung-Ho Kim; Eui Jun Hwang; Jaewoo Kim; Jong C. Park,"Sign language translation (SLT) is a task that provides translation between spoken and sign languages used in the same country, which tend to show high lexical similarity but low syntactic similarity. The recent emergence of large language models (LLMs) has been remarkable for all downstream tasks in natural language processing, but they have yet to be applied to SLT. In this paper, we explore how to use an LLM with vocabulary sharing for two gloss-based SLT tasks (text-to-gloss (T2G) and gloss-to-text (G2T)) on the NIASL2021 dataset, which consists of 180,848 preprocessed Korean and Korean Sign Language (KSL) sentence pairs. The experimental results showed that Ko-GPT-Trinity-1.2B+VS, a GPT-3-based SLT model with vocabulary sharing, outperformed other SLT models, achieving BLEU-4 scores of 22.06 and 45.89 on T2G and G2T tasks, respectively. We expect that the adoption of an LLM with vocabulary sharing will significantly lessen the resource scarcity problem of SLT.",IEEE,4-10 June 2023
78,1.0,Large Margin Training for Long Short-Term Memory Neural Networks in Neural Language Modeling,Zhizhong Ma; Junbo Ma; Xijuan Liu; Feng Hou,"Language models based on Long Short-Term Memory (LSTM) neural networks have been widely applied to automatic speech recognition, natural language processing research. However, current LSTMs are trained by employing the cross-entropy loss function, which only considers the target category without considering the competing categories during the training processes. Thus, current training methods cannot fully exploit the discriminative information provided by the data labels. To tackle this problem, we propose a Large Margin Long Short-Term Memory Neural Network (LMLSTM) model in this paper. Our model employs the large margin discriminative principle as a heuristic term to navigate the convergence process during training. Our model has improved the discriminative ability of the original LSTM while maintaining the capability when generating sequential data. Our proposed large margin term was tested on the Penn Treebank corpus language modelling task. Experimental results demonstrate that the proposed LMLSTM model outperforms current LSTM models in terms of accuracy and perplexity without increasing the depth.",IEEE,19-21 Aug. 2022
79,1.0,Improved mixed language speech recognition using asymmetric acoustic model and language model with code-switch inversion constraints,Ying Li; Pascale Fung,"We propose an integrated framework for large vocabulary continuous mixed language speech recognition that handles the accent effect in the bilingual acoustic model and the inversion constraint well known to linguists in the language model. Our asymmetric acoustic model with phone set extension improves upon previous work by striking a balance between data and phonetic knowledge. Our language model improves upon previous work by (1) using the inversion constraint to predict code switching points in the mixed language and (2) integrating a code-switch prediction model, a translation model and a reconstruction model together. This integration means that our language model avoids the pitfall of propagated error that could arise from decoupling these steps. Finally, a WFST-based decoder integrates the acoustic models, code-switch language model and a monolingual language model in the matrix language all together. Our system reduces word error rate by 1.88% on a lecture speech corpus and by 2.43% on a lunch conversation corpus, with statistical significance, over the conventional bilingual acoustic model and interpolated language model.",IEEE,26-31 May 2013
80,1.0,MAPLE-T: A Tool for Process Enactment with Traceability Support,Omar Hassane; Sadaf Mustafiz; Ferhat Khendek; Maria Toeroe,"Automating the enactment of processes using model-driven methods and tools paves the way for streamlining or optimizing these processes. Establishing traceability in automated processes is instrumental in carrying out analysis of the process and the involved artifacts. In this paper, we present MAPLE-T, an integrated process modelling and enactment environment with traceability information generation and analysis support built into Eclipse Papyrus. A process model (PM) defined as an Activity Diagram has associated model transformations implementing the various actions in the process. Enactment of the PM is carried out with the use of model transformation chaining in cooperation with model management means, in particular, megamodelling. We have incorporated both traceability in the small (at the model transformation level) and traceability in the large (at the PM level) features in our tool. The traceability information is retained in the megamodel and forms the basis for traceability analysis of the enacted process. We have built a change impact analysis module within MAPLE-T which allows the impact of a change in a model part of the PM to be assessed with the help of the derived megamodel. We demonstrate the use of MAPLE-T on a case study in the network functions virtualization (NFV) application domain. https://users.encs.concordia.ca/~magic/maplet-demo.php.",IEEE,15-20 Sept. 2019
81,1.0,A syntactic language model based on incremental CCG parsing,Hany Hassan; Khalil Sima'an; Andy Way,"Syntactically-enriched language models (parsers) constitute a promising component in applications such as machine translation and speech-recognition. To maintain a useful level of accuracy, existing parsers are non-incremental and must span a combinatorially growing space of possible structures as every input word is processed. This prohibits their incorporation into standard linear-time decoders. In this paper, we present an incremental, linear-time dependency parser based on Combinatory Categorial Grammar (CCG) and classification techniques. We devise a deterministic transform of CCG-bank canonical derivations into incremental ones, and train our parser on this data. We discover that a cascaded, incremental version provides an appealing balance between efficiency and accuracy.",IEEE,15-19 Dec. 2008
6,0.9101664,"Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models",Paula Maddigan; Teo Susnjak,"The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.",IEEE,2023
82,1.0,Large-Scale Distributed Language Modeling,Ahmad Emami; Kishore Papineni; Jeffrey Sorensen,"A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score.",IEEE,15-20 April 2007
83,1.0,Associative processes between behavioral symbols and a large scale language model,Wataru Takano; Yoshihiko Nakamura,"This paper describes an novel approach towards linguistic processing for robots through integration of a motion language model and a natural language model. The motion language model works for association of words from motion symbols. The natural language model is one used for a morphological analysis, which has been developed in natural language community. The natural language model is optimized using a enormous amount of words. So this model is scalable architecture. The motion language model and the natural language model can be integrated since both models are represented graphically. The integration of the motion language model and the natural language model allows robots not only to interpret motion patterns as sentences but also to generate motions from sentences. This paper demonstrates the validity of our proposed framework even in the case that large-scale word corpus is needed processing through experiments of interpreting motion patterns as sentences and generating motion patterns from sentences.",IEEE,3-7 May 2010
84,1.0,An Automated Tool for Generating UML Models from Natural Language Requirements,Deva Kumar Deeptimahanti; Muhammad Ali Babar,"This paper describes a domain independent tool, named, UML Model Generator from Analysis of Requirements (UMGAR), which generates UML models like the Use-case Diagram, Analysis class model, Collaboration diagram and Design class model from natural language requirements using efficient Natural Language Processing (NLP) tools. UMGAR implements a set of syntactic reconstruction rules to process complex requirements into simple requirements. UMGAR also provides a generic XMI parser to generate XMI files for visualizing the generated models in any UML modeling tool. With respect to the existing tools in this area, UMGAR provides more comprehensive support for generating models with proper relationships, which can be used for large requirement documents.",IEEE,16-20 Nov. 2009
85,1.0,AI Computing in Large-Scale Era: Pre-trillion-scale Neural Network Models and Exa-scale Supercomputing,Bor-Sung Liang,"The development of AI computing has reached a critical inflection point. The scale of large-scale AI neural network model parameters has grown rapidly to “pre-trillion-scale” level. The computing needs of training large-scale AI neural network models have reached “exa-scale” level. Besides, AI Foundation Model also affects the correctness of AI applications, and becoming a new information security issue. Future AI development will be pushed by progress of computing power (supercomputer), algorithm (neural network model and parameter scale), and application (foundation model and downstream fine tuning). In particular, the computational efficiency of AI will be a key factor in the commercialization and popularization of AI applications.",IEEE,17-20 April 2023
86,1.0,Text generation by probabilistic suffix tree language model,Sanparith Marukatat,"During last decade, language modeling has been dominated by neural structures; RNN, LSTM or Transformer. These neural language models provide excellent performance to the detriment of very high computational cost. This work investigates the use of probabilistic language model that requires much less computational cost. In particular, we are interested in variable-order Markov model that can be efficiently implemented on a probabilistic suffix tree (PST) structure. The PST construction is cheap and can be easily scaled to very large dataset. Experimental results show that this model can be used to generated realistic sentences.",IEEE,21-23 Dec. 2021
87,1.0,Autoregressive Self-Evaluation: A Case Study of Music Generation Using Large Language Models,Berker Banar; Simon Colton,"Autoregressive models have shown significant success in many tasks such as natural language generation and music composition. However, generic training mechanisms with off-the-shelf loss functions (e.g. cross-entropy), where not much attention is paid to the specifics of the task, do not necessarily guarantee success as different data modalities (e.g. text, visuals, music) exhibit different natures. In this study, we present a novel autoregressive self-evaluation framework to assess the performance of autoregressive models with both domain-agnostic and domain-specific metrics. We demonstrate this strategy with a case study of music generation using GPT-2 within a transfer learning paradigm. We contrast and compare the effects of fundamental parameters in autoregressive generation such as the temperature in sampling and the length of the generated sequence.",IEEE,5-6 June 2023
88,1.0,Analyzing various topic modeling approaches for Domain-Specific language model,Disha Kaur Phull; G. Bharadwaja Kumar,"In recent times, topic modeling approaches for adaptive language modeling have been extensively explored for Natural Language Processing applications such as machine translation, speech recognition etc. Language model is extremely fragile in adapting towards the required domain, so it needs to be channeled towards an area or a topic for producing optimal results. This paves the need to investigate various topic modeling approaches which are used to infer knowledge from a large corpora. In this paper, we mileage various topic modeling techniques which include Latent Semantic Indexing, Latent Dirichlet Allocation and Hierarchical Dirichlet Process. In this process, the baseline language model is dynamically adapted to different topics and the results are analyzed for these three topic modeling approaches.",IEEE,20-22 July 2017
89,1.0,The action language: refining a behavioral modeling language,S. Nordstrom; Shweta Shetty; Di Yao; Shikha Ahuja; Sandeep Neema; T. Bapty,"When modeling large-scale complex system behavior we believe there is merit in abandoning the once fashionable all-encompassing system modeling approach in favor of developing a set of models created in a variety of loosely coupled modeling languages. Loosely coupled languages are those that are considered to be nearly orthogonal with respect to their intention, but in reality may display a limited degree of trivial dependencies. We apply this approach toward component behavioral modeling to show how core behaviors of software components can be specified using a statecharts-like behavioral modeling language while elements of the behavior associated with implementation- or platform-specific concepts are modeled using a wholly separate language.",IEEE,4-7 April 2005
90,1.0,Hierarchical Bayesian Language Models for Conversational Speech Recognition,Songfang Huang; Steve Renals,"Traditional $n$ -gram language models are widely used in state-of-the-art large vocabulary speech recognition systems. This simple model suffers from some limitations, such as overfitting of maximum-likelihood estimation and the lack of rich contextual knowledge sources. In this paper, we exploit a hierarchical Bayesian interpretation for language modeling, based on a nonparametric prior called Pitman–Yor process. This offers a principled approach to language model smoothing, embedding the power-law distribution for natural language. Experiments on the recognition of conversational speech in multiparty meetings demonstrate that by using hierarchical Bayesian language models, we are able to achieve significant reductions in perplexity and word error rate.",IEEE,Nov. 2010
91,1.0,Large Vocabulary Continuous Speech Recognition System for Marathi,Snehal Chandulal Bajaj; Kamal Kant; Amol V Bole; Pranaw Kumar,"Lots of efforts have been made in the development of Automatic Speech Recognition systems for Indian languages, and good progress has been made for Hindi, Tamil, Telugu & Bengali, and Indian English. However, practically usable robust systems are still not available for many regional Indian languages including Marathi. This paper presents an effort toward the development of a large vocabulary continuous speech recognition system for Marathi and its integration into practical application, such as Speech-based typing system. This speech recognition system has been developed using the Kaldi toolkit. The Kaldi toolkit facilitates the exploration of different acoustic models such as Gaussian Mixture Model with Hidden Markov Model (GMM-HMM) and Deep Neural Networks with Hidden Markov Model (DNN-HMM). In this paper we have developed Marathi Speech Recognition system which is trained around 95 hours of audio data. We have used the trigram language model which is having 29 lakhs of words and 3 lakhs of vocabulary size. This system gives average word error rate of 19%.",IEEE,19-21 April 2023
92,1.0,Towards a large-vocabulary French vocal dictation based on a size-independent language-model search using the INRS recognizer,H. Tolba; D. O'Shaughnessy,"Reports the progress of the large-vocabulary French-speech vocal dictation studies at INRS-Te/spl acute/le/spl acute/com. To evaluate such progress, the hidden Markov model (HMM) based recognizer of INRS is used. This recognizer, which represents each phone using HMMs, uses context-dependent phone modeling and n-gram statistics in order to cope with both coarticulation and phonological phenomena, respectively. A series of experiments on speaker-independent continuous-speech recognition have been carried out using a subset of the large read-speech French-language corpus, BREF, containing recordings of texts selected from the French newspaper Le Monde. We show through experiments that using a lexical graph that ignores the language model states and homophone distinctions and postponing the application of such knowledge to a post-processor simplifies the recognition process while keeping its high accuracy. The word recognition rate, using gender-dependent vector quantization (VQ) models, a 20,000-word pronunciation variants-based lexicon and a bigram model estimated using Le Monde text data, was found to be 91.62% for males and 90.98% for females.",IEEE,5-9 June 2000
93,1.0,Joint generative and discriminative models for spoken language understanding,Marco Dinarelli; Alessandro Moschitti; Giuseppe Riccardi,"Spoken Language Understanding aims at mapping a natural language spoken sentence into a semantic representation. In the last decade two main approaches have been pursued: generative and discriminative models. The former is more robust to overfitting whereas the latter is more robust to many irrelevant features. Additionally, the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task. In this paper we describe a training framework where both models are used: a generative model produces a list of ranked hypotheses whereas a discriminative model, depending on string kernels and Support Vector Machines, re-ranks such list. We tested such approach on a new corpus produced in the European LUNA project. The results show a large improvement on the state-of-the-art in concept segmentation and labeling.",IEEE,15-19 Dec. 2008
94,1.0,A model-driven middleware approach to reduce the semantic gap between application domains and the generic infrastructure of smart cities,Paulo César F. Melo,"In Smart Cities, the city’s IT infrastructure must facilitate the development, deployment and operation of smart city applications for a number of different application domains that need to access a variety of heterogeneous resources. Building these applications becomes a great challenge since: they are used on large scale systems, have dynamic behavior, must access heterogeneous resources, can share resources from different domains, and must deal with the semantics of different domains. In this sense, platforms are commonly used to support application development, but they do not reduce complexity, are generic and deal with resource abstractions, while applications deal with domain abstractions. To overcome this semantic gap, this work proposes the use of domain specific middleware (DSM) with support for the development of model-driven applications defined in terms of a domain specific modeling language (DSML). To facilitate the definition of languages in different domains and, consequently, the execution of applications built according to these languages, a meta-meta language was defined that wraps the generic concepts used for the management of several domains. Furthermore, operational semantics are represented by models that describe domain-specific operations. The set of these elements associated with a middleware built according to the model-driven approach is defined as Model-Driven Domain-Specific Middleware (MDDSM). Therefore, this work proposes an integrated architecture that combines the use of MD-DSM and a smart cities platform to provide a flexible, robust and complete approach to facilitating the creation and execution of applications for different smart city domains. Thus, in the context of Smart Cities, middleware must deal with both application domain-related abstractions and platform-related abstractions (resources). Furthermore, the use of the model-driven approach allows representing such abstractions and relating domain semantics to generic resources. As a result, we have the construction of middleware and related elements in different domains and associated with a smart cities platform, demonstrating the expressiveness of the proposed approach.",IEEE,10-15 Oct. 2021
95,1.0,Towards efficient support for executing the Object Constraint Language,P. Collet; R. Rousseau,"The Object Constraint Language (OCL) forms part of the UML notation as a language to complete graphical models by expressing precise constraints or assertions. As OCL is developed as a non-executable language, expressed properties cannot be embedded as executable assertions in the resulting implementations to provide correctness testing. Nonetheless a large part of OCL seems to be easily executable, but straightforward implementations would be inefficient and detrimental to the approach. The paper proposes a pragmatic solution for an OCL runtime support and determines the origins of potential inefficiency. The evaluation of assertions is streamlined according to their roles and the possibility of sampling quantified assertions. The triggering of assertions is driven by a changed based system that simplifies large scale use while ensuring that unstable parts undergo more controls.",IEEE,5-5 Aug. 1999
96,1.0,"No more strings, please",Kevin Knight,"Summary form only given. In natural language research, many (grammar) trees were felled in 1992, to make room for the highly successful string-based HMM industry. A small literature survived on parsing (putting a tree on a string) and syntactic language modeling (putting a weight on a string). However, trees are making a comeback. Tree transformations are turning out to be very useful in large-scale machine translation (MT), and we will cover recent developments in this area. Most of the tree techniques used in MT turn out to be generic, leading to tools and software for manipulating tree automata in general. Tree acceptors and transducers generalize HMM techniques to the world of trees, raising many interesting theoretical and practical problems.",IEEE,10-13 Dec. 2006
97,1.0,Converting Neural Network Language Models into Back-off Language Models for Efficient Decoding in Automatic Speech Recognition,Ebru Arısoy; Stanley F. Chen; Bhuvana Ramabhadran; Abhinav Sethy,"Neural network language models (NNLMs) have achieved very good performance in large-vocabulary continuous speech recognition (LVCSR) systems. Because decoding with NNLMs is computationally expensive, there is interest in developing methods to approximate NNLMs with simpler language models that are suitable for fast decoding. In this work, we propose an approximate method for converting a feedforward NNLM into a back-off n-gram language model that can be used directly in existing LVCSR decoders. We convert NNLMs of increasing order to pruned back-off language models, using lower-order models to constrain the n-grams allowed in higher-order models. In experiments on Broadcast News data, we find that the resulting back-off models retain the bulk of the gain achieved by NNLMs over conventional n-gram language models, and give accuracy improvements as compared to existing methods for converting NNLMs to back-off models. In addition, the proposed approach can be applied to any type of non-back-off language model to enable efficient decoding.",IEEE,Jan. 2014
98,1.0,Phrase-based data selection for language model adaptation in spoken language translation,Shixiang Lu; Wei Wei; Xiaoyin Fu; Lichun Fan; Bo Xu,"In this paper, we propose an unsupervised phrase-based data selection model, address the problem of selecting no-domain-specific language model (LM) training data to build adapted LM for use. In spoken language translation (SLT) system, we aim at finding the LM training sentences which are similar to the translation task. Compared with the traditional bag-of-words models, the phrase-based data selection model is more effective because it captures contextual information in modeling the selection of phrase as a whole, rather than selection of single words in isolation. Large-scale experimental results demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and translation performance, respectively.",IEEE,5-8 Dec. 2012
99,1.0,Position Paper: Knowledge Sharing and Distances in Collaborative Modeling,Rodi Jolak; Grischa Liebel,"To develop systems effectively, a shared system understanding is required. Collaborative modeling is one way to capture this shared understanding. Increasingly, in large systems engineering projects different distances lead to social barriers between stakeholders. These barriers affect the quantity and quality of knowledge that is shared between stakeholders, thus reducing the quality of the resulting product. While it has been proposed to limit modeling activities to co-located teams, this might not always be possible or feasible. We argue that, despite the technological advances in collaborative modeling, effective collaboration can only be achieved if we understand how to account for social barriers. We propose to study, in depth, how these barriers affect modeling, and how their effects can be reduced. By understanding the effects of social barriers and accounting for them, we can maximize the benefits of collaborative modeling.",IEEE,15-20 Sept. 2019
100,1.0,Joint Morphological-Lexical Language Modeling for Processing Morphologically Rich Languages With Application to Dialectal Arabic,Ruhi Sarikaya; Mohamed Afify; Yonggang Deng; Hakan Erdogan; Yuqing Gao,"Language modeling for an inflected language such as Arabic poses new challenges for speech recognition and machine translation due to its rich morphology. Rich morphology results in large increases in out-of-vocabulary (OOV) rate and poor language model parameter estimation in the absence of large quantities of data. In this study, we present a joint morphological-lexical language model (JMLLM) that takes advantage of Arabic morphology. JMLLM combines morphological segments with the underlying lexical items and additional available information sources with regards to morphological segments and lexical items in a single joint model. Joint representation and modeling of morphological and lexical items reduces the OOV rate and provides smooth probability estimates while keeping the predictive power of whole words. Speech recognition and machine translation experiments in dialectal-Arabic show improvements over word and morpheme based trigram language models. We also show that as the tightness of integration between different information sources increases, both speech recognition and machine translation performances improve.",IEEE,Sept. 2008
101,1.0,A study on multilingual acoustic modeling for large vocabulary ASR,Hui Lin; Li Deng; Dong Yu; Yi-fan Gong; Alex Acero; Chin-Hui Lee,"We study key issues related to multilingual acoustic modeling for automatic speech recognition (ASR) through a series of large-scale ASR experiments. Our study explores shared structures embedded in a large collection of speech data spanning over a number of spoken languages in order to establish a common set of universal phone models that can be used for large vocabulary ASR of all the languages seen or unseen during training. Language-universal and language-adaptive models are compared with language-specific models, and the comparison results show that in many cases it is possible to build general-purpose language-universal and language-adaptive acoustic models that outperform language-specific ones if the set of shared units, the structure of shared states, and the shared acoustic-phonetic properties among different languages can be properly utilized. Specifically, our results demonstrate that when the context coverage is poor in language-specific training, we can use one tenth of the adaptation data to achieve equivalent performance in cross-lingual speech recognition.",IEEE,19-24 April 2009
102,1.0,Uniform internal model for hybrid language description,Zhu Ming; Bian Jinian; Xue Hongxi,"The HDL language, which is for hardware description, is not suitable for complex algorithmic programming, and while a high level language such as C describes algorithmic function effectively, it suffers when used for logic synthesis and verification. Development of system on a chip (SOC) leads to more and more components to be integrated into one chip. This paper represents a method which can deal with hardware components and algorithmic modules together through describing original units with C or VHDL and creating a uniform internal model, IIR (internal intermediate representation). The IIR internal model saves as an XML (Extensible Markup Language) format file externally. IIR can be used for partitioning, synthesis, and verification. By using the uniform model, analyzing source files repeatedly is avoided and the efficiency of research is improved.",IEEE,29 June-1 July 2002
103,1.0,Large vocabulary speech recognition with multispan statistical language models,J.R. Bellegarda,"Multispan language modeling refers to the integration of various constraints, both local and global, present in the language. It was recently proposed to capture global constraints through the use of latent semantic analysis, while taking local constraints into account via the usual n-gram approach. This has led to several families of data-driven, multispan language models for large vocabulary speech recognition. Because of the inherent complementarity in the two types of constraints, the multispan performance, as measured by perplexity, has been shown to compare favorably with the corresponding n-gram performance. The objective of this work is to characterize the behavior of such multispan modeling in actual recognition. Major implementation issues are addressed, including search integration and context scope selection. Experiments are conducted on a subset of the Wall Street Journal (WSJ) speaker-independent, 20000-word vocabulary, continuous speech task. Results show that, compared to standard n-gram, the multispan framework can lead to a reduction in average word error rate of over 20%. The paper concludes with a discussion of intrinsic multi-span tradeoffs, such as the influence of training data selection on the resulting performance.",IEEE,Jan. 2000
104,1.0,Pretrained models and evaluation data for the Khmer language,Shengyi Jiang; Sihui Fu; Nankai Lin; Yingwen Fu,"Trained on a large corpus, pretrained models (PTMs) can capture different levels of concepts in context and hence generate universal language representations, which greatly benefit downstream natural language processing (NLP) tasks. In recent years, PTMs have been widely used in most NLP applications, especially for high-resource languages, such as English and Chinese. However, scarce resources have discouraged the progress of PTMs for low-resource languages. Transformer-based PTMs for the Khmer language are presented in this work for the first time. We evaluate our models on two downstream tasks: Part-of-speech tagging and news categorization. The dataset for the latter task is self-constructed. Experiments demonstrate the effectiveness of the Khmer models. In addition, we find that the current Khmer word segmentation technology does not aid performance improvement. We aim to release our models and datasets to the community in hopes of facilitating the future development of Khmer NLP applications.",IEEE,Aug. 2022
105,1.0,Word Topical Mixture Models for Dynamic Language Model Adaptation,Hsuan-Sheng Chiu; Berlin Chen,"This paper considers dynamic language model adaptation for Mandarin broadcast news recognition. A word topical mixture model (TMM) is proposed to explore the co-occurrence relationship between words, as well as the long-span latent topical information, for language model adaptation. The search history is modeled as a composite word TMM model for predicting the decoded word. The underlying characteristics and different kinds of model structures were extensively investigated, while the performance of word TMM was analyzed and verified by comparison with the conventional probabilistic latent semantic analysis-based language model (PLSALM) and trigger-based language model (TBLM) adaptation approaches. The large vocabulary continuous speech recognition (LVCSR) experiments were conducted on the Mandarin broadcast news collected in Taiwan. Very promising results in perplexity as well as character error rate reductions were initially obtained.",IEEE,15-20 April 2007
106,1.0,Language-model look-ahead for large vocabulary speech recognition,S. Ortmanns; H. Ney; A. Eiden,"Presents an efficient look-ahead technique which incorporates the language model knowledge at the earliest possible stage during the search process. This so-called language model look-ahead is built into the time-synchronous beam search algorithm using a tree-organized pronunciation lexicon for a bigram language model. The language model look-ahead technique exploits the full knowledge of the bigram language model by distributing the language model probabilities over the nodes of the lexical tree for each predecessor word. We present a method for handling the resulting memory requirements. The recognition experiments performed on the 20,000-word North American Business task (Nov. 1996) demonstrate that, in comparison with the unigram look-ahead, a reduction by a factor of 5 in the acoustic search effort can be achieved without loss in recognition accuracy.",IEEE,3-6 Oct. 1996
107,1.0,Model query translator: A model-level query approach for large-scale models,Xabier De Carlos; Goiuria Sagardui; Aitor Murguzur; Salvador Trujillo; Xabier Mendialdua,"Persisting and querying models larger than a few tens of megabytes using XMI introduces a significant time and memory footprint overhead to MDD workflows. In this paper, we present an approach that attempts to address this issue using an embedded relational database as an alternative persistence layer for EMF models, and runtime translation of OCL-like expressions for efficiently querying such models. We have performed an empirical study of the approach using a set of large-scale reverse engineered models and queries from the Grabats 2009 Reverse Engineering Contest. Main contribution of this paper is the Model Query Translator, an approach that translates (and executes) at runtime queries from model-level (EOL) to persistence-level (SQL).",IEEE,9-11 Feb. 2015
108,1.0,Complete recognition of continuous Mandarin speech for Chinese language with very large vocabulary but limited training data,Hsin-Min Wang; Jia-Lin Shen; Yen-Ju Yang; Chiu-Yu Tseng; Lin-Shan Lee,"This paper presents the first known results for complete recognition of continuous Mandarin speech for Chinese language with very large vocabulary but very limited training data. Although some isolated-syllable-based or isolated-word-based large-vocabulary Mandarin speech recognition systems have been successfully developed, a continuous-speech-based system of this kind has never been reported before. For successful development of this system, several important techniques have been used, including acoustic modeling of a set of sub-syllabic models for base syllable recognition and another set of context-dependent models for tone recognition, a multiple candidate searching technique based on a concatenated syllable matching algorithm to synchronize base syllable and tone recognition, and a word-class-based Chinese language model for linguistic decoding. The best recognition accuracy achieved is 88.69% for finally decoded Chinese characters, with 88.69%, 91.57%, and 81.37% accuracy for base syllables, tones, and tonal syllables respectively.",IEEE,9-12 May 1995
109,1.0,Model based self adaptive behavior language for large scale real time embedded systems,S. Shetty; S. Neema; T. Bapty,"At Fermi lab, high energy physics experiments require very large number of real time computations. With thousands of processors (around /spl sim/1000 FPGA's, /spl sim/2500 embedded processors, /spl sim/2500 PC's and /spl sim/25,000,000 detector channels) involved in performing event filtering on a trigger farm, there is likely to be a large number of failures within the software and hardware systems. Historically, physicists have developed their own software and hardware for experiments such as BTeV [J.N. Buttler (2002)]. However, their time is best spent working on physics and not software development. The target users of this tool are the physicists. The tool should be user-friendly and the physicists should be able to introduce custom self-adaptive behaviors, since they can best define how the system should behave in fault conditions. The BTeV trigger system is being used as a model for researching tools for defining fault behavior and automatically generating the software. This paper presents a language to define the behaviors and an application scenario for the BTeV system and its expected fault scenarios. These self adaptive system tools are implemented using model integrated computing. The domain specific graphical language (DSL) is implemented within the generic modeling environment (GME) tool, which is a meta-programmable modeling environment developed at Vanderbilt University.",IEEE,27-27 May 2004
110,1.0,A Large-Scale Study of Language Models for Chord Prediction,Filip Korzeniowski; David R. W. Sears; Gerhard Widmer,"We conduct a large-scale study of language models for chord prediction. Specifically, we compare N-gram models to various flavours of recurrent neural networks on a comprehensive dataset comprising all publicly available datasets of annotated chords known to us. This large amount of data allows us to systematically explore hyperparameter settings for the recurrent neural networks-a crucial step in achieving good results with this model class. Our results show not only a quantitative difference between the models, but also a qualitative one: in contrast to static N-gram models, certain RNN configurations adapt to the songs at test time. This finding constitutes a further step towards the development of chord recognition systems that are more aware of local musical context than what was previously possible.",IEEE,15-20 April 2018
111,1.0,Large Language Models are Not Models of Natural Language: They are Corpus Models,Csaba Veres,"Natural Language Processing (NLP) has become one of the leading application areas in the current Artificial Intelligence boom. Transfer learning has enabled large deep learning neural networks trained on the language modeling task to vastly improve performance in almost all downstream language tasks. Interestingly, when the language models are trained with data that includes software code, they demonstrate remarkable abilities in generating functioning computer code from natural language specifications. We argue that this creates a conundrum for the claim that eliminative neural models are a radical restructuring in our understanding of cognition in that they eliminate the need for symbolic abstractions like generative phrase structure grammars. Because the syntax of programming languages is by design determined by phrase structure grammars, neural models that produce syntactic code are apparently uninformative about the theoretical foundations of programming languages. The demonstration that neural models perform well on tasks that involve clearly symbolic systems, proves that they cannot be used as an argument that language and other cognitive systems are not symbolic. Finally, we argue as a corollary that the term language model is misleading and propose the adoption of the working term corpus model instead, which better reflects the genesis and contents of the model.",IEEE,2022
112,1.0,Unsupervised acoustic model training for the Korean language,Antoine Laurent; William Hartmann; Lori Lamel,"This paper investigates unsupervised training strategies for the Korean language in the context of the DGA RAPID Rapmat project. As with previous studies, we begin with only a small amount of manually transcribed data to build preliminary acoustic models. Using the initial models, a larger set of untranscribed audio data is decoded to produce approximate transcripts. We compare both GMM and DNN acoustic models for both the unsupervised transcription and the final recognition system. While the DNN acoustic models produce a lower word error rate on the test set, training on the transcripts from the GMM system provides the best overall performance. We also achieve better performance by expanding the original phone set. Finally, we examine the efficacy of automatically building a test set by comparing system performance both before and after manually correcting the test set.",IEEE,12-14 Sept. 2014
113,1.0,A framework for large scalable natural language call routing systems,C. Wu; D. Lubensky; J. Huerta; X. Li; H.-K.J. Kuo,"A framework is proposed for enterprise automated call routing system development and large scalable natural language call routing application deployment based on IBM's speech recognition and NLU application engagement practices in recently years. To facilitate employing different call classification algorithms in an easy integration manner, this framework architecture provides a plug & play environment for evaluating promising call routing algorithms and a systematic approach to carry out a large scalable enterprise application deployment. The paradigm illustrates the complementary effort to develop an automatic call routing application for enterprise call centers and covers from call classification algorithm investigation to application programming model. Experimental results on a live data testing set collected from an enterprise call center shows that the performance of the call classification algorithm implemented in this framework is outstanding.",IEEE,26-29 Oct. 2003
114,1.0,Automatic clustering of part-of-speech for vocabulary divided PLSA language model,Motoyuki Suzuki; Naoto Kuriyama; Akinori Ito; Shozo Makino,"PLSA is one of the most powerful language models for adaptation to a target speech. The vocabulary divided PLSA language model (VD-PLSA) shows higher performance than the conventional PLSA model because it can be adapted to the target topic and the target speaking style individually. However, all of the vocabulary must be manually divided into three categories (topic, speaking style, and general category). In this paper, an automatic method for clustering parts-of-speech (POS) is proposed for VD-PLSA. Several corpora with different styles are prepared, and the distance between corpora in terms of POS is calculated. The ""general tendency score"" and ""style tendency score"" for each POS are calculated based on the distance between corpora. All of the POS are divided into three categories using two scores and appropriate thresholds. Experimental results showed the proposed method formed appropriate clusters, and VD-PLSA with acquired categories gave the highest performance of all other models. We applied the VD-PLSA into large vocabulary continuous speech recognition system. VD-PLSA improved the recognition accuracy for documents with lower out-of-vocabulary ratio, while other documents were not improved or slightly descended the accuracy.",IEEE,19-22 Oct. 2008
115,1.0,Offline recognition of unconstrained handwritten texts using HMMs and statistical language models,H. Bunke; S. Bengio; A. Vinciarelli,"This paper presents a system for the offline recognition of large vocabulary unconstrained handwritten texts. The only assumption made about the data is that it is written in English. This allows the application of statistical language models in order to improve the performance of our system. Several experiments have been performed using both single and multiple writer data. Lexica of variable size (from 10,000 to 50,000 words) have been used. The use of language models is shown to improve the accuracy of the system (when the lexicon contains 50,000 words, the error rate is reduced by /spl sim/50 percent for single writer data and by /spl sim/25 percent for multiple writer data). Our approach is described in detail and compared with other methods presented in the literature to deal with the same problem. An experimental setup to correctly deal with unconstrained text recognition is proposed.",IEEE,June 2004
116,1.0,Symbolic Math Reasoning with Language Models,Vedant Gaur; Nikunj Saunshi,"The emergence of large language models (LLMs) such as OpenAI’s GPT-3, Google’s LaMDA, Meta’s OPT [2, 3, 7, 10] etc. have revolutionized the field of natural language processing (NLP). These models with upwards of hundreds of billions of parameters are trained on large unlabeled text corpora and can subsequently solve downstream tasks with little to no labeled data. While these models are increasingly versatile in their abilities, e.g., solving math word problems, the larger question of their ability to reason remains. Using and modifying the SVAMP dataset, we find that GPT-3’s davinci-002 model, in addition to having good performance on numerical math word problems, also performs well on the potentially harder symbolic version of the same problems. Furthermore, adopting a two-step approach (solve symbolically and then substitute numerical values) leads to better accuracy on the numerical test set in the zero-shot regime. Additionally, we find that the use of specific prompting techniques pushes the model, in many cases, to actively describe its thought process and aid in the final answer output when faced with a complex, multi-step problem, aligning with recent observations.",IEEE,30 Sept.-2 Oct. 2022
117,1.0,"Comparing RNNs and log-linear interpolation of improved skip-model on four Babel languages: Cantonese, Pashto, Tagalog, Turkish",Mittul Singh; Dietrich Klakow,"Recurrent neural networks (RNNs) are a very recent technique to model long range dependencies in natural languages. They have clearly outperformed trigrams and other more advanced language modeling techniques by using non-linearly modeling long range dependencies. An alternative is to use log-linear interpolation of skip models (i.e. skip bigrams and skip trigrams). The method as such has been published earlier. In this paper we investigate the impact of different smoothing techniques on the skip models as a measure of their overall performance. One option is to use automatically trained distance clusters (both hard and soft) to increase robustness and to combat sparseness in the skip model. We also investigate alternative smoothing techniques on word level. For skip bigrams when skipping a small number of words Kneser-Ney smoothing (KN) is advantageous. For a larger number of words being skipped Dirichlet smoothing performs better. In order to exploit the advantages of both KN and Dirichlet smoothing we propose a new unified smoothing technique. Experiments are performed on four Babel languages: Cantonese, Pashto, Tagalog and Turkish. RNNs and log-linearly interpolated skip models are on par if the skip models are trained with standard smoothing techniques. Using the improved smoothing of the skip models along with distance clusters, we can clearly outperform RNNs by about 8-11 % in perplexity across all four languages.",IEEE,26-31 May 2013
118,1.0,Adapted language modeling for recognition of retelling story in language learning,Meng Chen; Yang Song; Lan Wang,"N-gram language modeling typically requires large quantities of in-domain training data, i.e., data that matches the task in both topic and style. For the task of retelling stories, obtaining large volumes of speech transcriptions is often unrealistic. In this paper, we propose a novel method of language modeling using mixture models with very limited text datain the task of retelling stories. We modeled topic-specific, spoken-style, and document-style language models separately and interpolated them. We also interpolated the class-based language model with the N-gram models. Experimental results show that up to 61.6% reduction of perplexity and 20.7% reduction of word error rate (WER) have been obtained by our best performing model.",IEEE,16-18 July 2012
119,1.0,On Artificial Intelligence for Simulation and Design Space Exploration in Gas Turbine Design,Sebastian Pilarski; Martin Staniszewski; Frederic Villeneuve; Daniel Varro,"Gas turbine design is a process that requires designing many interrelated subsystems, e.g., performance, secondary air system, air compression, or combustion. Subsystem models are created by various engineering design tools. During the design process there exists an extraordinary amount of generated data resulting from created models, simulation, and engine field tests. This data can be leveraged by artificial intelligence techniques such as machine learning to help accelerate the exploration of the large design spaces existing in the complex system of a gas engine. This paper presents a vision and road map of integrating such AIs and preliminary ideas on relevant AI models for such use cases. We explore increasing the realistic nature of existing simulations, approximating simulations to avoid excess computation, and cumulative effect modeling.",IEEE,15-20 Sept. 2019
120,1.0,Language Model Adaptation for Downstream Tasks using Text Selection,Jun Liang; Ander Martinez; Hajime Morita,"Previous research shows that the domain of the training data has a large impact on the performance of the downstream tasks. Selecting data from an appropriate domain leads to improvements on the performance. Using text classification can help discriminate the data which belong to different domains. In this paper, we use a text classification method to select data from a particular domain (task-specific target domain). We experiment with different sizes of target domain corpus to explore the effect of the method. A pretrained RoBERTa model is adapted to the target domain corpus using the selected data prior to training the model on the downstream tasks. Our experiments show that using a simple domain classifier to select a small dataset to adapt the model can help stabilize the performance of downstream tasks.",IEEE,25-27 March 2022
121,1.0,TiBERT: Tibetan Pre-trained Language Model,Sisi Liu; Junjie Deng; Yuan Sun; Xiaobing Zhao,"The pre-trained language model is trained on large-scale unlabeled text and can achieve state-of-the-art results in many different downstream tasks. However, the current pre-trained language model is mainly concentrated in the Chinese and English fields. For low resource language such as Tibetan, there is lack of a monolingual pre-trained model. To promote the development of Tibetan natural language processing tasks, this paper collects the large-scale training data from Tibetan websites and constructs a vocabulary that can cover 99.95% of the words in the corpus by using Sentencepiece. Then, we train the Tibetan monolingual pre-trained language model named TiBERT on the data and vocabulary. Finally, we apply TiBERT to the downstream tasks of text classification and question generation, and compare it with classic models and multilingual pre-trained models, the experimental results show that TiBERT can achieve the best performance. Our model is published in http://tibert.cmli-nlp.con",IEEE,9-12 Oct. 2022
122,1.0,"Discriminatively estimated joint acoustic, duration, and language model for speech recognition",Maider Lehr; Izhak Shafran,"We introduce a discriminative model for speech recognition that integrates acoustic, duration and language components. In the framework of finite state machines, a general model for speech recognition G is a finite state transduction from acoustic state sequences to word sequences (e.g., search graph in many speech recognizers). The lattices from a baseline recognizer can be viewed as an a posteriori version of G after having observed an utterance. So far, discriminative language models have been proposed to correct the output side of G and is applied on the lattices. The acoustic state sequences on the input side of these lattice can also be exploited to improve the choice of the best hypotheses through the lattice. Taking this view, the model proposed in this paper jointly estimates the parameters for acoustic and language components in a discriminative setting. The resulting model can be factored as corrections for the input and the output sides of the general model G. This formulation allows us to incorporate duration cues seamlessly. Empirical results on a large vocabulary Arabic GALE task demonstrate that the proposed model improves word error rate substantially, with a gain of 1.6% absolute. Through a series of experiments we analyze the contributions from and interactions between acoustic, duration and language components to find that duration cues play an important role in Arabic task.",IEEE,14-19 March 2010
123,1.0,Cross-lingual latent semantic analysis for language modeling,Woosung Kim; S. Khudanpur,"Statistical language model estimation requires large amounts of domain-specific text, which is difficult to obtain in many languages. We propose techniques which exploit domain-specific text in a resource-rich language to adapt a language model in a resource-deficient language. A primary advantage of our technique is that in the process of cross-lingual language model adaptation, we do not rely on the availability of any machine translation capability. Instead, we assume that only a modest-sized collection of story-aligned document-pairs in the two languages is available. We use ideas from cross-lingual latent semantic analysis to develop a single low-dimensional representation shared by words and documents in both languages, which enables us to (i) find documents in the resource-rich language pertaining to a specific story in the resource-deficient language, and (ii) extract statistics from the pertinent documents to adapt a language model to the story of interest. We demonstrate significant reductions in perplexity and error rates in a Mandarin speech recognition task using this technique.",IEEE,17-21 May 2004
124,1.0,Query-based composition for large-scale language model in LVCSR,Yang Han; Chenwei Zhang; Xiangang Li; Yi Liu; Xihong Wu,"This paper describes a query-based composition algorithm that can integrate an ARPA format language model in the unified WFST framework, which avoids the memory and time cost of converting the language models to WFST and optimizing the WFST of language models. The proposed algorithm is applied to on-the-fly one-pass decoder and rescoring decoder. Both modified decoder require less memory during decoding on different scale of language models. What's more, query-based on-the-fly one-pass decoder nearly has the same decoding speed as standard one and query-based rescoring decoder even use less time to rescore the lattice. Because of these advantages, large-scale language models can be applied by query-based composition algorithm to improve the performance of large vocabulary continuous speech recognition.",IEEE,4-9 May 2014
125,1.0,CLMAD: A Chinese Language Model Adaptation Dataset,Ye Bai; Jianhua Tao; Jiangyan Yi; Zhengqi Wen; Cunhang Fan,"A language model (LM) is an important part of a speech recognition system. Language model adaptation techniques use a large amount of source domain data and limited target domain data to improve the performance of language models in target domain. Even though text datasets are easy to obtain, there is no public Chinese text dataset for language model adaptation tasks. This paper presents a language model adaptation dataset which consists of four different domains of news data, i.e., sport, stock, fashion, finance. The discrepancy between the domains of data is evaluated. Model combination based adaptation of n-gram is evaluated on the dataset. Three different fine-tuning adaptation methods of recurrent neural network language models (RNNLMs) are evaluated. WER results on AIShell speech data with the language models trained on this dataset are also provided. The absolute WER reduction of lattice rescoring with adapted RNNLM is 4.74%.",IEEE,26-29 Nov. 2018
126,1.0,A Phone Mapping Technique for Acoustic Modeling of Under-Resourced Languages,Van Hai Do; Xiong Xiao; Eng Siong Chng; Haizhou Li,"This paper presents a novel method for acoustic modeling of a new language with a limited amount of training data. In this approach, we use well-trained acoustic models of a foreign language to generate acoustic scores for each feature vector of the target language. These scores are then used as the input for mapping to context dependent triphones of the target language using a limited amount of training data. With this approach, we do not need to modify or have a special requirement for the foreign acoustic models. In this paper, English is used as the foreign language while Malay is used as the target language. Experiments on a Malay large vocabulary continuous speech recognition (LVCSR) task show that with using only few minutes of training data we can achieve a low word error rate which outperforms the best monolingual baseline acoustic model significantly.",IEEE,13-15 Nov. 2012
127,1.0,Language model transformation applied to lightly supervised training of acoustic model for congress meetings,Tatsuya Kawahara; Masato Mimura; Yuya Akita,"For effective training of acoustic and language models for spontaneous speech such as meetings, it is significant to exploit the texts available in a large scale, which may not be faithful transcripts of the utterances. We have proposed a language model transformation scheme to cope with the differences between verbatim transcripts of spontaneous utterances and human-made transcripts such as those in proceedings. In this paper, we investigate its application to lightly supervised training of the acoustic model. By transforming the corresponding text in the proceedings, we can generate a very constrained model to predict the actual utterances. The experimental evaluation with the transcription system for the Japanese Congress meetings demonstrated that the proposed scheme can generate accurate labels for acoustic model training and thus realizes the comparable ASR (Automatic Speech Recognition) performance to the case using manual transcripts.",IEEE,19-24 April 2009
128,1.0,Language model adaptation using Random Forests,Anoop Deoras; Frederick Jelinek; Yi Su,"In this paper we investigate random forest based language model adaptation. Large amounts of out-of-domain data are used to grow the decision trees while very small amounts of in-domain data are used to prune them back, so that the structure of the trees are suitable for the desired domain while the probabilities in the tree nodes are reliably estimated. Extensive experiments are carried out and results are reported on a particular task of adapting Broadcast News language model to the MIT computer science lecture domain. We show 0.80% and 0.60% absolute WER improvement over language model interpolation and count merging techniques, respectively.",IEEE,14-19 March 2010
129,1.0,Spoken language identification using large vocabulary speech recognition,J.L. Hieronymus; S. Kadambe,"A task independent spoken language identification (LID) system which uses a large vocabulary automatic speech recognition (LVASR) module for each language to choose the most likely language spoken is described in detail. The system has been trained on 5 languages: English, German, Japanese, Mandarin Chinese and Spanish. It is demonstrated that the performance of a LID system which is based on LVASR gives very good performance, when trained and tested on a 5 language subset (English, German, Spanish, Japanese, and Mandarin Chinese) of the Oregon Graduate Institute language data base. The performance advantage is shown for both long (50 second) and short (10 second) test utterances. The five language results show 88% correct recognition for 50 second utterances without confidence measures and 98% correct with confidence measures. The recognition rate is 81% correct for 10 second utterances without confidence measures and 93% correct with confidence measures. The best performance has been obtained for systems trained on phonetically hand labeled speech.",IEEE,3-6 Oct. 1996
130,1.0,Neural Network Language Modeling with Letter-Based Features and Importance Sampling,Hainan Xu; Ke Li; Yiming Wang; Jian Wang; Shiyin Kang; Xie Chen; Daniel Povey; Sanjeev Khudanpur,"In this paper we describe an extension of the Kaldi software toolkit to support neural-based language modeling, intended for use in automatic speech recognition (ASR) and related tasks. We combine the use of subword features (letter n-grams) and one-hot encoding of frequent words so that the models can handle large vocabularies containing infrequent words. We propose a new objective function that allows for training of unnormalized probabilities. An importance sampling based method is supported to speed up training when the vocabulary is large. Experimental results on five corpora show that Kaldi-RNNLM rivals other recurrent neural network language model toolkits both on performance and training speed.",IEEE,15-20 April 2018
131,1.0,First-Pass Techniques for Very Large Vocabulary Speech Recognition ff Morphologically Rich Languages,Matti Varjokallio; Sami Virpioja; Mikko Kurimo,"In speech recognition of morphologically rich languages, very large vocabulary sizes are required to achieve good error rates. Especially traditional n-gram language models trained over word sequences suffer from data sparsity issues. The language modelling can often be improved by segmenting the words to sequences of subword units that are more frequent. Another solution is to cluster the words into classes and apply a class-based language model. We show that linearly interpolating n-gram models trained over words, subwords, and word classes improves the first-pass speech recognition accuracy in very large vocabulary speech recognition tasks for two morphologically rich and agglutinative languages, Finnish and Estonian. To overcome performance issues, we also introduce a novel language model look-ahead method utilizing a class bigram model. The method improves the results over a unigram look-ahead model with the same recognition speed, the difference increasing for small real-time factors. The improved model combination and look-ahead model are useful in cases where real-time recognition is required or when the improved hypotheses help with further recognition passes. For instance, neural network language models are mostly applied by rescoring the generated hypotheses due to higher computational costs.",IEEE,18-21 Dec. 2018
132,1.0,A meta-model for large-scale software system,Yinxing Wei; Shensheng Zhang; Farong Zhong,"For large-scale software system development, many configuration items will be produced during software life cycle and every configuration item will evolve independently. Therefore, there is a need for adopting a suitable model to describe the configuration items, the relationship among them, the evolving of a single item and the constraints of the system. By means of UML class diagram, we propose a software configuration meta-model to express configuration items, relationships among the items and the evolvement of each configuration item. The model consists of three kinds of classes: configuration item, port and version. We also defined four relationships: aggregation, generalization, dependency and successor. The notion of port is used to describe the static properties and dynamic behaviors of a configuration item. The constraints of configuration items are defined using the object constraint language (OCL). The model must conform to several constraints: well-formed configuration, behavior equivalence, upward compatibility, and structural conformance. In short, the model can express complex software product structure and describe constraints in a precise way. It provides a strong basis for controlling change and supporting support management and development in the large-scale software system.",IEEE,8-8 Oct. 2003
133,1.0,ACD++: A domain specific language for cell-DEVS modelling,Chong Jiao; Baohong Liu,"This paper introduces a library ACD++ for modeling and simulation of cellular models based on Cell-DEVS formalism. The goal is to allow the modeling of cellular models more flexible and adaptive. ACD++ is implemented in Ruby programming language, providing an internal Domain Specific Language (DSL) to simplify the construction of cellular models. Ruby's meta-programming characteristics and plentiful syntactic sugar enables the easy expression of complex logics behind the models. The Cell-DEVS formalism proved consistent with the DEVS hierarchy, improving the description of complex systems. Another strength lies in the extensibility of the DSL, allowing the modelers to introduce their domain specific vocabulary to facilitate the definition of specific models. The use of this library has allowed the development more flexible and adaptive, significantly reducing development time.",IEEE,10-12 July 2017
134,1.0,GPT-K: A GPT-based model for generation of text in Kannada,K H Manodnya; Animesh Giri,"Large AI-based language models are changing how we work with language. They are becoming increasingly popular because they allow us to create complex linguistic structures without requiring a lot of resources. A language model must have access to a large corpus of linguistic data (e.g., word frequencies) to learn and generate new words. GPT-2, a language model, can generate coherent paragraphs independently, without any input on what to write about or guidance on grammar rules. Although multiple pre-trained GPT-2 models exist for English and other high-resource languages, there are few to no such models for Indic languages like Kannada. In this study, we propose GPT-K, a GPT-2 based model for language modeling in Kannada. GPT-K has been trained on a large corpus of Kannada text and can effectively perform language modeling tasks in Kannada. The model generated syntactically correct text in most cases.",IEEE,8-9 Oct. 2022
135,1.0,She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models,Christoph Treude; Hideaki Hata,"Implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. To address this bias, it is important to understand it in more detail. This study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning GitHub issues and testing, are affected by implicit gender bias embedded in large language models. We systematically translated each task from English into a genderless language and back, and investigated the pronouns associated with each task. Based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. Specifically, requirements elicitation was associated with the pronoun “he” in only 6% of cases, while testing was associated with “he” in 100% of cases. Additionally, tasks related to helping others had a 91% association with “he” while the same association for tasks related to asking coworkers was only 52%. These findings reveal a clear pattern of gender bias related to software development tasks and have important implications for addressing this issue both in the training of large language models and in broader society.",IEEE,15-16 May 2023
136,1.0,NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks,Fawaz Sammani; Tanmoy Mukherjee; Nikos Deligiannis,"Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models11Throughout this paper, we refer to NLE models as Natural Language Explanation models aimed for vision and vision-language tasks. explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15× faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a selfevaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt.",IEEE,18-24 June 2022
137,1.0,Development of Part-of-Speech tagger for a low-resource endangered language,Toshal Gore; Vaibhav Khatavkar,"India is one of the multilingual countries where large number of languages are spoken, major languages being Hindi, Bengali and Marathi. Indian languages have limited research done in the Natural Language Processing (NLP) domain. This is because Indian languages use Brahmic script alphabets, instead of Latin alphabets, which is very difficult for NLP to understand and process. Most of the Indian languages have many dialects and also have many distinct linguistic characteristics as compared to English. Also, there are many Indian languages which are on the verge of extinction and there is very little progress done on NLP for such languages in order to preserve them. The size of dataset available for such low resource languages is very small. One such language is Katkari, which is an endangered Indian tribal language, and a dialect of Marathi language. The purpose of this work is to develop a Part-of-Speech (POS) tagger for Katkari language. POS tagging is a technique in which each word in the text is assigned a POS label based on its context. The POS taggers for several Indian languages are developed, but for Katkari language, work is yet to be done. Hence, this paper presents a POS tagger for Katkari language which is built with the help of Hidden Markov Model (HMM) and Viterbi algorithm. The Katkari POS tagger was compared with POS taggers of other Indian languages and the accuracy of the Katkari POS tagger was obtained as 86.84%.",IEEE,16-17 Dec. 2022
138,1.0,Full-Sum Decoding for Hybrid Hmm Based Speech Recognition Using LSTM Language Model,Wei Zhou; Ralf Schlüter; Hermann Ney,"In hybrid HMM based speech recognition, LSTM language models have been widely applied and achieved large improvements. The theoretical capability of modeling any unlimited context suggests that no recombination should be applied in decoding. This motivates to reconsider full summation over the HMM-state sequences instead of Viterbi approximation in decoding. We explore the potential gain from more accurate probabilities in terms of decision making and apply the full-sum decoding with a modified prefix-tree search framework. The proposed full-sum decoding is evaluated on both Switchboard and Librispeech corpora. Different models using CE and sMBR training criteria are used. Additionally, both MAP and confusion network decoding as approximated variants of general Bayes decision rule are evaluated. Consistent improvements over strong baselines are achieved in almost all cases without extra cost. We also discuss tuning effort, efficiency and some limitations of full-sum decoding.",IEEE,4-8 May 2020
139,1.0,Cognitive Behaviors Modeling Using UML Profile: Design and Experience,Zhi Zhu; Yonglin Lei; Yifan Zhu; Hessam Sarjoughian,"To achieve model reuse in combat effectiveness simulation systems development, cognitive decision behaviors are usually implemented using a scripting language, which is separate from the programming language used to implement simulation models. Therefore, it is desirable to establish a much better grounding for cognitive behaviors modeling. In the context of domain specific modeling, metamodeling from scratch for designing such a scripting language poses some limitations, among which is the issue of integrating various models that are represented by various customized languages with different syntax and semantics, together with a large expenditure of designing, implementing, and maintaining these languages and their supporting resources. Instead, UML profile-based metamodeling is adopted, as a lightweight extension to capture the cognitive domain specific concepts, relationships, and constraints. Moreover, a unifying framework is proposed to guide the cognitive domain specific profiles design. Upon this framework, the development process is shown through constructing an anti-submarine tactical profile in combat effectiveness simulation systems domain and the feasibility of the domain specific language is illustrated with an armed escort scenario.",IEEE,2017
140,1.0,Estimation of language models for new spoken language applications,S. Issar,"Spoken language interfaces can provide natural communication for many database retrieval tasks. The CMU ATIS system provides an example of accessing airline information using spoken natural language queries. However, a lot of training data is needed to develop a spoken language application. For example, one needs training data to generate a language model that can be used by the recognizer to reduce the search space. The author addresses some issues arising from small amount of training data available for a new spoken language application. The author is working on a spoken language interface to access information from a library catalogue. The catalogue contains around 13,000 titles, 6000 authors and 19000 subjects. There an more than 20,000 words in the dictionary. The user can seek information about books, authors, subjects, publishers, etc. For example, ""I'd like to see books dealing with Science fiction by Clarke."" The author describes some language modelling experiments for this task. The author briefly describes a speech interface for a library catalogue. The author also reviews class-based language models and describes their limitations. Finally, the author presents the approach to building statistical language models for new spoken language applications. This is important because a lot of training data is normally needed to generate a language model. However, it is not practical to have or collect a large corpus of data for each new spoken language application.",IEEE,3-6 Oct. 1996
141,1.0,Quantifying Domain Knowledge in Large Language Models,Sudhashree Sayenju; Ramazan Aygun; Bill Franks; Sereres Johnston; George Lee; Hansook Choi; Girish Modgil,"Transformer based Large language models such as BERT, have demonstrated the ability to derive contextual information from the words surrounding it. However, when these models are applied in specific domains such as medicine, insurance, or scientific disciplines, publicly available models trained on general knowledge sources such as Wikipedia, it may not be as effective in inferring the appropriate context compared to domain-specific models trained on specialized corpora. Given the limited availability of training data for specific domains, pre-trained models can be fine-tuned via transfer learning using relatively small domain-specific corpora. However, there is currently no standardized method for quantifying the effectiveness of these domain-specific models in acquiring the necessary domain knowledge. To address this issue, we explore hidden layer embeddings and introduce domain_gain, a measure to quantify the ability of a model to infer the correct context. In this paper, we show how our measure could be utilized to determine whether words with multiple meanings are more likely to be associated with domain-related meanings rather than their colloquial meanings.",IEEE,5-6 June 2023
142,1.0,Converting Neural Network Language Models into back-off language models for efficient decoding in automatic speech recognition,Ebru Arısoy; Stanley F. Chen; Bhuvana Ramabhadran; Abhinav Sethy,"Neural Network Language Models (NNLMs) have achieved very good performance in large-vocabulary continuous speech recognition (LVCSR) systems. Because decoding with NNLMs is very computationally expensive, there is interest in developing methods to approximate NNLMs with simpler language models that are suitable for fast decoding. In this work, we propose an approximate method for converting a feedforward NNLM into a back-off n-gram language model that can be used directly in existing LVCSR decoders. We convert NNLMs of increasing order to pruned back-off language models, using lower-order models to constrain the n-grams allowed in higher-order models. In experiments on Broadcast News data, we find that the resulting back-off models retain the bulk of the gain achieved by NNLMs over conventional n-gram language models, and give significant accuracy improvements as compared to existing methods for converting NNLMs to back-off models. In addition, the proposed approach can be applied to any type of non-back-off language model to enable efficient decoding.",IEEE,26-31 May 2013
143,1.0,Large margin estimation of n-gram language models for speech recognition via linear programming,Vladimir Magdin; Hui Jiang,"We present a novel discriminative training algorithm for n-gram language models for use in large vocabulary continuous speech recognition. The algorithm uses large margin estimation (LME) to build an objective function for maximizing the minimum margin between correct transcriptions and their competing hypotheses, which are encoded as word graphs generated from the Viterbi decoding process. The nonlinear LME objective function is approximated by a linear EM-style auxiliary function that leads to a linear programming problem, which is efficiently solved by convex optimization algorithms. Experimental results have shown that the proposed discriminative training method can outperform the conventional discounting-based maximum likelihood estimation methods. A relative reduction in word error rate of over 2.5% has been observed on the SPINE1 speech recognition task.",IEEE,14-19 March 2010
144,1.0,Statistical language model adaptation for Mandarin broadcast news transcription,B. Chen; Wen-Hung Tsai; Jen- Wei Kuo,"This paper investigates statistical language model adaptation for Mandarin broadcast news transcription. A topical mixture model was proposed to explore the long-span latent topical information for dynamic language model adaptation. The underlying characteristics and various kinds of model complexities were extensively investigated, while their performance was verified by comparison with the conventional MAP-based adaptation approaches, which are devoted to extracting the short-span n-gram information. Speech recognition experiments were conducted on the broadcast news collected in Taiwan. Very promising results in both perplexity and word error rate reductions were initially obtained.",IEEE,15-18 Dec. 2004
145,1.0,Leveraging Pre-Trained Representations to Improve Access to Untranscribed Speech from Endangered Languages,Nay San; Martijn Bartelds; Mitchell Browne; Lily Clifford; Fiona Gibson; John Mansfield; David Nash; Jane Simpson; Myfany Turpin; Maria Vollmer; Sasha Wilmoth; Dan Jurafsky,"Pre-trained speech representations like wav2vec 2.0 are a powerful tool for automatic speech recognition (ASR). Yet many endangered languages lack sufficient data for pre-training such models, or are predominantly oral vernaculars without a standardised writing system, precluding fine-tuning. Query-by-example spoken term detection (QbE-STD) offers an alternative for iteratively indexing untranscribed speech corpora by locating spoken query terms. Using data from 7 Australian Aboriginal languages and a regional variety of Dutch, all of which are endangered or vulnerable, we show that QbE-STD can be improved by leveraging representations developed for ASR (wav2vec 2.0: the English monolingual model and XLSR53 multilingual model). Surprisingly, the English model outperformed the multilingual model on 4 Australian language datasets, raising questions around how to optimally leverage self-supervised speech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0 representations (either English or XLSR53) offer large improvements (56-86% relative) over state-of-the-art approaches on our endangered language datasets.",IEEE,13-17 Dec. 2021
146,1.0,Exploring Large Language Models’ Emotion Detection Abilities: Use Cases From the Middle East,Radhakrishnan Venkatakrishnan; Mahsa Goodarzi; M. Abdullah Canbaz,"Emotion detection is a critical component in allowing machines to understand and respond to human emotions. In this paper, we explore the potential of pre-trained transformer-based language models, namely, GPT3.5 and RoBERTa for emotion detection in natural language processing. Specifically, we focus on examining the quality of emotion detection in LLMs and their potential as automatic labeling generators to improve accuracy. The emotional response to two significant events, the murder of Zhina (Mahsa) Amini in Iran and the earthquake in Turkey and Syria, is analyzed. We observe that GPT’s generative nature hinders its performance in fine-grained emotion classification, whereas RoBERTa’s fine-tuning abilities and extensive pre-training specifically for emotions enable more accurate predictions within a limited set of emotional labels.",IEEE,5-6 June 2023
147,1.0,User-Centered Performance Engineering of Model Transformations,Raffaela Groner,"In Model-Driven Engineering, models are key artifacts. Due to the fact that the systems to be developed become larger and more complex, the corresponding models also become larger and more complex. This trend also influences operations on these models, such as transformations. They are applied at design time and at runtime, e.g. to update models, generate code or to create new models. With increasing model size, their execution time increases, making their performance an important quality aspect. Current research mainly concentrates on further improvements of the transformation engine that performs the transformation, but this will not solve the problem alone. Engine optimizations will never be able to mitigate every possible performance problem due to the fact that there's an arbitrary amount of ways to define a transformation as well as the models and meta-models that all affect the runtime. Therefore, transformation engineers must also ensure that they define their transformations in such a way that they have a short execution time. To achieve this, a performance engineering approach for model transformations is necessary. This approach must consist of steps and techniques that help to analyze and improve performance. In this paper we present our performance engineering approach for declarative model transformations. We identified the five artifacts Guidelines, Monitoring, Analyses, Visualizations and Improvement proposals that form our approach. These artifacts are intended to help an engineer to understand the execution of a transformation and the causes of performance problems with the help of Analyses and Visualizations based on our Monitoring in order to improve them. During the improvement the engineer will be supported by Guidelines and Improvement proposals.",IEEE,15-20 Sept. 2019
148,1.0,Semi-supervised Learning of Domain-Specific Language Models from General Domain Data,Shuanhu Bai; Min Zhang; Haizhou Li,"We present a semi-supervised learning method for building domain-specific language models (LM) from general-domain data. This method is aimed to use small amount of domain-specific data as seeds to tap domain-specific resources residing in larger amount of general-domain data with the help of topic modeling technologies. The proposed algorithm first performs topic decomposition (TD) on the combined dataset of domain-specific and general-domain data using probabilistic latent semantic analysis (PLSA). Then it derives domain-specific word n-gram counts with mixture modeling scheme of PLSA. Finally, it uses traditional n-gram modeling approach to construct domain-specific LMs from the domain-specific word n-gram counts. Experimental results show that this approach can outperform both stat-of-the-art methods and the simulated supervised learning method with our data sets. In particular, the semi-supervised learning method can achieve better performance even with very small amount of domain-specific data.",IEEE,7-9 Dec. 2009
149,1.0,An additional approach to pre-trained code model with multilingual natural languages,Teruno Kajiura; Nao Souma; Miyu Sato; Mai Takahashi; Kimio Kuramitsu,"Pre-trained language models have achieved many prominent results in natural language processing. Since software engineering widely includes many natural language documents, the application of pre-trained language models have received much attention in software engineering tasks. However, pretraining a large volume of source code requires a huge amount of computational resources and time. In this study, we propose an additional pre-training approach to a well-trained language model. Our initial results on mT5, multilingual T5 with an additional pretraining of Python code shows improved performance on multiple software engineering tasks including code generation, code summarisation, code repair, and error diagnosis.",IEEE,6-9 Dec. 2022
150,1.0,A statistical language modeling approach integrating local and global constraints,J.R. Bellegarda,"A new framework is proposed to integrate the various constraints, both local and global, that are present in language. Local constraints are captured via n-gram language modeling, while global constraints are taken into account through the use of latent semantic analysis. An integrative formulation is derived for the combination of these two paradigms, resulting in several families of multi-span language models for large-vocabulary speech recognition. Because of the inherent complementarity in the two types of constraints, the performance of the integrated language models, as measured by perplexity, compares favorably with the corresponding n-gram performance.",IEEE,17-17 Dec. 1997
151,1.0,Context-sensitive language modeling for large sets of proper nouns in multimodal dialogue systems,Alexander Gruenstein; Stephanie Seneff,"We explore several language modeling strategies for increasing the recognition accuracy among large sets of proper nouns in a map- based multimodal dialogue system which provides restaurant information. In particular, we evaluate several mechanisms for exploiting dialogue context, the two most promising of which involve a semi- static metropolitan-region based large set of proper nouns competing with a smaller, in-focus subset. We show that these techniques decrease word, concept, and proper noun error rates under several training conditions. We also present a technique to generalize sparse training data through derived templates to improve language model robustness.",IEEE,10-13 Dec. 2006
152,1.0,Extracting answers to natural language questions from large-scale corpus,Peng Li; Xiao Long Wang; Yi Guan; Yu Ming Zhao,"This paper provides a novel and tractable method for extracting exact textual answers from the returned documents that are retrieved by traditional IR system in large-scale collection of texts. In our approach, WordNet and Web information are employed to improve the performance as external auxiliary resources, then some NLP technologies are used to constitute the empirical answer ranking formula, such as POS tagging, Named Entity recognition, and parser etc. The method involves automatically ranking passages with System Similarity Model, automatically downloading related Web pages by means of Web crawler, and automatically mining answers with empirical formula from candidate answer sets. The series of experimental results show that the overall performance of our system is good and the structure of the system is reasonable.",IEEE,30 Oct.-1 Nov. 2005
153,1.0,Comparison of Modified Kneser-Ney and Witten-Bell smoothing techniques in statistical language model of Bahasa Indonesia,Ismail,"Smoothing is one technique to overcome data spar-sity in statistical language model. Although in its mathematical definition there is no explicit dependency upon specific natural language, different natures of natural languages result in different effects of smoothing techniques. This is true for Russian language as shown by Whittaker [2]. In this paper, We compared Modified Kneser-Ney and Witten-Bell smoothing techniques in statistical language model of Bahasa Indonesia. We used train sets of totally 22M words that we extracted from Indonesian version of Wikipedia. As far as we know, this is the largest train set used to build statistical language model for Bahasa Indonesia. The experiments with 3-gram, 5-gram, and 7-gram showed that Modified Kneser-Ney consistently outperforms Witten-Bell smoothing technique in term of perplexity values. It is interesting to note that our experiments showed 5-gram model for Modified Kneser-Ney smoothing technique outperforms that of 7-gram. Meanwhile, Witten-Bell smoothing is consistently improving over the increase of n-gram order.",IEEE,28-30 May 2014
154,1.0,BioELM: Integrating Biomedical Knowledge into Language Model with Entity-Linking,Qing Li; Guanzhong Wu; Tao You,"Pretrained language models have achieved widespread success on various natural language processing tasks. In the biomedical domain, one line of research is to utilize a large amount of in-domain corpus for pre-training.While these models achieved remarkable improvement on in-domain tasks, they do not take into account the positive role of large-scale in-domain knowledge bases. Integrating biomedical knowledge in the knowledge base like the Unified Medical Language System(UMLS) into these models can further benefit in-domain downstream tasks, such as biomedical named entities and relation extraction. To this end, we proposed BioELM, a pre-trained language model based on entity linking that explicitly leverages knowledge from the UMLS knowledge base. We utilize a two-layer entity-linking structure to integrate entity representations. To optimize the pre-training process, we optimized the masked language modeling and added two training objectives as named entity recognition and entity linking. We validate the performance of our BioELM on named entity recognition and relation extraction tasks on the BLURB benchmark. The experimental results demonstrate that the pre-training tasks and entity-linking strategy on BioELM can improve the performance on both biomedical named entity recognition and relation extraction tasks.",IEEE,6-8 Dec. 2022
155,1.0,A Neural Network approach for mixing language models,Youssef Oualil; Dietrich Klakow,"The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.",IEEE,5-9 March 2017
156,1.0,Analysis of the Technical Principles of ChatGPT and Prospects for Pre-trained Large Models,Ziwen Jin,"ChatGPT is a pre-trained model in the field of natural language processing. As a generative model, the technical foundation of ChatGPT is a deep learning model called the ""Generative Adversarial Network"". The pre-trained large model architecture of ChatGPT can be summarized as ""corpus system+pre-training+fine-tuning"". Under the combined force of massive data, super large models, and enormous computing power, chatGPT has ushered in the era of universal artificial intelligence and formed a new paradigm of generative AI development. Generative AI products represented by ChatGPT will promote the development and implementation of Artificial Intelligence Generated Content (AIGC), and trigger significant changes in areas such as information acquisition methods and economic and social cost structures",IEEE,26-28 May 2023
157,1.0,A Voice-Controlled Motion Reproduction Using Large Language Models for Polishing Robots,Yuki Tanaka; Seiichiro Katsura,"In recent years, the shortage of professionally skilled people in industrial fields has been a major social problem. To solve this problem, the transfer of skills to robots has been attracting much attention. However, they are not familiar with robot control, and hard to teach robots their skills by numerical commands or program source code. For more user-friendly human-robot interaction, a lot of studies have been conducted. In previous researches, robot task processes are pre-defined and not changed in task execution. We developed a robot system using the motion-copying system and GPT-3, one of the Large Language Models. This system can not only copy the motion but also modify saved motion in execution by using natural language commands. We evaluated the proposed system by applying it to polishing robots and confirmed that the surface of used workpieces was changed following to input commands.",IEEE,15-17 March 2023
158,1.0,Exploring Large Language Models in a Limited Resource Scenario,Anand Panchbhai; Smarana Pankanti,"Generative Pre-trained Transformers (GPT) have gained a lot of popularity in the domain of Natural Language Processing (NPL). Lately, GPTs have been fine-tuned for tasks like sentiment analysis and text summarization. As the number of tunable parameters increases with larger language models (like GPT-3), it becomes resource-heavy to fine-tune these models on commercially available personal computer systems. In addition to that, GPT-3 is only available through an API which makes it even harder to fine-tune it for a specific task. This makes these models less accessible to the general public and researchers. Alternative ways are required to better understand the nature of these language models and employ them for challenging NLP tasks without explicit fine-tuning. This study capitalizes on the raw capabilities of GPT-2, it proposes and proves the efficacy of one such system in the task of sentiment analysis without explicit fine-tuning. It also sheds light into the nature of such generative language models and shows how explainability can be exploited to achieve good results with minimum resources. It was observed that the proposed system does a good job of capturing the sentiment of a given text. It reached an accuracy of 82% on a part of the IMDB Data set of Movie Reviews. The system performed better with natural language prompt when compared to symbol-based syntactic prompts.",IEEE,28-29 Jan. 2021
159,1.0,Leaving the visual language ghetto,M. Munch; A. Schurr,"Visual languages (VLs)-invented outside the visual programming language (VPL) community-are quite successfully used around the world. There are, for example, international VL standards used in the telecommunication, software engineering and automatic control engineering industries, but alas, most of these language standards were defined without knowing the VL definition and design principles developed by the VPL community. This paper suggests a procedure for employing our knowledge about VLs to develop incremental improvements for widely accepted visual modeling/programming language standards. It starts with a survey of component-based VLs and ends with some proposals about how to to incorporate modeling-in-the-large concepts into the IEC-1131 standard for function block languages.",IEEE,13-16 Sept. 1999
160,1.0,Large vocabulary speech recognition of Slovenian language using morphological models,M. Maucec; T. Rotovnik; Z. Kacic; B. Horvat,"This paper concerns the development of an automatic speech recognition system for the Slovenian language. The large number of unique words in inflected languages is identified as the primary reason for performance degradation. This article discusses statistical language models. A novel variation of the n-gram modelling theme is examined. Modelling units are chosen to be stems and endings instead of words. Only data-driven algorithms are employed to decompose words into stems and endings automatically. Significant reduction of OOV rate results when using stems and endings for modeling the Slovenian language. We also discuss corpus-based topic-adapted language models. Language models are most often used in a homogeneous topic environment. The problem of topic detection in highly inflected language is outlined, caused by the appearance of several word forms derived from the same lemma. The problem is solved by using data-driven algorithms to group words of the same lemma into classes.",IEEE,22-24 Sept. 2003
161,1.0,Web Based Software Modeling Exercises in Large-Scale Software Engineering Courses,Birgit Demuth; Daniel Weigel,We present a Web based elearning system to support software modeling exercises in large-scale software engineering courses. Students get the task to create a domain model based one given textual specification of an application domain. They have to specify the domain model by a UML class diagram and import it into the elearning system. The system verifies the student's solution and compares it with a set of predetermined sample solutions. This approach is a first step towards an automatic testing of object-oriented models under the conditions of university classes with a large number of students. We discuss it and report about initial experiences in using the elearning system.,IEEE,17-20 Feb. 2009
162,1.0,Significance of neural phonotactic models for large-scale spoken language identification,Brij Mohan Lal Srivastava; Hari Vydana; Anil Kumar Vuppala; Manish Shrivastava,"Language identification (LID) is vital frontend for spoken dialogue systems operating in diverse linguistic settings to reduce recognition and understanding errors. Existing LID systems which use low-level signal information for classification do not scale well due to exponential growth of parameters as the classes increase. They also suffer performance degradation due to the inherent variabilities of speech signal. In the proposed approach, we model the language-specific phonotactic information in speech using recurrent neural network for developing an LID system. The input speech signal is tokenized to phone sequences by using a common language-independent phone recognizer with varying phonetic coverage. We establish a causal relationship between phonetic coverage and LID performance. The phonotactics in the observed phone sequences are modeled using statistical and recurrent neural network language models to predict language-specific symbol from a universal phonetic inventory. Proposed approach is robust, computationally light weight and highly scalable. Experiments show that the convex combination of statistical and recurrent neural network language model (RNNLM) based phonotactic models significantly outperform a strong baseline system of Deep Neural Network (DNN) which is shown to surpass the performance of i-vector based approach for LID. The proposed approach outperforms the baseline models in terms of mean F1 score over 176 languages. Further we provide significant information-theoretic evidence to analyze the mechanism of the proposed approach.",IEEE,14-19 May 2017
163,1.0,Voice search language model adaptation using contextual information,Justin Scheiner; Ian Williams; Petar Aleksic,"It has been shown that automatic speech recognition (ASR) system quality can be improved by augmenting n-gram language models with contextual information [1][2]. In the voice search domain, there are a large number of useful contextual signals for a given query. Some of these signals are speaker location, speaker identity, time of the query, etc. Each of these signals comes with relevant contextual information (e.g. location specific entities, favorite queries, recent popular queries) that is not included in the language model's training data. We show that these contextual signals can be used to improve ASR system quality. This is achieved by adjusting n-gram language model probabilities on-the-fly based on the contextual information relevant for the current voice search request. We analyze three example sources of context: location context, previously entered typed and spoken queries. We present a set of approaches we have used to improve ASR quality using these sources of context. Our main objective is to automatically, in real time, take advantage of all available sources of contextual information. In addition, we investigate challenges that come with applying our approach to a number of languages (unsegmented languages, languages with diacritics) and present solutions used.",IEEE,13-16 Dec. 2016
164,1.0,Language model switching based on topic detection for dialog speech recognition,I.R. Lane; T. Kawahara; T. Matsui,"An efficient, scalable speech recognition architecture is proposed for multidomain dialog systems by combining topic detection and topic-dependent language modeling. The inferred domain is automatically detected from the user's utterance, and speech recognition is then performed with an appropriate domain-dependent language model. The architecture improves accuracy and efficiency over current approaches and is scaleable to a large number of domains. In this paper, a novel framework using a multilayer hierarchy of language models is introduced in order to improve robustness against topic detection errors. The proposed system provides a relative reduction in WER of 10.5% over a single language model system. Furthermore it achieves an accuracy that is comparable to using multiple language models in parallel while using only a fraction of the computational cost.",IEEE,6-10 April 2003
165,1.0,SML4C: Fully Automatic Classification of State Machine Models for Model Inspection in Education,Shinpei Ogata; Mizue Kayama,"In the task-based learning of state machine modeling, understanding a large number of learner-created models is a very time-consuming task for instructors. Given that learner-created models with different descriptions often represent the same behavior, if such models can be classified correctly by behavioral similarity, then the instructors may efficiently create feedback with no oversights by inspecting one model per class. Such approach, in essence, allows learners to receive an early and beneficial feedback. However, the problem is that there is no existing method that can easily and automatically classify these learner-created state machine models in terms of behavioral similarity, which is our firsthand attempt in this study. We referred to our proposed tool as SMart-Learning for Classification (SML4C). It specifically measures the behavioral similarity of the state machine models based on the results of their testing. This approach prevents the measurement results from depending on the descriptive variety. In a previous work we realized a method and tool to test the state machine models, where instructors were required to manually create test cases. On the contrary, the method proposed herein provides a new function of fully and automatically generating test cases from the learner-created models. Thus, state machine models can be easily classified using various test cases that capture the generation-source model's behaviors. We validated the effectiveness of the method via application to 63 learner-created models that express the collaborative behaviors between two mobile robots.",IEEE,15-20 Sept. 2019
166,1.0,Teaching Modelling Literacy: An Artificial Intelligence Approach,Rijul Saini; Gunter Mussbacher; Jin L.C. Guo; Jöerg Kienzle,"In Model-Driven Engineering (MDE), models are used to build and analyze complex systems. In the last decades, different modelling formalisms have been proposed for supporting software development. However, their adoption and practice strongly rely on mastering essential modelling skills to develop a complete and coherent model-based system. Moreover, it is often difficult for novice modellers to get direct and timely feedback and recommendations on their modelling strategies and decisions, particularly in large classroom settings which hinders their learning. Certainly, there is an opportunity to apply Artificial Intelligence (AI) techniques to an MDE learning environment to empower the provisioning of automated and intelligent modelling advocacy. In this paper, we propose a framework called ModBud (a modelling buddy) to educate novice modellers about the art of abstraction. ModBud uses natural language processing (NLP) and machine learning (ML) to create modelling bots with the aim of improving the modelling skills of novice modellers and assisting other practitioners, too. These bots could be used to support teaching with automatic creation or grading of models and enhance learning beyond the traditional classroom-based MDE education with timely feedback and personalized tutoring. Research challenges for the proposed framework are discussed and a research roadmap is presented.",IEEE,15-20 Sept. 2019
167,1.0,Statistical analysis of large sets of models,Önder Babur,"Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.",IEEE,3-7 Sept. 2016
168,1.0,Semantics of Co-simulation Algorithms with Simulator Contracts,Cláudio Gomes; Levi Lúcio; Hans Vangheluwe,"The rapid adoption of co-simulation techniques allows for holistic complex system development. However, ensuring trustworthy results when combining simulators requires a careful consideration of their implementation and capabilities. Especially in black box integration, these are frequently left implicit. In this paper, we explore a way to account for simulator capabilities, by formalizing the execution of a co-simulation that respects such contracts. This formalization is specific to two kinds of contracts, but could serve as a basis to a general approach to black box co-simulation. An example application of the semantics to generate master algorithms is presented.",IEEE,15-20 Sept. 2019
169,1.0,Language model adaptation for automatic call transcription,Ali Haznedaroglu; Levent M. Arslan,This paper presents a method of language model adaptation for call-center conversations using automatic speech recognition (ASR) transcripts and their confidence scores. The goal is to select the optimal adaptation set by estimating the recognition errors and minimizing the adaptation language model (LM) perplexity. ASR transcripts are ranked with respect to their confidence scores and adaptation data selection is done iteratively by filtering the most reliable transcript set that minimizes the LM perplexity. Model adaptation is then carried out by interpolating the selected adaptation LM with the baseline in-domain LM. We have evaluated our approach on agent speech of real call-center conversations and experiments show that 4% relative word error rate reduction is achieved with the proposed approach.,IEEE,4-9 May 2014
170,1.0,Stream my models: Reactive peer-to-peer distributed models@run.time,Thomas Hartmann; Assaad Moawad; Francois Fouquet; Gregory Nain; Jacques Klein; Yves Le Traon,"The models@run.time paradigm promotes the use of models during the execution of cyber-physical systems to represent their context and to reason about their runtime behaviour. However, current modeling techniques do not allow to cope at the same time with the large-scale, distributed, and constantly changing nature of these systems. In this paper, we introduce a distributed models@run.time approach, combining ideas from reactive programming, peer-to-peer distribution, and large-scale models@run.time. We define distributed models as observable streams of chunks that are exchanged between nodes in a peer-to-peer manner. A lazy loading strategy allows to transparently access the complete virtual model from every node, although chunks are actually distributed across nodes. Observers and automatic reloading of chunks enable a reactive programming style. We integrated our approach into the Kevoree Modeling Framework and demonstrate that it enables frequently changing, reactive distributed models that can scale to millions of elements and several thousand nodes.",IEEE,30 Sept.-2 Oct. 2015
171,1.0,Exploiting both local and global constraints for multi-span statistical language modeling,J.R. Bellegarda,"A new framework is proposed to integrate the various constraints, both local and global, that are present in the language. Local constraints are captured via n-gram language modeling, while global constraints are taken into account through the use of latent semantic analysis. An integrative formulation is derived for the combination of these two paradigms, resulting in several families of multi-span language models for large vocabulary speech recognition. Because of the inherent complementarity in the two types of constraints, the performance of the integrated language models, as measured by the perplexity, compares favorably with the corresponding n-gram performance.",IEEE,15-15 May 1998
172,1.0,Data Driven Approach for Language Model Adaptation using Stepwise Relative Entropy Minimization,Abhinav Sethy; Shrikanth Narayanan; Bhuvana Ramabhadran,"The ability to build domain and task specific language models from large generic text corpora is of considerable interest to the language modeling community. One of the key challenges is to identify the relevant text material in the collection. The text selection problem can be cast in a semi-supervised learning framework. Motivated by recent advancements in semi-supervised learning which emphasize the need of balanced label assignments, we present a stepwise relative entropy minimization scheme which focuses on selection of a set of sentences instead of selecting sentences solely on their individual merit. Our results on the IBM European Parliament Plenary Speech (EPPS) transcription system, show significant performance improvement (0.5% on an 8.9% baseline), with just a seventh of the out-of-domain data. The IBM EPPS LVCSR system which has a 60K vocabulary is a particularly hard baseline for out-of-domain adaptation because of low WER with in-domain training data.",IEEE,15-20 April 2007
173,1.0,Morpheme concatenation approach in language modeling for large-vocabulary Uyghur speech recognition,Mijit Ablimit; Askar Hamdulla; Tatsuya Kawahara,"For large-vocabulary continuous speech recognition (LVCSR) of highly-inflected languages, selection of an appropriate recognition unit is the first important step. The morpheme-based approach is often adopted because of its high coverage and linguistic properties. But morpheme units are short, often consisting of one or two phonemes, thus they are more likely to be confused in ASR than word units. Generally, word units provide better linguistic constraint, but increases the vocabulary size explosively, causing OOV (out-of-vocabulary) and data sparseness problems in language modeling. In this research, we investigate approaches of selecting word entries by concatenating morpheme sequences, which would reduce word error rate (WER). Specifically, we compare the ASR results of the word-based model and those of the morpheme-based model, and extract typical patterns which would reduce the WER. This method has been successfully applied to an Uyghur LVCSR system, resulting in a significant reduction of WER without a drastic increase of the vocabulary size.",IEEE,26-28 Oct. 2011
174,1.0,Keynote Talk 2 Training Large Language Models: Challenges and Opportunities,Mostofa Patwary,"Language models with large number of parameters trained on massive datasets can achieve state-of-the-art accuracies in various natural language processing applications including summarization, automatic dialogue generation, translation, semantic search, and code autocompletion. However, training such models is challenging as these models no longer fit in the largest GPU memory and can require a very long training time. Therefore, numerous innovations and breakthroughs are required in dataset, algorithms, software, and hardware altogether to make training these models a reality. In this talk, I present our efforts to train the Megatron-Turing Natural Language Generation model (MT-NLG), the largest and the most powerful monolithic transformer language model trained to date, with 530 billion parameters. I will also showcase several applications of MT-NLG and discuss future research and numerous opportunities that this model presents.",IEEE,30 May-3 June 2022
175,1.0,Extremely Low Resource Text simplification with Pre-trained Transformer Language Model,Takumi Maruyama; Kazuhide Yamamoto,"Recent text simplification approaches regard the task as a monolingual text-to-text generation inspired by machine translation. In particular, the transformer-based translation model outperform previous methods. Although machine translation approaches need a large-scale parallel corpus, parallel corpora for text simplification are very small compared to machine translation tasks. Therefore, we attempt a simple approach which fine-tunes the pre-trained language model for text simplification with a small parallel corpus. Specifically, we conduct experiments with the following two models: transformer-based encoder-decoder model and a language model that receives a joint input of original and simplified sentences, called TransformerLM. Thus, we show that TransformerLM, which is a simple text generation model, substantially outperforms a strong baseline. In addition, we show that fine-tuned TransformerLM with only 3,000 supervised examples can achieve performance comparable to a strong baseline trained by all supervised data.",IEEE,15-17 Nov. 2019
176,1.0,The Design of a Large-Scale Area Lighting Scheme Design Language for Olympic Park,Feng Chen; Xiaohui Rong; Pan Deng,"Based on the development of lighting control and management system in the central area of Beijing Olympic park, a large-scale area lighting scheme design language (LALSDL) has been proposed. Through the analysis of the demand of control and manage the lighting devices in the central area of Beijing Olympic park, the characteristics that LALSDL should have are summarized; through the summary of the involved operations of the lighting devices in the central area of Beijing Olympic park, lighting device declaration and operation statement are defined; device call expression (DCE) is used to define the call procedure of lighting device, The operational semantics of DCE is strictly defined. The results of this paper have been applied to the lighting control and management system in the central area of Beijing Olympic park, which was in good condition in the course of running.",IEEE,25-26 July 2009
177,1.0,How Well Can Language Models Understand Politeness?,Can Li; Bin Pang; Wenbo Wang; Lingshu Hu; Matthew Gordon; Detelina Marinova; Bitty Balducci; Yi Shang,"Politeness plays a key role in social communications. Previous work proposed an SVM-based computational method for predicting politeness using linguistic features on a corpus that contains Wikipedia and Stack Exchange requests data. To extend this prior work, we focus on evaluating the performance of state-of-the-art language models on politeness prediction using the same dataset. Two models are applied in this study. First, we fine-tune BERT on politeness data and then use the fine-tuned model for politeness prediction. Second, we use ChatGPT to predict politeness. The results show that both fine-tuned BERT and ChatGPT achieved better results than the state-of-the-art results on both Wikipedia and Stack Exchange data. Fine-tuned BERT outperforms zero shot ChatGPT, but ChatGPT can provide explanations for its prediction. Moreover, fine-tuned BERT outperforms human-level performance by 2.28% on Wikipedia corpus.",IEEE,5-6 June 2023
178,1.0,Extracting Requirements Models from Natural-Language Document for Embedded Systems,Chunhui Wang; Lu Hou; Xiaohong Chen,"Most of the requirements of embedded systems are written in natural language by users or customers. When the size of the document is large, it is not easy for developers to understand and analyze these requirements. Requirements modeling has been widely used and proven to be helpful to understand and analyze requirements. Manual analysis of these natural language requirements and extracting models are time-consuming and error-prone. Therefore, in this paper, we present a framework to extract model elements and semi-automatically generate requirements models from the NL requirements document for embedded systems. This leads to considerably simplify and accelerate the requirements development for embedded systems.",IEEE,15-19 Aug. 2022
179,1.0,Language model adaptation for conversational speech recognition using automatically tagged pseudo-morphological classes,C. Crespo; D. Tapias; G. Escalada; J. Alvarez,"Statistical language models provide a powerful tool for modelling natural spoken language. Nevertheless a large set of training sentences is required to estimate reliably the model parameters. The authors present a method for estimating n-gram probabilities from sparse data. The proposed language modeling strategy allows one to adapt a generic language model (LM) to a new semantic domain with just a few hundred sentences. This reduced set of sentences is automatically tagged with eighty different pseudo-morphological labels, and then a word-bigram LM is derived from them. Finally, this target domain word-bigram LM is interpolated with a generic back-off word-bigram LM, which was estimated using a large text database. This strategy reduces by 27% the word error rate of the SPATIS (SPanish ATIS) task.",IEEE,21-24 April 1997
180,1.0,Position Information for Language Modeling in Speech Recognition,Hsuan-Sheng Chiu; Guan-Yu Chen; Chun-Jen Lee; Berlin Chen,"This paper considers word position information for language modeling. For organized documents, such as technical papers or news reports, the composition and the word usage of articles of the same style are usually similar. Therefore, the documents can be separated into partitions consisting of identical rhetoric or topic styles by the literary structures, e.g., introductory remarks, related studies or events, elucidations of methodology or affairs, conclusions of the articles, and references, or footnotes of reporters. In this paper, we explore word position information and then propose two position- dependent language models for speech recognition. The structures and characteristics of these position-dependent language models were extensively investigated, while its performance was analyzed and verified by comparing it with the existing n-gram, mixture- and topic-based language models. The large vocabulary continuous speech recognition (LVCSR) experiments were conducted on the broadcast news transcription task. The preliminary results seem to indicate that the proposed position-dependent models are comparable to the mixture- and topic-based models.",IEEE,16-19 Dec. 2008
181,1.0,Resampling auxiliary data for language model adaptation in machine translation for speech,Sameer Maskey; Abhinav Sethy,"Performance of n-gram language models depends to a large extent on the amount of training text material available for building the models and the degree to which this text matches the domain of interest. The language modeling community is showing a growing interest in using large collections of auxiliary textual material to supplement sparse in-domain resources. One of the problems in using such auxiliary corpora is that they may differ significantly from the specific nature of the domain of interest. In this paper, we propose three different methods for adapting language models for a speech to speech (S2S) translation system when auxiliary corpora are of different genre and domain. The proposed methods are based on centroid similarity, n-gram ratios and resampled language models. We show how these methods can be used to select out of domain textual data such as newswire text to improve a S2S system. We were able to achieve an overall relative improvement of 3.8% in BLEU score over a baseline system that uses only in-domain conversational data.",IEEE,19-24 April 2009
182,1.0,Modeling large scale complex cyber physical control systems based on system of systems engineering approach,Lichen Zhang,"Most cyber physical systems are composed of subsystems. The subsystems themselves may have smaller sub-systems. Complex cyber physical systems rely heavily on the interplay of dozens of individual sub-systems. Thus, cyber physical systems are typical system of systems (SoS). In order to specify and model such kind of systems, we need develop specification and modeling methods which would be capable to encompass the systems of systems (SoS) specific properties of cyber physical systems. In this paper, we propose a new paradigm for specifying and modeling automotive cyber physical systems based on system-of-systems approach. In this paper, we propose an approach to support specification and modeling automotive cyber physical systems based on systems of systems engineering in the well established modeling language Modelicaml. The main aim of ModelicaML is to enable an efficient and effective way to use Modelica, UML and SysML models reusing notations that are also used for software modeling. We apply formal specification method in requirement analysis process in order to ensure that the software requirements model satisfies required system function and performance goals and constraints, including safety. The effectiveness of the approach is demonstrated with a case study of Vehicular Ad-hoc NETwork.",IEEE,12-13 Sept. 2014
183,1.0,Constructing use case models from Arabic user requirements in a semi-automated approach,Sari Jabbarin; Nabil Arman,"Automated software engineering has attracted a large amount of research efforts. The use of object-oriented methodologies for software systems development has made it necessary to develop approaches that automate the construction of different UML models in a semi-automated approach from textual user requirements. UML use case models represent an essential artifact that provide a perspective of the system under analysis or development The development of such use case models is very crucial in an object-oriented development methodology. The main principles used in obtaining these models are described. A natural language processing tool is used to parse different statements of the user requirements written in Arabic to obtain lists of nouns, noun phrases, verbs, verb phrases, etc. that aid in finding potential actors and use cases. A set of steps that represent our approach for constructing a use case model is presented.",IEEE,17-19 Jan. 2014
184,1.0,Petri net design language,Y. Chu,A Petri net design language is presented that is based on the earlier Computer Design Language (CDL). The language should be able to handle large-scale nets with hundreds or thousands of places and transitions and to provide a simulation environment. An example is given to illustrate the language.<>,IEEE,29-31 Aug. 1988
185,1.0,A class-based language model for large-vocabulary speech recognition extracted from part-of-speech statistics,C. Samuelsson; W. Reichl,"A novel approach is presented to class-based language modeling based on part-of-speech statistics. It uses a deterministic word-to-class mapping, which handles words with alternative part-of-speech assignments through the use of ambiguity classes. The predictive power of word-based language models and the generalization capability of class-based language models are combined using both linear interpolation and word-to-class backoff, and both methods are evaluated. Since each word belongs to one precisely ambiguity class, an exact word-to-class backoff model can easily be constructed. Empirical evaluations on large-vocabulary speech-recognition tasks show perplexity improvements and significant reductions in word error-rate.",IEEE,15-19 March 1999
186,1.0,Modeling disfluency and background events in ASR for a natural language understanding task,R.C. Rose; G. Riccardi,"This paper investigates techniques for minimizing the impact of non-speech events on the performance of large vocabulary continuous speech recognition (LVCSR) systems. An experimental study is presented that investigates whether the careful manual labeling of disfluency and background events in conversational speech can be used to provide an additional level of supervision in training HMM acoustic models and statistical language models. First, techniques are investigated for incorporating explicitly labeled disfluency and background events directly into the acoustic HMM. Second, phrase-based statistical language models are trained from utterance transcriptions which include labeled instances of these events. Finally, it is shown that significant word accuracy and run-time performance improvements are obtained for both sets of techniques on a telephone-based spoken language understanding task.",IEEE,15-19 March 1999
187,1.0,A comparative study of languages for model-based systems-of-systems engineering (MBSSE),Dov Dori; Niva Wengrowicz; Yehudit Judy Dori,"Teaching model-based systems engineering has become more challenging as research and development has shifted from systems to include systems-of-systems (SoSs). SoSs involve primarily combination of hardware and software, as well as humans and organizations. The need to teach model-based system-of-systems engineering has thus raised the need to determine what the most suitable language for modeling and teaching SoSs is. We developed a new course format, in which groups jointly reverse-engineer and model Web-based information systems in two different modeling languages, UML and OPM, and then individually assess their peers' projects. The goal of this study was to compare UML with OPM as two candidates. We developed and evaluated an online peer assessment tool which students used. About 130 undergraduate students in groups of six divided into teams of three modeled 23 systems in both UML and OPM. They then evaluated, compared, and ranked the clarity & understandability and the completeness of the four models of two systems that their peers had constructed. Findings demonstrate that neither the order of the models assessment nor the assessor gender affected the grading, indicating assessment reliability. We found significant differences in model clarity and understandability in favor of OPM and no differences in completeness between OPM and UML models. These findings validate our approach of teaching both conceptual modeling languages and using peer assessment in large-scale project-based undergraduate information systems engineering courses.",IEEE,3-7 Aug. 2014
188,1.0,Topic cache language model for speech recognition,Chuang-Hua Chueh; Jen-Tzung Chien,"Traditional n-gram language models suffer from insufficient long-distance information. The cache language model, which captures the dynamics of word occurrences in a cache, is feasible to compensate this weakness. This paper presents a new topic cache model for speech recognition based on the latent Dirichlet language model where the latent topic structure is explored from n-gram events and employed for word prediction. In particular, the long-distance topic information is continuously updated from the large-span historical words and dynamically incorporated in generating the topic mixtures through Bayesian learning. The topic cache language model does effectively characterize the unseen n-gram events and catch the topic cache for long-distance language modeling. In the experiments on Wall Street Journal corpus, the proposed method achieves better performance than baseline n-gram and the other related language models in terms of perplexity and recognition accuracy.",IEEE,14-19 March 2010
189,1.0,UML Modeling of Online Public Bus Reservation System in Egypt,Ayman R. Mohammed; Sally S. Kassem,"Designing a proper public transportation system is a main concern for many countries with large population size like Egypt. Therefore, the online based transportation business for individuals is highly growing in Egypt and the concept of online reservation in the transportation field has arisen. In this paper, a model in the analysis phase for an online public bus reservation system (OPBRS) is proposed. The system is large, complex and includes many interrelated functions. Therefore, the object-oriented modeling approach is chosen for developing the system by using the industry standard Unified Modeling Language (UML). The basic functions of the system are defined. Hence, the functional and structural models are presented to provide a framework for a public transportation OPBRS in Egypt.",IEEE,26-27 Oct. 2020
190,1.0,Gaussian Mixture Language Models for Speech Recognition,Mohamed Afify; Olivier Siohan; Ruhi Sarikaya,"We propose a Gaussian mixture language model for speech recognition. Two potential benefits of using this model are smoothing unseen events, and ease of adaptation. It is shown how this model can be used alone or in conjunction with a a conventional N-gram model to calculate word probabilities. An interesting feature of the proposed technique is that many methods developed for acoustic models can be easily ported to GMLM. We developed two implementations of the proposed model for large vocabulary Arabic speech recognition with results comparable to conventional N-gram.",IEEE,15-20 April 2007
191,1.0,Efficient and Robust Language Modeling in an Automatic Children's Reading Tutor System,Xiaolong Li; Yun-Cheng Ju; Li Deng; Alex Acero,"Recently, there has been a rapidly increasing interest in using ASR for children's language learning. An automatic reading tutor system built with ASR technologies can track children's oral reading against story texts, detect reading miscues, and measure the level of reading fluency. They may even diagnose the nature of the miscues and provide feedback to improve reading skills. In such tasks, N-gram language models (LM) may be trained from the whole story text, or may be generated based on current story sentence with heuristic probabilities for both regular words in the sentence and explicitly predicted reading miscues. The disadvantages of those methods are either they require a relatively large text and are time-consuming, or a large-sized LM and complex processing are needed to accommodate all possible words in reading stories as well as in reading miscues. This paper proposes an efficient and robust LM which can be easily built on-the-fly with current reading sentences. With an additional parallel ""garbage"" model, the LM can also deal effectively with a wide range of reading miscues. Our experiments in a standard children's reading task show that the new LM reaches the state-of-the-art performance in detecting reading miscues with a fast speed while only a relatively simple children's acoustic model of speech was used.",IEEE,15-20 April 2007
192,1.0,Automated Solution Development for Smart Grids: Tapping the Power of Large Language Models,Khuram Shahzad; Sohail Iqbal; Muhammad Moazam Fraz,"The development of automated solutions is poten-tially crucial to enable self-healing in smart grids. Software developers will be overwhelmed with the rise of Metaverse and game industry. Therefore, it is essential to fully automate the entire software development process with the help of cutting-edge artificial intelligence (AI) technologies and advanced data analytics approaches. In this paper, we propose a new framework for automated solution development for smart grids that makes use of AI and advanced data analytics to address this challenge. Recent developments in AI technologies are used by our proposed framework to facilitate automated solution development. Our modular approach simplifies and expedites several phases of the software development process by building on the shortcomings of the current frameworks. To generate revenue, our suggested framework offers a number of business models from its adoption. We expect that our idea will inspire others to make unique contributions to the development of automated solution in smart grids to improve its operations and boost our economy.",IEEE,9-10 June 2023
193,1.0,Bangla word clustering based on N-gram language model,Sabir Ismail; M. Shahidur Rahman,"In this paper, we describe a method for producing Bangla word clusters based on semantic and contextual similarity. Word clustering is important for parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. Computerization of Bangla language processing has been started a long ago, but still it is in neophyte stage and suffers from resource scarcity. We propose anunsupervised machine learning technique to develop Bangla word clusters based on their semantic and contextual similarity using N-gram language model. According to N-gram model, a word can be predictedbased on its previous and next words sequence. N-gram model is applied successfully for word clustering in English and some other languages. As word clustering in Bangla is a new dimension in Bangla language processing research, so we think this process is good way to start and our assumption is true as our result is quite decent. We produced 456 clusters using a locally available large Bangla corpus. Subjective score derived from the clusters reveal strong similarity of the words in the same cluster.",IEEE,10-12 April 2014
194,1.0,CIPTA: Contrastive-based Iterative Prompt-tuning Using Text Annotation from Large Language Models,Yu Yan; Wenzhuo Du; Di Yang; Dechun Yin,"In recent years, public opinion analysis has become increasingly important due to the widespread use of social media platforms and the growing influence of online information on public security. Prompt tuning, a typical few-shot learning method, ensures that the model quickly adapts to opinion analysis with different classification rules. However, existing prompt tuning for opinion analysis cannot guarantee the effectiveness of the model in zero-shot or one-shot cases. In this study, we propose the Contrastive-based Iterative Prompt-tuning method using Text-Annotation from Large Language Models (LLMs), CIPTA, for low-resource public opinion analysis. Specifically, with a small amount of manually labeled data, CIPTA leverages the knowledge from LLMs to text annotation and utilizes unsupervised contrastive embedding training to optimize text representation. Based on the prompt tuning method and the iterative training over unlabeled data, the model further utilizes the knowledge from the pre-training stage. Experiment results on tweet data show that our CIPTA achieves encouraging performance in public opinion analysis.",IEEE,12-14 May 2023
195,1.0,Artifact Analysis of Smell Evolution and Maintenance Tasks in Simulink Models,Saheed Popoola; Jeff Gray,"Bad smells often indicate a possible concern in a software design that may present challenges related to comprehension and maintenance. As a system evolves through a series of changes and maintenance activities, the bad smells embedded in the system may also evolve with the potential for introducing additional new smells. Existing bad smells research often targets textual code-based implementations. We found very little research on bad smells in systems designed with graphical languages that are used often in industry. This paper presents our analysis on the evolution of four bad smells in 575 Simulink models across 31 open-source repositories. We conducted our analysis by creating a chain of model-driven tools that could assist with various analysis needs. Our first step was to extract the evolution history of Simulink models in GitHub. Next, we manually classified each version to a maintenance category (i.e., adaptive, preventive, corrective, or perfective). Then, we developed queries to detect instances of four selected bad smells. Finally, we analysed the evolution of each of the smells across the version history of the repositories, the relationships between the smells and the size of the models, and the impact of maintenance activities on the evolution of the identified bad smells. The results suggest that: 1) larger models tend to contain more types of smells, 2) an increase in the instances of smells is usually associated with an increase in model size, but an increase in model size does not necessarily imply an increase in the number of smells, 3) the majority of bad smells are introduced during the initial construction of the models, although a significant portion of the smells are introduced at later stages, and 4) adaptive maintenance tasks often lead to an increase in the number of smells in Simulink models, but corrective maintenance tasks often correlate with a decrease in the number of smells.",IEEE,10-15 Oct. 2021
196,1.0,Heuristic Inference of Model Transformation Definitions from Type Mappings,Christopher Gerking; Ingo Budde,"Developers of model transformations are often required to produce verbose transformation definitions. This verbosity results from the need to link the elements of the target model explicitly by initializing their features, which requires developers to handwrite large amounts of repetitive transformation instructions. Whereas existing hybrid transformation languages automate the application of rules, they do not save developers from these boilerplate instructions. In this paper, we demonstrate that a large fraction of such instructions can be intelligently inferred from a set of simple mappings between the source and target types of a transformation. On this basis, we propose a heuristic inference mechanism to augment such type mappings with additional mappings for the initialization of features, thereby saving developers from the need to handwrite boilerplate instructions. We showcase the ability of our mechanism to reduce the verbosity of transformations using four case studies.",IEEE,15-20 Sept. 2019
197,1.0,Using End-to-end Multitask Model for Simultaneous Language Identification and Phoneme Recognition,Linjia Sun,"Correlated speech tasks can learn related information from each other to enhance the performance. In this paper, a unified model based on end-to-end DNN and multi-task learning is proposed, which can simultaneously realize language identification and phoneme recognition. As a special feature of this model, we use the syllable, a larger unit in linguistics, to model temporal patterns and multiple cues to improve the robust and accuracy of language identification/phoneme recognition, including phonetic, prosodic and phonotactic. These experimental results on three speech corpuses shown that the unified model and multiple cues achieves the accuracy improvements. In addition, the experimental results on NIST LRE07 shown that our method outperforms the other methods in the task of language identification.",IEEE,21-24 Oct. 2022
198,1.0,A Visual Specification Language for Model-to-Model Transformations,Esther Guerra; Juan de Lara; Dimitris Kolovos; Richard Paige,"Model Driven Engineering promotes models as the core assets of projects and hence model transformations become first-class citizens in this approach. Likewise, the development of large scale transformations necessitates a systematic engineering process and supporting modelling notations. However, although many languages have been proposed to implement transformations, few allow their specification at a higher level of abstraction. In this paper we present a visual, formal, declarative specification language to express model-to-model transformations and their correctness properties. The language supports the two main approaches to model-to-model transformation -- trace-based and traceless -- with a unified formal semantics. Moreover, we provide a compilation of specifications into OCL as this has many practical applications, e.g. it allows injecting assertions and correctness properties for automated testing of transformation implementations based on OMG standards.",IEEE,21-25 Sept. 2010
199,1.0,Hardware description language (HDL): An efficient approach to device independent designs for VLSI market segments,Okafor Kennedy Chinedu; Ezekwe Chinwe Genevera; Ogungbenro Oluwaseyi Akinyele,"Contemporarily, owing to astronomical advancements in the very large scale integration (VLSI) market segments, hardware engineers are now focusing on how to develop their new digital system designs in programmable languages like very high speed integrated circuit hardware description language, (VHDL) and Verilog and consequently target it to Complex programmable logic devices (CPLDs) and Field programmable gate arrays (FPGAs). This paper focuses on using VHDL, to design an application specific integrated circuit (ASIC) liquid dispenser controller system while targeting the device independent architecture (Ultra 3700 CPLD series) for synthesis, optimization and fitting to realize the design. The ASIC controller has two bin cans to dispense regular and diet drinks. The system dispenses a drink if the user activates a button for that drink and at least one can is available. A refill signal appears when both bins are empty. Activating a reset signal informs the system that the machine has been refilled and the bins are full. The design methodology is presented with other details in the body of this paper.",IEEE,24-26 Nov. 2011
200,1.0,Distributed training of large scale exponential language models,Abhinav Sethy; Stanley F. Chen; Bhuvana Ramabhadran,"Shrinkage-based exponential language models, such as the recently introduced Model M, have provided significant gains over a range of tasks [1]. Training such models requires a large amount of computational resources in terms of both time and memory. In this paper, we present a distributed training algorithm for such models based on the idea of cluster expansion [2]. Cluster expansion allows us to efficiently calculate the normalization and expectations terms required for Model M training by minimizing the computation needed between consecutive n-grams. We also show how the algorithm can be implemented in a distributed environment, greatly reducing the memory required per process and training time.",IEEE,22-27 May 2011
201,1.0,Big Data Storage using Model Driven Engineering: From Big Data Meta-model to Cloudera PSM meta-model,Allae Erraissi; Mouad Banane; Abdessamad Belangour; Mohamed Azzouazi,"The storage and retrieval of large volumes of information, as well as the search for intelligence within a mass of data, is at the heart of the concept of Big Data, and this is why this technology is so important for the IT community and society as a whole. It allows companies to gain valuable insight into their markets and customers so they can offer better-suited products or services. Thus, Big Data Storage plays a key role in decision making. After defining a generic meta-model in previous work for the Storage layer. This paper presents a shift from the generic Big Data Storage layer meta-model (PIM) to a Cloudera Distribution Storage layer meta-model (PSM) using the ATL transformation language. The result of the ATL transformation represents the PSM (Platform-Specific Models) according to the architecture of the MDA.",IEEE,26-27 Oct. 2020
202,1.0,Investigations on the use of morpheme level features in Language Models for Arabic LVCSR,Amr El-Desoky Mousa; Ralf Schlüter; Hermann Ney,"A major challenge for Arabic Large Vocabulary Continuous Speech Recognition (LVCSR) is the rich morphology of Arabic, which leads to high Out-of-vocabulary (OOV) rates, and poor Language Model (LM) probabilities. In such cases, the use of morphemes rather than full-words is considered a better choice for LMs. Thereby, higher lexical coverage and less LM perplexities are achieved. On the other side, an effective way to increase the robustness of LMs is to incorporate features of words into LMs. In this paper, we investigate the use of features derived for morphemes rather than words. Thus, we combine the benefits of both morpheme level and feature rich modeling. We compare the performance of stream-based, class-based and Factored LMs (FLMs) estimated over sequences of morphemes and their features for performing Arabic LVCSR. A relative reduction of 3.9% in Word Error Rate (WER) is achieved compared to a word-based system.",IEEE,25-30 March 2012
203,1.0,Trends and challenges in language modeling for speech recognition and machine translation,Holger Schwenk,"Summary form only given. Language models play an important role in large vocabulary continuous speech recognition (LVCSR) systems and statistical approaches to machine translation (SMT), in particular when modeling morphologically rich languages. Despite intensive research over more than 20 years, state-of-the-art LVCSR and SMT systems seem to use only one dominant approach: n-gram back-off language models. This talk first reviews the most important approaches to language modeling. I then discuss some of the recent trends and challenges for the future. An interesting alternative to the back-off n-gram approach are the so-called continuous space methods. The basic idea is to perform the probability estimation in a continuous space. By these means better probability estimations of unseen word sequences can be expected. There is also a relative large body of works on adaptive language models. The adaptation can aim to tailor a language model to a particular task or domain, or it can be performed over time. Another very active research area are discriminative language models. Finally, I will review the challenges and benefits of language models trained an very large amounts of training material.",IEEE,13 Nov.-17 Dec. 2009
204,1.0,Exploring Pre-Trained Language Models to Build Knowledge Graph for Metal-Organic Frameworks (MOFs),Yuan An; Jane Greenberg; Xiaohua Hu; Alex Kalinowski; Xiao Fang; Xintong Zhao; Scott McCLellan; Fernando J. Uribe-Romo; Kyle Langlois; Jacob Furst; Diego A. Gómez-Gualdrón; Fernando Fajardo-Rojas; Katherine Ardila; Semion K. Saikin; Corey A. Harper; Ron Daniel,"Building a knowledge graph is a time-consuming and costly process which often applies complex natural language processing (NLP) methods for extracting knowledge graph triples from text corpora. Pre-trained large Language Models (PLM) have emerged as a crucial type of approach that provides readily available knowledge for a range of AI applications. However, it is unclear whether it is feasible to construct domain-specific knowledge graphs from PLMs. Motivated by the capacity of knowledge graphs to accelerate data-driven materials discovery, we explored a set of state-of-the-art pre-trained general-purpose and domain-specific language models to extract knowledge triples for metal-organic frameworks (MOFs). We created a knowledge graph benchmark with 7 relations for 1248 published MOF synonyms. Our experimental results showed that domain-specific PLMs consistently outperformed the general-purpose PLMs for predicting MOF related triples. The overall benchmarking results, however, show that using the present PLMs to create domain-specific knowledge graphs is still far from being practical, motivating the need to develop more capable and knowledgeable pre-trained language models for particular applications in materials science.",IEEE,17-20 Dec. 2022
205,1.0,"Domain Robust, Fast, and Compact Neural Language Models",Alexander Gerstenberger; Kazuki Irie; Pavel Golik; Eugen Beck; Hermann Ney,"Despite advances in neural language modeling, obtaining a good model on a large scale multi-domain dataset still remains a difficult task. We propose training methods for building neural language models for such a task, which are not only domain robust, but reasonable in model size and fast for evaluation. We combine knowledge distillation from pretrained domain expert language models with the noise contrastive estimation (NCE) loss. Knowledge distillation allows to train a single student model which is both compact and domain robust, while the use of NCE loss makes the model self-normalized, which enables fast evaluation. We conduct experiments on a large English multi-domain speech recognition dataset provided by AppTek. The resulting student model is of the size of one domain expert, while it gives similar perplexities as various teacher models on their expert domain; the model is self-normalized, allowing for 30% faster first pass decoding than the naive models which require the full soft- max computation, and finally it gives improvements of more than 8% relative in terms of word error rate over a large multidomain 4-gram count model trained on more than 10 B words.",IEEE,4-8 May 2020
206,1.0,A form-based dialogue manager for spoken language applications,D. Goddeau; H. Meng; J. Polifroni; S. Seneff; S. Busayapongchai,"A popular approach to dialogue management is based on a finite state model, where user utterances trigger transitions between the dialogue states, and these states, in turn, determine the system's response. The paper describes an alternative dialogue planning algorithm based on the notion of filling in an electronic form, or ""E-form"". Each slot has associated prompts that guide the user through the dialogue, and a priority that determines the order in which the system tries to acquire information. These slots can be optional or mandatory. However, the user is not restricted to follow the system's lead, and is free to ignore the prompts and take the initiative in the dialogue. The E-form based dialogue planner has been used in an application to search a database of used car advertisements. The goal is to assist the user in selecting, from this database, a small list of cars which match their constraints. For a large number of dialogues collected from over 600 naive users, we found over 70% compliance in answering specific system prompts.",IEEE,3-6 Oct. 1996
207,1.0,VisTML: A Visual Modeling Language for Model Transformation,He Xiao; Ma Zhiyi; Liu Yi; Chen Hongjie; Shao Weizhong,"Recently model transformations quickly become large size and complex in structure. It is necessary to apply the modeling technique when developing transformations to manage the complexity. The paper proposes VisTML - a visual modeling language for model transformations. It provides six diagrams, which cover the whole lifecycle of transformation development and enable us to specify a transformation from various views and in different phases, i.e. the goal diagram, the transformation diagram, the rule diagram, the composition diagram, the test diagram, and the deployment diagram. We also provide a tool support and a real case of VisTML to illustrate the feasibility of VisTML. Besides, a comparison of VisTML, UML, and transML is presented at last.",IEEE,5-8 Dec. 2011
208,1.0,Language modeling with stochastic automata,Jianying Hu; W. Turin; M.K. Brown,"It is well known that language models are effective for increasing the accuracy of speech and handwriting recognizers, but large language models are often required to achieve low model perplexity (or entropy) and yet still have adequate language coverage. We study three efficient methods for stochastic language modeling in the context of the stochastic pattern recognition problem (variable-length Markov models, variable n-gram stochastic automata and refined probabilistic finite automata), and we give the results of a comparative performance analysis. In addition, we show that a method which combines two of these language modeling techniques yields an even better performance than the best of the single techniques tested.",IEEE,3-6 Oct. 1996
209,1.0,Style-specific language model adaptation for Korean conversational speech recognition,Young-Hee Park; Minhwa Chung,"We present our style-specific language model adaptation method for Korean conversational speech recognition. Compared with the written text corpora, conversational speech shows different characteristics of content and style such as filled pauses, word omission, and contraction, which are related to function words and depend on preceding or following words in Korean spontaneous speech. Since obtaining sufficient data for training language model is often difficult in a conversational domain, language model adaptation with large out-of-domain data is useful. For style-specific language model adaptation, first, we estimate in-domain dependent n-gram model by relevance weighting of out-of-domain text data according to style and content similarity. Here, style is represented by n-gram based tf/sup */idf similarity. Second, we train in-domain language model including disfluency model. Recognition results show-that n-gram based tf/sup */idf similarity weighting effectively reflects style difference and disfluencies can be used as a good predictor to the neighboring words.",IEEE,26-29 Oct. 2003
210,1.0,Smoothed language model incorporation for efficient time-synchronous beam search decoding in LVCSR,D. Willett; E. McDermott; S. Katagiri,"For performing the decoding search in large vocabulary continuous speech recognition (LVCSR) with hidden Markov models (HMM) and statistical language models, the most straightforward and popular approach is the time-synchronous beam search procedure. A drawback of this approach is that the time-asynchrony of the language model weight application during search leads to performance degradations. This is particularly so when performing the search with a tight pruning beam. This study presents a method for smoothing the language model within the recognition network. The optimization goal is the smearing of transition probabilities from HMM state to HMM state in favor of a more time-synchronous language model weight application. In addition, state-based language model look-ahead is proposed and evaluated. Both language model smoothing techniques lead to a remarkable improvement in accuracy-to-run-time ratio, while their combined application yields only limited improvements.",IEEE,9-13 Dec. 2001
211,1.0,Large vocabulary speech recognition in French,M. Adda-Decker; G. Adda; J. Gauvain; L. Lamel,"We present some design considerations concerning our large vocabulary continuous speech recognition system in French. The impact of the epoch of the text training material on lexical coverage, language model perplexity and recognition performance on newspaper texts is demonstrated. The effectiveness of larger vocabulary sizes and larger text training corpora for language modeling is investigated. French is a highly inflected language producing large lexical variety and a high homophone rate. About 30% of recognition errors are shown to be due to substitutions between inflected forms of a given root form. When word error rates are analysed as a function of word frequency, a significant increase in the error rate can be measured for frequency ranks above 5000.",IEEE,15-19 March 1999
212,1.0,How Robust Is a Large Pre-trained Language Model for Code Generationƒ A Case on Attacking GPT2,Rui Zhu; Cunming Zhang,"Large pre-trained language models have shown strong capabilities in the field of natural language processing, and in recent years research demonstrated that they can also produce surprising results from natural language descriptions in automatic code-generation applications. Although such models have performed well in a variety of domains, there is evidence that they can be affected by adversarial attacks, Consequently, it has become important to measure the robustness of models by producing adversarial examples. In this study, we proposed an attack method called the Modifier for Code Generation Attack (M-CGA), which is the first time a white box adversarial attack has been applied to the field of code generation. The M-CGA method measures the robustness of a model by producing adversarial examples that can cause the model to produce code that is incorrect or does not meet the criteria for use. Preliminary experimental results showed the M-CGA method to be an effective attack method, providing a new research direction in automatic code synthesis.",IEEE,21-24 March 2023
213,1.0,Improving Graph-Based Dependency Parsing Models With Dependency Language Models,Min Zhang; Wenliang Chen; Xiangyu Duan; Rong Zhang,"For graph-based dependency parsing, how to enrich high-order features without increasing decoding complexity is a very challenging problem. To solve this problem, this paper presents an approach to representing high-order features for graph-based dependency parsing models using a dependency language model and beam search. Firstly, we use a baseline parser to parse a large-amount of unannotated data. Then we build the dependency language model (DLM) on the auto-parsed data. A set of new features is represented based on the DLM. Finally, we integrate the DLM-based features into the parsing model during decoding by beam search. We also utilize the features in bilingual text (bitext) parsing models. The main advantages of our approach are: 1) we utilize rich high-order features defined over a view of large scope and additional large raw corpus; 2) our approach does not increase the decoding complexity. We evaluate the proposed approach on the monotext and bitext parsing tasks. In the monotext parsing task, we conduct the experiments on Chinese and English data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. In the bitext parsing task, we conduct the experiments on a Chinese-English bilingual data and our score is the best reported so far.",IEEE,Nov. 2013
214,1.0,Language modelling for large vocabulary speech recognition,J.P. Ueberla,"A speech recognizer is a device that translates speech into written text. As input, it takes the acoustic signal recorded by a microphone. As output, it produces a string of words intended to correspond to the input. The mapping from acoustic signal to a string of words is a complex task and it involves several stages. The central stage of this recognition process contains a search component, that makes use of two different sources of information, the acoustic model and the language model. Intuitively, the acoustic model gives a measure for how likely it is that a given chunk of the acoustic input corresponds to a given acoustic unit (e.g. phoneme, word). The language model gives a measure for how likely it is that this unit appears at this point in time, e.g. given the units that preceded it. Language models differ in the way the contexts are defined and in the way the probability distributions are being calculated. Different language models are presented and reviewed.",IEEE,7-7 May 1997
215,1.0,Acoustic modeling for under-resourced languages based on vectorial HMM-states representation using Subspace Gaussian Mixture Models,Mohamed Bouallegue; Emmanuel Ferreira; Driss Matrouf; Georges Linarès; Maria Goudi; Pascal Nocera,"This paper explores a novel method for context-dependent models in automatic speech recognition (ASR), in the context of under-resourced languages. We present a simple way to realize a tying states approach, based on a new vectorial representation of the HMM states. This vectorial representation is considered as a vector of a low number of parameters obtained by the Subspace Gaussian Mixture Models paradigm (SGMM). The proposed method does not require phonetic knowledge or a large amount of data, which represent the major problems of acoustic modeling for under-resourced languages. This paper shows how this representation can be obtained and used for tying states. Our experiments, applied on Vietnamese, show that this approach achieves a stable gain compared to the classical approach which is based on decision trees. Furthermore, this method appears to be portable to other languages, as shown in the preliminary study conducted on Berber.",IEEE,2-5 Dec. 2012
216,1.0,"Dual-Space Re-ranking Model for Efficient Document Retrieval, User Modeling and Adaptation",Ján Stas; Daniel Hládek; Martin Lojka; Jozef Juhár,"The increasing demand for the performance improvement and robustness of automatic transcription of spontaneous speech in Slovak forces us to look for the advanced methods of adaptation of acoustic and language models to the user-specific voice characteristics and the topic of their speech. One of the ways how to increase the domain robustness of language models is to improve the process of retrieving text documents relevant to the current topic of the speech and use them to adapt the existing background language model. This paper focuses on the analysis, design and implementation of a new dual-space re-ranking model for document retrieval, adaptation of language models to the current topic of speech and personalization of speech recognition system. The experimental results of the proposed dual-space reranking model based on the averaging coefficients produced by latent semantic indexing and paragraph vectors ranking models show an additional 1% relative improvement in word error rate against the efficiency of single-space model ranking.",IEEE,16-19 Sept. 2018
217,1.0,Towards language independent acoustic modeling,W. Byrne; P. Beyerlein; J.M. Huerta; S. Khudanpur; B. Marthi; J. Morgan; N. Peterek; J. Picone; D. Vergyri; T. Wang,"We describe procedures and experimental results using speech from diverse source languages to build an ASR system for a single target language. This work is intended to improve ASR in languages for which large amounts of training data are not available. We have developed both knowledge-based and automatic methods to map phonetic units from the source languages to the target language. We employed HMM adaptation techniques and discriminative model combination to combine acoustic models from the individual source languages for recognition of speech in the target language. Experiments are described in which Czech Broadcast News is transcribed using acoustic models trained from small amounts of Czech read speech augmented by English, Spanish, Russian, and Mandarin acoustic models.",IEEE,5-9 June 2000
218,1.0,Enriching the transfer learning with pre-trained lexicon embedding for low-resource neural machine translation,Mieradilijiang Maimaiti; Yang Liu; Huanbo Luan; Maosong Sun,"Most State-Of-The-Art (SOTA) Neural Machine Translation (NMT) systems today achieve outstanding results based only on large parallel corpora. The large-scale parallel corpora for high-resource languages is easily obtainable. However, the translation quality of NMT for morphologically rich languages is still unsatisfactory, mainly because of the data sparsity problem encountered in Low-Resource Languages (LRLs). In the low-resource NMT paradigm, Transfer Learning (TL) has been developed into one of the most efficient methods. It is difficult to train the model on high-resource languages to include the information in both parent and child models, as well as the initially trained model that only contains the lexicon features and word embeddings of the parent model instead of the child languages feature. In this work, we aim to address this issue by proposing the language-independent Hybrid Transfer Learning (HTL) method for LRLs by sharing lexicon embedding between parent and child languages without leveraging back translation or manually injecting noises. First, we train the High-Resource Languages (HRLs) as the parent model with its vocabularies. Then, we combine the parent and child language pairs using the oversampling method to train the hybrid model initialized by the previously parent model. Finally, we fine-tune the morphologically rich child model using a hybrid model. Besides, we explore some exciting discoveries on the original TL approach. Experimental results show that our model consistently outperforms five SOTA methods in two languages Azerbaijani (Az) and Uzbek (Uz). Meanwhile, our approach is practical and significantly better, achieving improvements of up to 4.94 and 4.84 BLEU points for low-resource child languages Az ! Zh and Uz ! Zh, respectively.",IEEE,Feb. 2022
219,1.0,Incremental language models for speech recognition using finite-state transducers,H.J.G.A. Dolfing; I.L. Hetherington,"In the context of the weighted finite-state transducer approach to speech recognition, we investigate a novel decoding strategy to deal with very large n-gram language models often used in large-vocabulary systems. In particular, we present an alternative to full, static expansion and optimization of the finite-state transducer network. This alternative is useful when the individual knowledge sources, modeled as transducers, are too large to be composed and optimized. While the recognition decoder perceives a single, weighted finite-state transducer, we apply a divide-and-conquer technique to split the language model into two parts which add up exactly to the original language model. We investigate the merits of these 'incremental language models' and present some initial results.",IEEE,9-13 Dec. 2001
220,1.0,Question Generation using Knowledge Graphs with the T5 Language Model and Masked Self-Attention,Kosuke Aigo; Takashi Tsunakawa; Masafumi Nishida; Masafumi Nishimura,"Question generation is helpful for understanding reading comprehension, spontaneous questioning in chatting systems, and expanding datasets for answering questions. In previous studies, many models have been used to generate questions from contexts, but none was suitable in large-length contexts. To overcome this challenge, we generated questions from an intermediate representation of a context, such as knowledge graphs. In this study, we focused on developing questions using knowledge graphs with the T5 language model. We used the language model to create questions using the knowledge graph and mask the self-attention of the encoder to train the model by explicitly preserving the graph’s structure. As a result of the automatic evaluation, the T5 language model with and without mask was comparable with the bidirectional Graph2Seq model (G2S), known as the QG model, using knowledge graphs. More-over, the masked language model was slightly better than the non-masked model in t5-small on four benchmarks. The code and data are publicly available at https://github.com/Macho000/T5-for-KGQG.",IEEE,12-15 Oct. 2021
221,1.0,Implicit language information replacing method in Japanese encoder–decode ASR model,Daiki Mori; Kengo Ohta; Ryota Nishimura; Norihide Kitaoka,"Recent years, automatic speech recognition (ASR) tasks often use language models as an adjunct to ASR models. The density ratio approach (DRA) is one of several language model integration methods. It is known that Japanese has a much larger number of characters than the alphabet language, and that there are variations in reading with homonyms and the same characters. It was unclear whether the “implicit language information” of character-based encoder–decoder ASR model using beam search algorithm is approximated by the external language model. In our experiments, We have applied an DRA to a Japanese encoder–decoder ASR model to reduce character error rate (CER) in cross-domain scenarios. Cross-domain CERs were calculated for the Japanese academic presentation speech (APS) corpus and the Japanese simulated presentation speech (SPS) corpus. This method achieved a relative error reduction of 11.0% and 22.5% with the RNN and Transformer models compared to the shallow fusion. To investigate the applicability of different speaking styles to different domains, We also conducted an experiment to replace the “implicit language information” inside the CSJ ASR model with Mainichi Shimbun language model. For the JNAS task, the DRA achieved a relative error reduction of 7.3% compared to the shallow fusion method.",IEEE,28-29 Sept. 2022
222,1.0,Continuous improvement applied to simulation modeling: a case study,A. Jayaraman; J.A. Green; A.K. Gunal,"A typical transfer machine consists of several synchronous stations served by a common transfer mechanism. Many simulation languages and simulators available today may provide useful constructs that allow detailed modeling of such mechanisms. However, large models involving many entities can be slow to run. The paper presents several techniques that can be used to build accurate and efficient models of systems that include one or more transfer machines and long conveyors. The techniques described in the paper were developed during a simulation study of a large-scale manufacturing system. The overall approach to the modeling also demonstrates the application of the ""continuous improvement"" concepts in simulation model development.",IEEE,3-6 Dec. 1995
223,1.0,Efficient Artistic Image Style Transfer with Large Language Model (LLM): A New Perspective,Bo Pan; YuKai Ke,"With the development of intelligent information systems, image style transfer technology has been widely known. However, many image style transfer methods can only target one style, which is inefficient in application. In this study, the novel efficient artistic image style transfer with LLM is proposed. To be gin with the study, the related single style transfer models are reviewed to serve as the background. Then, the novel image demosaicing is designed to serve as the pre-processing for the complex images. As the core of the model, novel style transfer algorithm is proposed with the LLM. The novel neural network organization is designed and the core functions are optimized. Furthermore, to validate the performance, the visualzied style transfer test is conducted and the numerical simulation results on the efficiency is tested.",IEEE,1-3 June 2023
224,1.0,Cross language information retrieval based on LDA,Ai Wang; YaoDong Li; Wei Wang,"This paper proposed a LDA-based cross-language retrieval model that did not rely on word-by-word translation of query or document. Instead, a parallel corpus was used to estimate a cross-language LDA (Latent Dirichlet Allocation) model. We assumed that a topic variable Z in LDA could generate both an English token and a Chinese token, given that the parallel corpus contained two languages: English and Chinese. Therefore, the LDA model was easy to be extended to multi-language information retrieval as long as a multi-lingual parallel corpus was provided. The proposed LDA-based crosslanguage retrieval model was compared with three popular retrieval models: LDA-based mono-lingual document model; Mono-lingual TF.IDF retrieval model; Cross-lingual Latent Semantic Indexing retrieval model on CNKI datasets. Experimental results showed that this model was very effective and achieved very good performance.",IEEE,20-22 Nov. 2009
225,1.0,The estimation of powerful language models from small and large corpora,P. Placeway; R. Schwartz; P. Fung; L. Nguyen,"The authors consider the estimation of powerful statistical language models using a technique that scales from very small to very large amounts of domain-dependent data. They begin with improved modeling of the grammar statistics, based on a combination of the backing-off technique and zero-frequency techniques. These are extended to be more amenable to the particular system considered here. The resulting technique is greatly simplified, more robust, and gives improved recognition performance over either of the previous techniques. The authors also consider the problem of robustness of a model based on a small training corpus by grouping words into obvious semantic classes. This significantly improves the robustness of the resulting statistical grammar. A technique that allows the estimation of a high-order model on modest computation resources is also presented. This makes it possible to run a 4-gram statistical model of a 50-million word corpus on a workstation of only modest capability and cost. Finally, the authors discuss results from applying a 2-gram statistical language model integrated in the HMM (hidden Markov model) search, obtaining a list of the N-best recognition results, and rescoring this list with a higher-order statistical model.<>",IEEE,27-30 April 1993
226,1.0,Seal: Efficient Training Large Scale Statistical Machine Translation Models on Spark,Rong Gu; Min Chen; Wenjia Yang; Chunfeng Yuan; Yihua Huang,"Statistical machine translation (SMT) is an important research branch in natural language processing (NLP). Similar to many other NLP applications, large scale training data can potentially bring higher translation accuracy for SMT models. However, the traditional single-node SMT model training systems can hardly cope with the fast-growing amount of large scale training corpus in the big data era, which makes the urgent requirement of efficient large scale machine translation model training systems. In this paper, we propose Seal, an efficient, scalable, and end-to-end offline SMT model training toolkit based on Apache Spark which is a widely-used distributed data-parallel platform. Seal parallelizes the training process of the entire three key SMT models that are the word alignment model, the translation model, and the N -Gram language model, respectively. To further improve the performance of the model training in Seal, we also propose a number of system optimization methods. In word alignment model training, by optimizing the block size tuning, the overhead of IO operation and communication is greatly reduced. In translation model training, by well encoding the training corpus, the data size transferred over the network can be reduced significantly, thus improving the overall training efficiency. We also optimize the maximum likelihood estimation (MLE) algorithm to solve the data skew issue on the join operation which is adopted both in the translation model training and the language model training. The experiment results show that Seal outperforms the well-known SMT training system Chaski with about 5× speedup for word alignment model training. For the syntactic translation model and language model training, Seal outperforms the existing cutting-edge tools with about 9~18× speedup and 8~9× speedup on average, respectively. On the whole, Seal outperforms the existing distributed system with 4~6× speedup and the single-node system with 9~60× speedup on average respectively. Besides, Seal achieves near-linear data and node scalability.",IEEE,11-13 Dec. 2018
227,1.0,"A domain specific language and methodology for control systems GUI specification, verification and prototyping",Matteo Risoldi; Didier Buchs,"A work-in-progress domain-specific language and methodology for modeling complex control systems GUIs is presented. MDA techniques are applied for language design and verification, simulation and prototyping.",IEEE,23-27 Sept. 2007
228,1.0,The Generation of Large-scale Wireless Sensor Network Model Based on OPNET-EMA,Chen Cheng; Yu Shaojun,"At present, the simulation of large-scale wireless sensor networks has become an academic hotspot and how to quickly and accurately build a large-scale sensor network model with good performance is one of the core problems. Most scholars usually use traditional OPNET graphical editor to deploy various network objects manually when building network models. However, this modeling method has some flows such as low efficiency, poor accuracy, and inability to automatically generate network models by accessing database. To solve the above problems, this paper puts forward a large-scale wireless sensor network modeling technology based on OPNET - EMA to automatically generate accurate models with data from database by adopting the XML (Hypertext Markup Language) technology after studying EMA (External Module Access) function provided by OPNET simulation platform. The method proposed in this paper has good theoretical and practical significance for the research of modeling technology of large-scale wireless sensor networks.",IEEE,27-29 Nov. 2020
229,1.0,Fast and accurate recognition of very-large-vocabulary continuous Mandarin speech for Chinese language with improved segmental probability modeling,Jia-Lin Shen; Lin-Shan Lee,"This paper presents a fast and accurate recognition of continuous Mandarin speech with very large vocabulary using an improved segmental probability model (SPM) approach. In order to extensively utilize the acoustic and linguistic knowledge to further improve the recognition performance, a few special techniques are thus developed. Preliminary simulation results show that the final achievable rate for the base syllable recognition with the improved segmental probability modeling is as high as 91.62%, which indicates a 18.48% error rate reduction and more than 3 times faster than the well-studied sub-syllable-based CHMM. Also, a tone recognizer and a word-based Chinese language model are included and the achieved recognition accuracy for the final decoded Chinese characters is 92.10%.",IEEE,9-9 May 1996
230,1.0,An object-oriented neural network language,Daikui Shouren Hu,"The author points out that there are many commonalities of neural network and object-oriented methodology, and gives an informal overview of the object-oriented neural network language (OONNL), specifically designed for the neurocomputer (software simulation/hardware implementation). It is a procedural and general-purpose language, which allows parallelism via the object-oriented concept. The parallelism expressible in OONNL is independent of the underlying hardware. It is easy to work, as it has few language constructs, yet it allows the definition of various kinds of neural network models and learning rules designed by the user. It has been adopted to be the simulation language of a large-scale neural network simulating system (GKD-NNSS). Its effectiveness and efficiency have been demonstrated by applications.<>",IEEE,18-21 Nov. 1991
231,1.0,Large vocabulary Uyghur continuous speech recognition based on stems and suffixes,Xin Li; Shang Cai; Jielin Pan; Yonghong Yan; Yafei Yang,"In this paper, we study the vocabulary design problem in Uyghur large vocabulary continuous speech recognition (LVCSR). Uyghur is an agglutinative language in which words can be formed by concatenating several suffixes to the stem. As a result, the number of word types in Uyghur is unlimited. If the word is used as the recognition unit, the out-of-vocabulary (OOV) rate will be very large with typical vocabulary sizes of 60 k-100 k. To avoid this problem, we split words into stems and suffixes and use these sub-words as the recognition units. Speech recognition experiments are performed in two test sets, one including sentences in books and another including sentences in conversations. Compared to the 80 k-word baseline, the use of stems and suffixes can alleviate the OOV rate problem dramatically and the best system reduces the word error rate (WER) from 46.5% to 44.5% in the book sentences test set and from 57.6% to 47.5% in the conversation sentences test set.",IEEE,29 Nov.-3 Dec. 2010
232,1.0,A novel algorithm for unsupervised prosodic language model adaptation,Sankaranarayanan Ananthakrishnan; Shrikanth Narayanan,"Symbolic representations of prosodic events have been shown to be useful for spoken language applications such as speech recognition. However, a major drawback with categorical prosody models is their lack of scalability due to the difficulty in annotating large corpora with prosodic tags for training. In this paper, we present a novel, unsupervised adaptation technique for bootstrapping categorical prosodic language models (PLMs) from a small, annotated training set. Our experiments indicate that the adaptation algorithm significantly improves the quality and coverage of the PLM. On a test set derived from the Boston University Radio News corpus, the adapted PLM gave a relative improvement of 13.8% over the seed PLM on the binary pitch accent detection task, while reducing the OOV rate by 16.5% absolute.",IEEE,31 March-4 April 2008
233,1.0,The application of ESA model in teaching large class English comprehensive reading,Tang Xuemei,"More admission of students into colleges challenges the teaching of English majors to a great extent. This article illustrates the practice of ESA model in teaching English major's large class comprehensive reading and further proves that this model is an effective and practical approach in language teaching. Based on this teaching experiment, it is shown that ESA model in English comprehensive reading with the combination and flexible use of Engage-Study-Activate enables students to be the center of class, activates their interest and realizes the role exchange in teaching and learning to enhance students' overall ability in using language.",IEEE,24-27 Aug. 2010
234,1.0,Turkish Large Vocabulary Continuous Speech Recognition by using limited audio corpus,Derya Susman; Selçuk Köprü; Adnan Yazici,"In this paper, the recognition performances of several methodologies proposed in the context of Turkish Large Vocabulary Continuous Speech Recognition are retrieved by using a limited audio corpus. Word based, stem based, stem-ending based, and morph based language models are utilized with different n-gram orders. Word based and stem-ending based language models are extended by using several approaches. Also, a hybrid language model which is based on word based and stem-ending based language models is proposed. Word based language model is observed to outperform sub-word language models when limited audio corpus is used.",IEEE,18-20 April 2012
235,1.0,On Compressing N-Gram Language Models,Teemu Hirsimaki,"In large-vocabulary speech recognition systems, the major part of memory resources is typically consumed by a large n-gram language model. Representing the language model compactly is important in recognition systems targeted for small devices with limited memory resources. This paper extends the compressed language model structure proposed earlier by Whittaker and Raj. By separating n-grams that are prefixes to longer n-grams, redundant information can be omitted. Experiments on English 4-gram models and Finnish 6-gram models show that extended structure can achieve up to 30% lossless memory reductions when compared to baseline structure of Whittaker and Raj.",IEEE,15-20 April 2007
236,1.0,Large-Scale Unsupervised Pre-Training for End-to-End Spoken Language Understanding,Pengwei Wang; Liangchen Wei; Yong Cao; Jinghui Xie; Zaiqing Nie,"End-to-end Spoken Language Understanding (SLU) is proposed to infer the semantic meaning directly from audio features without intermediate text representation. In this paper, we explore unsupervised pre-training for End-to-end SLU models by learning representations from large-scale raw audios. The pre-trained model preserves semantic features which benefit the downstream SLU tasks as the learned model weights are further fine-tuned on the task specific training data. Our approach out-perform the state-of-the-art end-to-end SLU system with over 18.33% error reduction.",IEEE,4-8 May 2020
237,1.0,Threat Analysis Using Topic Models in Large-Scale Vulnerability Databases and Security Incident Case Documents,Hiroki Koyanagi; Kazuo Takaragi; Sven Wohlgemuth; Katsuyuki Umezawa,"It is crucial to design products bearing security in mind from the initial development stage. Consequently, many threat analysis support tools have been developed. However, it is difficult to determine the inherent threats in various designed documents written in natural language, which is used in the initial development stage. It is not uncommon to find attacks that closely resemble past attacks. In addition, many designs are limited in the number of data they can handle. We propose a method of extracting existing vulnerabilities similar to those used in the attack by collating a large vulnerability database with existing attack cases using Latent Dirichlet Allocation, one of the topic model methods. We apply the proposed method to several cases and verify its effectiveness.",IEEE,8-9 Nov. 2021
238,1.0,A hybrid Verilog-A and equation-defined subcircuit approach to mos switched current analog cell modeling and simulation in the transient and large signal AC domains,M.E. Brinson; S. Jahn; H. Nabijou,"Conventional modeling and simulation of two phase switched current MOS integrated circuits is normally undertaken at semiconductor device level. This allows primary and secondary circuit effects to be studied and characterized. However, with the growing complexity of these circuits, transient domain simulation times can become prohibitively long, restricting the size of circuit that can be easily investigated. Measurable reductions in transient simulation run times can be achieved by modeling part, or all, of a switched current design as a macromodel. This paper introduces a hybrid approach to MOS switched current circuit modeling that combines the primary features of compact device modeling with functional circuit macromodeling. To illustrate the proposed hybrid modeling procedure the properties, and simulation model, of a MOS switched current analog memory cell are described. The material presented also demonstrates how recent trends in Quite universal circuit simulator (Qucs) technology promote embedded Verilog-A models and equation-defined subcircuits as integral elements in mixed-mode circuit and system design.",IEEE,24-26 June 2010
239,1.0,Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models,Hendrik Strobelt; Albert Webson; Victor Sanh; Benjamin Hoover; Johanna Beyer; Hanspeter Pfister; Alexander M. Rush,"State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.",IEEE,Jan. 2023
240,1.0,Performance evaluation of deep bottleneck features for spoken language identification,Bing Jiang; Yan Song; Si Wei; Meng-Ge Wang; Ian McLoughlin; Li-Rong Dai,"Our previous work has shown that Deep Bottleneck Features (DBF), generated from a well-trained Deep Neural Network (DNN), can provide high performance Language Identification (LID) when Total Variability (TV) modelling is used for a back-end. This may largely be attributed to the powerful capability of the DNN for finding a frame-level representation which is robust to variances caused by different speakers, channels and background noise. However the DBF in the previous work were extracted from a DNN that was trained using a large ASR dataset. Optimal LID DBF parameters may differ from those that are known to be optimal for ASR. Thus this paper focuses on investigating different DBF extractors, input layer window sizes and dimensionality, and bottleneck layer location. Additionally, principal component analysis (PCA) is used to decorrelate the DBF. Experiments, based on the Gaussian Mixture Model-Universal Background Model (GMM-UBM) operating on the NIST LRE 2009 database, are conducted to evaluate the system. Results allow comparison between different DBF extractor parameters, as well as demonstrating that LID based on DBF can significantly outperform the conventional shift delta cepstral (SDC) features.",IEEE,12-14 Sept. 2014
241,1.0,Language-Agnostic and Language-Aware Multilingual Natural Language Understanding for Large-Scale Intelligent Voice Assistant Application,Daniel Yue Zhang; Jonathan Hueser; Yao Li; Sarah Campbell,"Natural language understanding (NLU) is one of the most critical components in goal-oriented dialog systems and enables innovative Big Data applications such as intelligent voice assistants (IVA) and chatbots. While recent advances in deep learning-based NLU models have achieved significant improvements in terms of accuracy, most existing works are monolingual or bilingual. In this work, we propose and experiment with techniques to develop multilingual NLU models. In particular, we first propose a purely language-agnostic multilingual NLU framework using a multilingual BERT (mBERT) encoder, a joint decoder design for intent classification and s lot filling tasks, and a novel co-appearance regularization technique. Then three distinct language-aware multilingual NLU approaches are proposed including using language code as explicit input; using language-specific parameters during decoding; and using implicit language identification as an auxiliary task. We show results for a large-scale, commercial IVA system trained on a various set of intents with huge vocabulary sizes, as well as on a public multilingual NLU dataset. We performed experiments in explicit consideration of code-mixing and language dissimilarities which are practical concerns in large-scale real-world IVA systems. We have found that language-aware designs can improve NLU performance when language dissimilarity and code-mixing exist. The empirical results together with our proposed architectures provide important insights towards designing multilingual NLU systems.",IEEE,15-18 Dec. 2021
242,1.0,Joint Morphological-Lexical Language Modeling (JMLLM) for Arabic,Ruhi Sarikaya; Mohamed Afify; Yuqing Gao,"Language modeling for inflected languages such as Arabic poses new challenges for speech recognition due to rich morphology. The rich morphology results in large increases in perplexity and out-of-vocabulary (OOV) rate. In this study, we present a new language modeling method that takes advantage of Arabic morphology by combining morphological segments with the underlying lexical items and additional available information sources with regards to morphological segments and lexical items within a single joint model. Joint representation and modeling of morphological and lexical items reduces the OOV rate and provides smooth probability estimates. Preliminary experiments detailed in this paper show satisfactory improvements over word and morpheme based trigram language models and their interpolations.",IEEE,15-20 April 2007
243,1.0,Applying the reduction approach to EQNM2L large models,Ahlem Nasri; Abdelhabib Bourouis; Brahim Belattar,"At a time when complexity becomes a major issue in system modeling, it becomes necessary to simplify, as much as possible, complex systems to be modeled. The inherent complexity of this latter obstructs understanding of their models. In turn, the number of components, interactions between them and the size of the model, increase when complexity increases. As a result, visualization and comprehension become difficult. As well, the analysis or the simulation of large models becomes time consuming tasks. For these reasons, there is a need to reduce the system size, which reduces the number of components and time analysis or simulation. In this field, there is a wide range of methodologies for reducing large models, but they are often specialized according to modeling languages. In this work, we are interested to queueing networks reduction methodologies. This article gives an approach for reducing EQNM2L (Extended Queuing Networks Modeling and Markup Language) large model to a smaller one. The objective is to reduce the analysis time as well as the simulation time of the network. The basic idea is to divide the model into a set of submodels which are analyzed in isolation. In this work, we propose to substitute each submodel by a single M/G/∞ station. The main conclusion of this work is that this adopted rule provides exact results for the mean number of clients at the network and the mean time spent at the whole network. For other parameters, approximated results are obtained.",IEEE,30 March-1 April 2016
244,1.0,Research on Historical Concept Modeling Language and Modeling Tool based on Model Driven Architecture,Mingzhu Ma; Hongxing Liu; Zhenqi Li,"This paper proposes a framework for a model-driven historical compilation method. The basic idea is to distinguish historical conceptual models and historical works models, so as to support large and medium-scale historical compilations of “top-down, gradual refinement”. It focuses on defining the historical concept modeling language, and designing its abstract grammar, concrete grammar and semantics. Based on MetaEdit+, a preliminary graphical historical concept modeling tool is designed and implemented.",IEEE,10-12 Dec. 2021
245,1.0,Large Margin Training Improves Language Models for ASR,Jilin Wang; Jiaji Huang; Kenneth Ward Church,"Language models (LM) have been widely deployed in modern ASR systems. The LM is often trained by minimizing its perplexity on speech transcript. However, few studies try to discriminate a ""gold"" reference against inferior hypotheses. In this work, we propose a large margin language model (LMLM). LMLM is a general framework that enforces an LM to assign a higher score to the ""gold"" reference, and a lower one to the inferior hypothesis. The general framework is applied to three pretrained LM architectures: left-to-right LSTM, transformer encoder, and transformer decoder. Results show that LMLM can significantly outperform traditional LMs that are trained by minimizing perplexity. Especially for challenging noisy cases. Finally, among the three architectures, transformer encoder achieves the best performance.",IEEE,6-11 June 2021
246,1.0,Fine-Tuning Language Models to Mitigate Gender Bias in Sentence Encoders,Tommaso Dolci,"Language models are used for a variety of downstream applications, such as improving web search results or parsing CVs to identify the best candidate for a job position. At the same time, concern is growing around word and sentence embeddings, popular language models that have been shown to exhibit large amount of social bias. In this work, by leveraging the possibility to further train state-of-the-art pre-trained embedding models, we propose to mitigate gender bias by fine-tuning sentence encoders on a semantic similarity task built around gender stereotype sentences and corresponding gender-swapped anti-stereotypes, in order to enforce similarity between the two categories. We test our intuition on two popular language models, BERT-Base and DistilBERT, and measure the amount of gender bias mitigation using the Sentence Encoder Association Test (SEAT). Our solution shows promising results despite using a small amount of training data, proving that post-processing bias mitigation techniques based on fine-tuning can effectively reduce gender bias in sentence encoders.",IEEE,15-18 Aug. 2022
247,1.0,Feature modeling of two large-scale industrial software systems: Experiences and lessons learned,Daniela Lettner; Klaus Eder; Paul Grünbacher; Herbert Prähofer,"Feature models are frequently used to capture the knowledge about configurable software systems and product lines. However, feature modeling of large-scale systems is challenging as many models are needed for diverse purposes. For instance, feature models can be used to reflect the perspectives of product management, technical solution architecture, or product configuration. Furthermore, models are required at different levels of granularity. Although numerous approaches and tools are available, it remains hard to define the purpose, scope, and granularity of feature models. In this paper we thus present experiences of developing feature models for two large-scale industrial automation software systems. Specifically, we extended an existing feature modeling tool to support models for different purposes and at multiple levels. We report results on the characteristics and modularity of the feature models, including metrics about model dependencies. We further discuss lessons learned during the modeling process.",IEEE,30 Sept.-2 Oct. 2015
248,1.0,Automatic Performance Model Transformation from UML to C++,Sabri Pllana; Siegfried Benkner; Fatos Xhafa; Leonard Barolli,"We address the issue of the development of performance models for programs that may be executed on large-scale computing systems. The commonly used approaches apply non-standard notations for model specification and often require that the software engineer has a thorough understanding of the underlying performance modeling technique. We propose to bridge the gap between the performance modeling and software engineering by incorporating UML. In our approach we aim to permit the graphical specification of performance model in a human-intuitive fashion on one hand, but on the other hand we aim for a machine-efficient model evaluation. The user specifies graphically the performance model using UML. Thereafter, the transformation of the performance model from the human-usable UML representation to the machine-efficient C++ representation is done automatically. We describe our methodology and illustrate it with the automatic transformation of a sample performance model.",IEEE,8-12 Sept. 2008
249,1.0,A maximum entropy language model integrating N-grams and topic dependencies for conversational speech recognition,S. Khudanpur; Jun Wu,"A compact language model which incorporates local dependencies in the form of N-grams and long distance dependencies through dynamic topic conditional constraints is presented. These constraints are integrated using the maximum entropy principle. Issues in assigning a topic to a test utterance are investigated. Recognition results on the Switchboard corpus are presented showing that with a very small increase in the number of model parameters, reduction in word error rate and language model perplexity are achieved over trigram models. Some analysis follows, demonstrating that the gains are even larger on content-bearing words. The results are compared with those obtained by interpolating topic-independent and topic-specific N-gram models. The framework presented here extends easily to incorporate other forms of statistical dependencies such as syntactic word-pair relationships or hierarchical topic constraints.",IEEE,15-19 March 1999
250,1.0,Morpheme-based feature-rich language models using Deep Neural Networks for LVCSR of Egyptian Arabic,Amr El-Desoky Mousa; Hong-Kwang Jeff Kuo; Lidia Mangu; Hagen Soltau,"Egyptian Arabic (EA) is a colloquial version of Arabic. It is a low-resource morphologically rich language that causes problems in Large Vocabulary Continuous Speech Recognition (LVCSR). Building LMs on morpheme level is considered a better choice to achieve higher lexical coverage and better LM probabilities. Another approach is to utilize information from additional features such as morphological tags. On the other hand, LMs based on Neural Networks (NNs) with a single hidden layer have shown superiority over the conventional n-gram LMs. Recently, Deep Neural Networks (DNNs) with multiple hidden layers have achieved better performance in various tasks. In this paper, we explore the use of feature-rich DNN-LMs, where the inputs to the network are a mixture of words and morphemes along with their features. Significant Word Error Rate (WER) reductions are achieved compared to the traditional word-based LMs.",IEEE,26-31 May 2013
251,1.0,Pipilika N-Gram Viewer: An Efficient Large Scale N-Gram Model for Bengali,Adnan Ahmad; Mahbubur Rub Talha; Md. Ruhul Amin; Farida Chowdhury,"In this paper, we introduce a large-scale Bengali N-gram model, trained on online newspaper corpus and present results and analysis of two different experiments done by using the model, namely Context-aware spell checker and Trending topic detection. We also present the process with emphasis on the problems that arise in working with data at this scale. One signicant aspect of our N-gram model is that the model contains information of N-gram occurrence per day over a period of eight years, from the year 2009-2017. This enables further applications of the model, for example, Trending topic detection. Our Bengali N-gram language model contains N-grams up to 5-gram with more than 2 million unique Unigrams and over 656 million duplicate Unigrams. We evaluate our model by calculating the perplexities of different years. We obtain F -score of 86.6% in an experiment of Context-aware spell checker. In another experiment, we successfully detected the trending topics of a given time frame. This paper also presents first Bengali N-gram viewer, where one can query by a particular N-gram and see the resulting graph of the frequency of that term occurred using different time frames.",IEEE,21-22 Sept. 2018
252,1.0,Language Model Supervision for Handwriting Recognition Model Adaptation,Christopher Tensmeyer; Curtis Wigington; Brian Davis; Seth Stewart; Tony Martinez; William Barrett,"Not all languages and domains of handwriting have large labeled datasets available for training handwriting recognition (HWR) models. One way to address this problem is to leverage high resource languages to help train models for low resource languages. In this work, we adapt HWR models trained on a source language to a target language that uses the same writing script. We do so using only labeled data in the source language, unlabeled data in the target language, and a language model in the target language. The language model is used to produce target transcriptions to allow regular example based training. Using this approach we demonstrate improved transferability among French, English, and Spanish languages using both historical and modern handwriting datasets.",IEEE,5-8 Aug. 2018
253,1.0,Future vector enhanced LSTM language model for LVCSR,Qi Liu; Yanmin Qian; Kai Yu,"Language models (LM) play an important role in large vocabulary continuous speech recognition (LVCSR). However, traditional language models only predict next single word with given history, while the consecutive predictions on a sequence of words are usually demanded and useful in LVCSR. The mismatch between the single word prediction modeling in trained and the long term sequence prediction in read demands may lead to the performance degradation. In this paper, a novel enhanced long short-term memory (LSTM) LM using the future vector is proposed. In addition to the given history, the rest of the sequence will be also embedded by future vectors. This future vector can be incorporated with the LSTM LM, so it has the ability to model much longer term sequence level information. Experiments show that, the proposed new LSTM LM gets a better result on BLEU scores for long term sequence prediction. For the speech recognition rescoring, although the proposed LSTM LM obtains very slight gains, the new model seems obtain the great complementary with the conventional LSTM LM. Rescoring using both the new and conventional LSTM LMs can achieve a very large improvement on the word error rate.",IEEE,16-20 Dec. 2017
254,1.0,Efficacy of a constantly adaptive language modeling technique for web-scale applications,Kuansan Wang; Xiaolong Li,"In this paper, we describe CALM, a method for building statistical language models for the Web. CALM addresses several unique challenges dealing with the Web contents. First, CALM does not rely on the whole corpus to be available to build the language model. Instead, we design CALM to progressively adapt itself as Web chunks are made available by the crawler. Second, given the dynamic and dramatic changes in the Web contents, CALM is designed to quickly enrich its lexicon and N-grams as new vocabulary and phrases are discovered. To reduce the amount of heuristics and human interventions typically needed for model adaptation, we derive an information theoretical formula for CALM to facilitate the optimal adaptation in the maximum a posteriori (MAP) sense. Testing against a collection of Web chunks where new vocabulary and phrases are dominant, we show CALM can achieve comparable and satisfactory model measured in perplexity. We also show CALM is robust against over training and the initial condition, suggesting that any assumptions made in obtaining the initial model can gradually see their impacts diminished as CALM runs its full course and adapt to more data.",IEEE,19-24 April 2009
255,1.0,Evaluation of neural network language models in handwritten Chinese text recognition,Yi-Chao Wu; Fei Yin; Cheng-Lin Liu,"Handwritten Chinese text recognition based on over-segmentation and path search integrating contexts has been demonstrated successful, where language models play an important role. Recently, neural network language models (NNLMs) have shown superiority to back-off N-gram language models (BLMs) in handwriting recognition, but have not been studied in Chinese text recognition system. This paper investigates the effects of NNLMs in handwritten Chinese text recognition and compares the performance with BLMs. We trained character-level language models in 3-, 4- and 5- gram on large scale corpora and applied them in text line recognition system. Experimental results on the CASIA-HWDB database show that NNLM and BLM of the same order perform comparably, and the hybrid model by interpolating NNLM and BLM improves the recognition performance significantly.",IEEE,23-26 Aug. 2015
256,1.0,Measuring and Reducing Modeling Effort in Domain-Specific Modeling Languages with Examples,James H. Hill,"Domain-specific modeling languages (DSMLs) facilitate rapid and ``correct-by-construction'' realization of concepts for the target domain. Although DSMLs provide such benefits, there is implied (or hidden) modeling effort---in terms of user actions---associated with using a DSML that can negatively impact its effectiveness. It is therefore critical that DSML developers understand the meaning of modeling effort and how to reduce it so their DSML is of high quality. This paper provides two contributions to research on developing DSMLs. First, the paper defines a metric for measuring model effort. Secondly, this paper discusses several techniques, with examples, reducing (or improving) modeling effort. The techniques discussed in the paper have been applied to an open-source DSML called the Platform Independent Component Modeling Language (PICML), which is currently used in both academic and industry settings for designing and implementing large-scale distributed systems. Finally, results show that it is possible to reduce modeling effort without requiring user studies to analyze such concerns.",IEEE,27-29 April 2011
257,1.0,Integrated phrase segmentation and alignment algorithm for statistical machine translation,Y. Zhang; S. Vogel; A. Waibel,"We present an integrated phrase segmentation/alignment algorithm (ISA) for statistical machine translation. Without the need of building an initial word-to-word alignment or initially segmenting the monolingual text into phrases as other methods do, this algorithm segments the sentences into phrases and finds their alignments simultaneously. For each sentence pair, ISA builds a two-dimensional matrix to represent a sentence pair where the value of each cell corresponds to the point-wise mutual information (MI) between the source and target words. Based on the similarities of MI values among cells, we identify the aligned phrase pairs. Once all the phrase pairs are found, we know both how to segment one sentence into phrases and also the alignments between the source and target sentences. We use monolingual bigram language models to estimate the joint probabilities of the identified phrase pairs. The joint probabilities are then normalized to conditional probabilities, which are used by the decoder. Despite its simplicity, this approach yields phrase-to-phrase translations with significant higher precisions than our baseline system where phrase translations are extracted from the HMM word alignment. When we combine the phrase-to-phrase translations generated by this algorithm with the baseline system, the improvement on translation quality is even larger.",IEEE,26-29 Oct. 2003
258,1.0,Entropy Based Pruning of Backoff Maxent Language Models with Contextual Features,Tongzhou Chen; Diamantino Caseiro; Pat Rondon,"In this paper, we present a pruning technique for maximum entropy (MaxEnt) language models. It is based on computing the exact entropy loss when removing each feature from the model, and it explicitly supports backoff features by replacing each removed feature with its backoff. The algorithm computes the loss on the training data, so it is not restricted to models with n-gram like features, allowing models with any feature, including long range skips, triggers, and contextual features such as device location. Results on the I-billion word corpus show large perplexity improvements relative for frequency pruned models of comparable size. Automatic speech recognition (ASR) experiments show word error rate improvements in a large-scale cloud based mobile ASR system for Italian.",IEEE,15-20 April 2018
259,1.0,Three probabilistic language models for a large-vocabulary speech recognizer,P. Dumouchel; V. Gupta; M. Lennig; P. Mermelstein,"Relative performance is compared for three different language models applied to the linguistic decoding part of a 75000-word speech recognizer. These models are the trigram model, the tri-POS model (POS stands for parts of speech), and a smoothed trigram model with tied distributions for words three or more syllables long. The full trigram model gives the best performance but is most expensive in terms of data and storage requirements. The smoothed trigram and tri-POS models yield equivalent performance. For general text entry tasks, use of the tri-POS model is suggested since it is less sensitive to variations in the discourse domains. For applications specific to individual discourse domains, trigram models trained on data obtained from the target domain are recommended.<>",IEEE,11-14 April 1988
260,1.0,The (ab)use of Open Source Code to Train Large Language Models,Ali Al-Kaswan; Maliheh Izadi,"In recent years, Large Language Models (LLMs) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as Software Engineering. LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization. We argue why the use of copyleft code to train LLMs is a legal and ethical dilemma. Finally, we provide four actionable recommendations to address this issue.",IEEE,20-20 May 2023
261,1.0,Using an Architecture Description Language to Model a Large-Scale Information System -- An Industrial Experience Report,Eoin Woods; Rabih Bashroush,"An organisation that had developed a large Information System wanted to embark on a programme of significant evolution for the system. As a precursor to this, it was decided to create a comprehensive architectural description. This undertaking faced a number of challenges, including a low general awareness of software modelling and software architecture practices. The approach taken for this project included the definition of a simple, specific, architecture description language. This paper describes the experiences of the project and the ADL created as part of it.",IEEE,20-24 Aug. 2012
262,1.0,Sign Language Recognition System Based on Weighted Hidden Markov Model,Wenwen Yang; Jinxu Tao; Changfeng Xi; Zhongfu Ye,"Sign language recognition (SLR) plays an important role in communication between deaf and hearing society. However, the recognition result is still worse for signer independent recognition. The reason is that there exists large variation between the signs from different subjects. In this paper, weighted hidden markov model (HMM) is proposed to deal with the variation. Unlike traditional HMM, WHMM assigns each sign samples with different weights. For the sign sample with big variation, the sample weight is big accordingly. Furthermore, we utilize Kinect to produce robust sign features to improve recognition rate. Our system is evaluated on one Chinese sign language dataset of 156 isolated sign words. Experimental result shows our proposed method outperforms other methods with a high recognition rate of 94.74%.",IEEE,12-13 Dec. 2015
263,1.0,Training candidate selection for effective rejection in open-set language identification,Qian Zhang; John H.L. Hansen,"Research in open-set language identification (LID) generally focuses more on accurate in-set modeling versus improved out-of-set (OOS) rejection. Unknown or OOS language rejection is a challenge, since research developers seldom commit equivalent OOS corpus development effort versus the desired in-set languages. To address this, we propose an OOS candidate selection method for universal OOS language coverage. Since effective selection always requires abundant knowledge of inter-language relationships, three broad measurements across world languages are considered. Finally, the advanced OOS selection method is evaluated on a database derived from a large-scale corpus (LRE-09) with a state-of-the-art i-Vector system followed by two back-ends. The baseline system is realized using a random selection of OOS candidates. With the proposed selection method and probabilistic linear discriminative analysis (PLDA) back-end, the OOS rejection performance is improved with false alarm and miss rates achieving a relative reduction of 32.6% and 4.4%, respectively. In addition, the overall classification performance are relatively improved 8.4% and 7.5% according to the two back-ends based on an average cost function.",IEEE,7-10 Dec. 2014
264,1.0,Research of model scheduling strategy in large Web3D scene based on XML,Lihong Luo; Xiamei Tan,"In this paper, we analyze the three bottlenecks that limit the large-scene in virtual reality applications and especially in the Web-based applications. We also analyze the technical characteristics and defects of the LOD strategy and then propose a novel partitioning model. Based on the proposed model and XML, we present a scheme to overcome the three bottlenecks regarding CPU, memory and network bandwidth. Our scheme will help the large-scene Web-based virtual reality applications to be implemented in practice.",IEEE,29 June-1 July 2017
265,1.0,Avoiding unpredicted behaviour of large scale embedded systems by design and application of modelling rules,B. Florentz; M. Mutz; M. Huhn,"One big problem in integrating components of complex systems is unpredicted behaviour of one or more components and as a result of the whole system. To overcome this problem, modelling rules for models of a particular formalism are needed to avoid incompatibilities between models of different sources and misunderstandings between developers. We provide a pragmatic approach for designing and applying modelling rules for e.g. UML statecharts supported by different tools to avoid the problem of unpredicted behaviour. Therefore, one needs to understand the syntax and semantics of statecharts and the reasons for different behaviour of look-alike models. A statechart simulator called SCSim based on a modular semantics for statecharts helps to understand these differences and to find matching modelling rules. A tool called Rule Checker applies these rules on statechart models. Finally, we can check if a statechart is well designed according to this rules.",IEEE,2-2 Nov. 2004
266,1.0,Variational approximation of long-span language models for lvcsr,Anoop Deoras; Tomáš Mikolov; Stefan Kombrink; Martin Karafiát; Sanjeev Khudanpur,"Long-span language models that capture syntax and semantics are seldom used in the first pass of large vocabulary continuous speech recognition systems due to the prohibitive search-space of sentence-hypotheses. Instead, an N-best list of hypotheses is created using tractable n-gram models, and rescored using the long-span models. It is shown in this paper that computationally tractable variational approximations of the long-span models are a better choice than standard n-gram models for first pass decoding. They not only result in a better first pass output, but also produce a lattice with a lower oracle word error rate, and rescoring the N-best list from such lattices with the long-span models requires a smaller N to attain the same accuracy. Empirical results on the WSJ, MIT Lectures, NIST 2007 Meeting Recognition and NIST 2001 Conversational Telephone Recognition data sets are presented to support these claims.",IEEE,22-27 May 2011
267,1.0,Cascade RNN-Transducer: Syllable Based Streaming On-Device Mandarin Speech Recognition with a Syllable-To-Character Converter,Xiong Wang; Zhuoyuan Yao; Xian Shi; Lei Xie,"End-to-end models are favored in automatic speech recognition (ASR) because of its simplified system structure and superior performance. Among these models, recurrent neural network transducer (RNN-T) has achieved significant progress in streaming on-device speech recognition because of its high-accuracy and low-latency. RNN-T adopts a prediction network to enhance language information, but its language modeling ability is limited because it still needs paired speech-text data to train. Further strengthening the language modeling ability through extra text data, such as shallow fusion with an external language model, only brings a small performance gain. In view of the fact that Mandarin Chinese is a character-based language and each character is pronounced as a tonal syllable, this paper proposes a novel cascade RNN-T approach to improve the language modeling ability of RNN-T. Our approach firstly uses an RNN-T to transform acoustic feature into syllable sequence, and then converts the syllable sequence into character sequence through an RNN-T-based syllable-to-character converter. Thus a rich text repository can be easily used to strengthen the language model ability. By introducing several important tricks, the cascade RNN-T approach surpasses the character-based RNN-T by a large margin on several Mandarin test sets, with much higher recognition quality and similar latency.",IEEE,19-22 Jan. 2021
268,1.0,Enriching large language models with semantic lexicons and analogies. (Enrichir des modèles de langue de grande taille avec des lexiques sémantiques et des analogies).,Georgios Zervakis,,DBLP,2023
6,0.9101664,"Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models.",Paula Maddigan; Teo Susnjak,,DBLP,2023
269,1.0,Editorial: What Have Large-Language Models and Generative Al Got to Do With Artificial Life?,Alan Dorin; Susan Stepney,,DBLP,2023
270,1.0,Large Language Models.,Vinton G. Cerf,,DBLP,2023
271,1.0,L-Space and Large Language Models.,Jofish Kaye,,DBLP,2023
272,1.0,Can large language models write reflectively.,Yuheng Li; Lele Sha; Lixiang Yan; Jionghao Lin; Mladen Rakovic; Kirsten Galbraith; Kayley M. Lyons; Dragan Gasevic; Guanliang Chen,,DBLP,2023
273,1.0,Large Language Models Demonstrate the Potential of Statistical Learning in Language.,Pablo Contreras Kallens; Ross Deans Kristensen-McLachlan; Morten H. Christiansen,,DBLP,2023
274,1.0,Conceptual Modeling and Large Language Models: Impressions From First Experiments With ChatGPT.,Hans-Georg Fill; Peter Fettke; Julius Köpke,,DBLP,2023
5,0.8066712,Can Large Language Models Better Predict Software Vulnerability?,Evangelos Katsadouros; Charalampos Z. Patrikakis; George F. Hurlburt,,DBLP,2023
275,1.0,Cybercrime and Privacy Threats of Large Language Models.,Nir Kshetri,,DBLP,2023
276,1.0,ChatGPT and a new academic reality: Artificial Intelligence-written research papers and the ethics of the large language models in scholarly publishing.,Brady D. Lund; Ting Wang; Nishith Reddy Mannuru; Bing Nie; Somipam Shimray; Ziang Wang,,DBLP,2023
277,1.0,Improving mathematics assessment readability: Do large language models help?,Nirmal Patel; Pooja Nagpal; Tirth Shah; Aditya Sharma; Shrey Malvi; Derek Lomas,,DBLP,2023
278,1.0,Do Large Language Models Understand Chemistry? A Conversation with ChatGPT.,Cayque Monteiro Castro Nascimento; André Silva Pimentel,,DBLP,2023
279,1.0,"Biases in Large Language Models: Origins, Inventory, and Discussion.",Roberto Navigli; Simone Conia; Björn Ross,,DBLP,2023
280,1.0,Large language models and scientific publishing.,Ronald Rousseau; Liying Yang; Johan Bollen; Zhesi Shen,,DBLP,2023
281,1.0,Parameter-efficient fine-tuning of large-scale pre-trained language models.,Ning Ding; Yujia Qin; Guang Yang; Fuchao Wei; Zonghan Yang; Yusheng Su; Shengding Hu; Yulin Chen; Chi-Min Chan; Weize Chen; Jing Yi; Weilin Zhao; Xiaozhi Wang; Zhiyuan Liu; Hai-Tao Zheng; Jianfei Chen; Yang Liu; Jie Tang; Juanzi Li; Maosong Sun,,DBLP,2023
282,1.0,Large language models challenge the future of higher education.,Silvia Milano; Joshua A. McGrane; Sabina Leonelli,,DBLP,2023
283,1.0,Large Language Models and the Reverse Turing Test.,Terrence J. Sejnowski,,DBLP,2023
284,1.0,Prompting Is Programming: A Query Language for Large Language Models.,Luca Beurer-Kellner; Marc Fischer; Martin T. Vechev,,DBLP,2023
285,1.0,Fine-tuning large neural language models for biomedical natural language processing.,Robert Tinn; Hao Cheng; Yu Gu; Naoto Usuyama; Xiaodong Liu; Tristan Naumann; Jianfeng Gao; Hoifung Poon,,DBLP,2023
286,1.0,"Application of Large Language Models to Software Engineering Tasks: Opportunities, Risks, and Implications.",Ipek Ozkaya,,DBLP,2023
287,1.0,Agile Methodology for the Standardization of Engineering Requirements Using Large Language Models.,Archana Tikayat Ray; Bjorn F. Cole; Olivia J. Pinon Fischer; Anirudh Prabhakara Bhat; Ryan T. White; Dimitri N. Mavris,,DBLP,2023
288,1.0,Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models.,Hendrik Strobelt; Albert Webson; Victor Sanh; Benjamin Hoover; Johanna Beyer; Hanspeter Pfister; Alexander M. Rush,,DBLP,2023
289,1.0,Metaphorian: Leveraging Large Language Models to Support Extended Metaphor Creation for Science Writing.,Jeongyeon Kim; Sangho Suh; Lydia B. Chilton; Haijun Xia,,DBLP,2023
290,1.0,Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course.,Skylar Kolisko; Carolyn Jane Anderson,,DBLP,2023
291,1.0,Can You Answer This? - Exploring Zero-Shot QA Generalization Capabilities in Large Language Models (Student Abstract).,Saptarshi Sengupta; Shreya Ghosh; Preslav Nakov; Prasenjit Mitra,,DBLP,2023
292,1.0,Robot Behavior-Tree-Based Task Generation with Large Language Models.,Yue Cao; C. S. George Lee,,DBLP,2023
293,1.0,Can Large Language Models Safely Address Patient Questions Following Cataract Surgery?,Mohita Chowdhury; Ernest Lim; Aisling Higham; Rory McKinnon; Nikoletta Ventoura; Yajie He; Nick de Pennington,,DBLP,2023
294,1.0,WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models.,John M. Giorgi; Augustin Toma; Ronald Xie; Sondra Chen; Kevin R. An; Grace X. Zheng; Bo Wang,,DBLP,2023
